from datasets import Dataset 
from ragas.evaluation import evaluate 
from ragas.metrics import (faithfulness, answer_relevancy, context_recall, context_precision,)
import pandas as pd
from multiquery_chain import final_chain  # ne znam zasto je ovo uopste importovano ovde 
from chroma_db_connection import create_retriever 
from langchain_openai.chat_models import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
import os 
from dotenv import load_dotenv 

load_dotenv() 

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)


class Evaluation():
    
    def __init__(self,retriever,rag_chain) -> None:
        self.questions = [
            "Explain to me Default dataset and show me relevant plots representing number of individuals who defaulted.",
            "When it comes to the deciding on the important variables, explain the ways we make decision about the number of the model variables.",
            "What is Linear Discriminant Analysis for more than one predictors? Do all observations have the same covariance matrix?Show me multivariate Gaussian density for more predictors (graphs for two and three predictors) and formula for it.",
            "Explain me ridge regression on the Credit dataset and tell me formula for function for estimating coefficients for Ridge regression",
            "Show me the image representing is there any correlation between education level and wage?",
            "How does advertising location affect sales performance?", 
            "Show me the comparison between Bayes and KNN decision boundary.",
            "Show me Multivariate Gaussian distribution in the 3 - dimensional space.", # tabela + opis tabele 
            "Retrieve me results about sales on TV measured by all regression metrics.",
            "Show me from scratch how to perform linear regression in Python on Boston dataset.", # python kod 
            "Show me all regression metric values and coeficients for all predictors on Boston dataset after training linear regression in Python.", # tabela sa rezultatima python koda 
            "Tell me all classification metrics.",
            "Explain me change of log of number of bikers in the Bikeshare dataset.", # slika
            "What is outlier? ", # 112 - kratak odgovor 
            "What is collinearity?", # 115 - kratak odgovor
            "Does collinearity reduce the accuracy of the estimates regarding the regression coefficients?", # 116
            "Table of metrics of multiple regression models (Model 1 and Model 2) on Credit data set.", # tabela 117
            "What are the most common problems of collinearity?",
            "What are two solutions of the problem of collinearity?", # 118 
            "Which object in Python contains references to built-in functions?", # 126
            "Python method for producing confidence intervals for the predicted values.", # 129 
            "Based on the linearity of boundaries which approach could have the best results (QDA, Naive Bayes, KNN) ? ", # 175 str 
            "Show me appliance of Linear regression on Bikeshare data.", # 178 
            "Table of results for a Poission regression model fit to predict bikers in the Bikeshare data", #180 
            "Point out distincions between the Poissson and linear regression models.", # 180 
            "Explain me which variables are being present in Smarket data?", # 182 
            "Python implementation of generalized linear models on the Smarket dataset.", # 183 
            "Tuning Parameters in KNN algorithm for n_neighbors.", # 196
            "Backward stepwise selection algorithm", # 234 
            "Adventages of lasso over ridge regression", # 255
            "What is principal components analysis?", 
            "Defining the Null and Alternative Hypotheses",
            "How to decide whether to reject the null hypothesis?",
            "Explain the Benjamini–Hochberg Procedure",
            "Enumerate the most common types of linkage in hierarchial clustering?",
            "Explain me an approach to interpreting the results of clustering.",
            "Test error rates for neural networks with regularization on the MNIST data.",
            "What is pooling layer in neural networks?"
        ]
        self.retriever = retriever
        self.ground_truth = ground_truth
        self.answers = []
        self.contexts = [] 
        self.rag_chain = rag_chain 
    
    def evaluate_chain(self):
        print("Usao u metodu...")
        for query in self.questions:
            print("Sledece pitanje...")
            #self.answers.append(self.rag_chain.invoke({"question":query}))  # multiquery kao query decomposition pristup 
            self.answers.append(self.rag_chain(query,self.retriever)) # rekurzivna dekompozicija + dekompozicija sa individualnim odgovorima
            self.contexts.append([document for document in self.retriever.invoke(query)])  # ono sto izlazi iz retrievera

        data = {
            "question":self.questions,
            "answer":self.answers,
            "contexts":self.contexts,
            "ground_truth":self.ground_truth
        }
        print(type(self.ground_truth))
        dataset = Dataset.from_dict(data)
        dataset.to_csv("results/dataset_evaluation_HyDE.csv")
        result = evaluate(
            dataset = dataset, 
            metrics=[
                context_precision,
                context_recall,
                faithfulness,
                answer_relevancy,
            ],
            llm = ChatOpenAI(model="gpt-4o-mini",temperature=0,api_key=OPENAI_API_KEY),
            embeddings = embeddings,
            )
        
        return result.to_pandas()


ground_truth=[
    "\n".join([
        "In this chapter, we will illustrate the concept of classification using the simulated Default data set. We are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance. The data set is displayed in Figure 4.1. In the left-hand panel of Figure 4.1, we have plotted annual income and monthly credit card balance for a subset of 10,000 individuals. The individuals who defaulted in a given month are shown in orange, and those who did not in blue. (The overall default rate is about 3%, so we have plotted only a fraction of the individuals who did not default.) It appears that individuals who defaulted tended to have higher credit card balances than those who did not. In the center and right-hand panels of Figure 4.1, two pairs of boxplots are shown. The first shows the distribution of balance split by the binary default variable; the second is a similar plot for income. In this chapter, we learn how to build a model to predict default (Y) for any given value of balance (X1) and income (X2). Since Y is not quantitative, the simple linear regression model of Chapter 3 is not a good choice: we will elaborate on this further in Section 4.2.",
        "The provided image consists of three plots that visualize the relationships between income, balance, and default status.\n\n1. **Scatter Plot (Left)**\n   - **Axes**: The x-axis represents the \"Balance\" and the y-axis represents \"Income.\"\n   - **Data Points**: There are two types of data points: blue circles and brown crosses.\n     - **Blue Circles**: Represent individuals who did not default.\n     - **Brown Crosses**: Represent individuals who defaulted.\n   - **Observations**: \n     - Individuals who did not default (blue circles) are spread across a wide range of balance and income values, but they are more concentrated at lower balances.\n     - Individuals who defaulted (brown crosses) tend to have higher balances. They are also spread across various income levels, but there is a higher concentration on the right side of the plot, indicating higher balances.\n\n2. **Box Plot for Balance (Middle)**\n   - **Axes**: The y-axis represents the \"Balance.\"\n   - **Categories**: The x-axis categorizes data into two groups: \"No\" (did not default) and \"Yes\" (defaulted).\n   - **Observations**:\n     - The median balance for individuals who did not default is lower compared to those who defaulted.\n     - The interquartile range (IQR) for individuals who defaulted is higher compared to those who did not default.\n     - There are some outliers in both categories.\n\n3. **Box Plot for Income (Right)**\n   - **Axes**: The y-axis represents the \"Income.\"\n   - **Categories**: The x-axis categorizes data into two groups: \"No\" (did not default) and \"Yes\" (defaulted).\n   - **Observations**:\n     - The median income is slightly higher for individuals who did not default compared to those who defaulted.\n     - The interquartile ranges (IQR) for both categories are similar, indicating a similar spread in income for both groups.\n     - There are a few outliers in both categories.\n\nOverall, the plots illustrate that individuals who defaulted tend to have higher balances, while income does not show a very strong differentiation between those who defaulted and those who did not."
    ]),
    "\n".join([
        'Two: Deciding on Important Variables\n\nAs discussed in the previous section, the first step in a multiple regression analysis is to compute the F-statistic and to examine the associated p- value. If we conclude on the basis of that p-value that at least one of the predictors is related to the response, then it is natural to wonder which are the guilty ones! We could look at the individual p-values as in Table 3.4, but as discussed (and as further explored in Chapter 13), if p is large we are likely to make some false discoveries.\n\nIt is possible that all of the predictors are associated with the response, but it is more often the case that the response is only associated with a subset of the predictors. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as variable selection. The variable selection problem is studied extensively in Chapter 6, and so here we will provide only a brief outline of some classical approaches.\n\nIdeally, we would like to perform variable selection by trying out a lot of different models, each containing a different subset of the predictors. For instance, if p = 2, then we can consider four models: (1) a model contain- ing no variables, (2) a model containing X, only, (3) a model containing',
        'Forward selection. We begin with the model—a model that con- tains an intercept but no predictors. We then fit p simple linear re- gressions and add to the model the variable that results in the lowest RSS. We then add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied.\n\nBackward selection. We start with all variables in the model, and remove the variable with the largest p-value—that is, the variable hat is the least statistically significant. The new (p — 1)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached. For instance, we may stop when all remaining variables have a p-value below some hreshold.\n\nMized selection. This is a combination of forward and backward se- ection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We con- inue to add variables one-by-one. Of course, as we noted with the Advertising example, the p-values for variables can become larger as new predictors are added to the model. Hence, if at any point the p-value for one of the variables in the model rises above a certain hreshold, then we remove that variable from the model. We con- inue to perform these forward and backward steps until all variables in the model have a sufficiently low p-value, and all variables outside he model would have a large p-value if added to the model.',
        'Backward selection requires that the number of samples n is larger than the number of variables p (so that the full model can be fit). In contrast, forward stepwise can be used even when n < p, and so is the only viable subset method when p is very large.\n\nHybrid Approaches\n\nThe best subset, forward stepwise, and backward stepwise selection ap- proaches generally give similar but not identical models. As another al- ternative, hybrid versions of forward and backward stepwise selection are available, in which variables are added to the model sequentially, in analogy to forward selection. However, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit. Such an approach attempts to more closely mimic best sub- set selection while retaining the computational advantages of forward and backward stepwise selection.',
        'Backward selection cannot be used if p > n, while forward selection can always be used. Forward selection is a greedy approach, and might include variables early that later become redundant. Mixed selection can remedy this.\n\nMallow’s Cp\n\nAkaike\n\ninformation\n\ncriterion\n\nBayesi information\n\ncriterion\n\nadjusted R?\n\nforward\n\nselection\n\nmodel\n\nbackward\n\nselection\n\nmixed selection\n\nThree: Model Fit\n\nTwo of the most common numerical measures of model fit are the RSE and R?, the fraction of variance explained. These quantities are computed and interpreted in the same fashion as for simple linear regression.\n\nRecall that in simple regression, R? is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals Cor(Y, yy, the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.'       
        '8This is related to the important concept of multiple testing, which is the focus of Chapter 13.\n\nhigh- dimensional\n\nvariable selection\n\nX» only, and (4) a model containing both X; and X2. We can then se- lect the best model out of all of the models that we have considered. How do we determine which model is best? Various statistics can be used to judge he quality of a model. These include Mallow’s C,, Akaike informa- tion criterion (AIC), Bayesian information criterion (BIC), and adjusted R?. These are discussed in more detail in Chapter 6. We can also deter- mine residuals, in order to search for patterns. which model is best by plotting various model outputs, such as the\n\nUnfortunately, there are a total of 2? models that contain subsets of p variables. This means that even for moderate p, trying out every possible subset of the predictors is infeasible. For instance, we saw that if p = 2, then there 930 — small and e are 2? = 4 models to consider. But if p = 30, then we must consider 073,741,824 models! This is not practical. Therefore, unless p is very ; we cannot consider all 2? models, and instead we need an automated fficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:' 
    ]),
    "\n".join([
        "The multivariate Gaussian distribution assumes that each individual pre- dictor follows a one-dimensional normal distribution, as in (4.16), with some correlation between each pair of predictors. Two examples of multivariate Gaussian distributions with p = 2 are shown in Figure 4.5. The height of the surface at any particular point represents the probability that both X1 and X»2 fall in a small region around that point. In either panel, if the sur- face is cut along the X; axis or along the X2 axis, the resulting cross-section will have the shape of a one-dimensional normal distribution. The left-hand panel of Figure 4.5 illustrates an example in which Var(X1) = Var(X2) and Cor(X 1, X2) = 0; this surface has a characteristic bell shape. However, the bell shape will be distorted if the predictors are correlated or have unequal variances, as is illustrated in the right-hand panel of Figure 4.5. In this situation, the base of the bell will have an elliptical, rather than circular, shape. To indicate that a p-dimensional random variable X has a multi- variate Gaussian distribution, we write X ~ N(y,%). Here E(X) = p is the mean of X (a vector with p components), and Cov(X) = & is the p X p covariance matrix of X. Formally, the multivariate Gaussian density is defined as",
        'fee) = Gage (—Fle— TB Me—u)). 4.28)\n\nIn the case of p > 1 predictors, the LDA classifier assumes that the observations in the kth class are drawn from a multivariate Gaussian dis- tribution N(j,, =), where ju, is a class-specific mean vector, and ¥ is a covariance matrix that is common to all K classes. Plugging the density function for the kth class, f,(X =), into (4.15) and performing a little bit of algebra reveals that the Bayes classifier assigns an observation X = x\n\nmultivariate Gaussian\n\nFIGURE 4.6. An example with three classes. The observations from each class are drawn from a multivariate Gaussian distribution with p = 2, with a class-spe- cific mean vector and a common covariance matrix. Left: Ellipses that contain 95 % of the probability for each of the three classes are shown. The dashed lines are the Bayes decision boundaries. Right: 20 observations were generated from each class, and the corresponding LDA decision boundaries are indicated using solid black lines. The Bayes decision boundaries are once again shown as dashed lines.\n\nto the class for which\n\n1 x(x) = 2 DO pg — shh Elie + log T (4.24)\n\nis largest. This is the vector/matrix version of (4.18).\n\nAn example is shown in the left-hand panel of Figure 4.6. Three equally- sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represent regions that con- tain 95% of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which 6; (x) = 6¢(x); ie.',
        'The provided image consists of two three-dimensional surface plots side by side. \n\nBoth plots feature a smooth, bell-shaped surface centered on a grid plane. The surfaces exhibit a gradient color scheme that transitions from blue at the base to yellow and then to pink at the peak, indicating varying elevations.\n\nEach plot is labeled with the axes \\(X_1\\) and \\(X_2\\) along the horizontal plane, with the vertical axis representing the height of the surface. The surfaces suggest a peak in the middle, which smoothly descends towards the edges, forming a symmetric, rounded hill in both plots. The perspective is set to show a three-dimensional view, making the elevation changes clearly visible.\n\nThe main difference between the two plots is the orientation or slight variation in the shape of the surfaces, with the peak on the left plot appearing slightly more pointed compared to the peak in the right plot, which is more spread out.',
        'The image consists of two scatter plots placed side by side, each containing data points and decision boundaries.\n\n**Left Plot:**\n- The plot displays three sets of ellipses, each filled with a different color: yellow, light blue, and green.\n- Each ellipse represents a cluster of data points in a 2-dimensional space.\n- The axes are labeled \\(X_1\\) (horizontal axis) and \\(X_2\\) (vertical axis), both ranging from -4 to 4.\n- The plot contains three dashed lines forming a triangular decision boundary, dividing the space into three regions.\n- The ellipses overlap slightly, indicating that the data points are not perfectly separable.\n\n**Right Plot:**\n- This plot shows individual data points instead of ellipses.\n- The data points are color-coded: yellow, light blue, and green, corresponding to the colors of the ellipses in the left plot.\n- The axes are similarly labeled \\(X_1\\) and \\(X_2\\) with the same range from -4 to 4.\n- Solid and dashed lines form decision boundaries, partitioning the space into three regions.\n- The decision boundaries are consistent with the clusters, with each region predominantly containing data points of a single color.\n\nOverall, the image appears to illustrate a classification problem, with the left plot showing the theoretical distribution of the data and the right plot showing actual data points and their classification boundaries.'
    ]),
    "\n".join([
        'An Application to the Credit Data\n\nIn Figure 6.4, the ridge regression coefficient estimates for the Credit data set are displayed. In the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\. For example, the black solid line represents the ridge regression estimate for the income coefficient, as \\ is varied. At the extreme left-hand side of the plot, \\ is essentially zero, and so the corresponding ridge coefficient estimates are the same as the usual least squares esti- mates. But as A increases, the ridge coefficient estimates shrink towards zero. When X is extremely large, then all of the ridge coefficient estimates are basically zero; this corresponds to the model that contains no pre- dictors. In this plot, the income, limit, rating, and student variables are displayed in distinct colors, since these variables tend to have by far the largest coefficient estimates. While the ridge coefficient estimates tend to decrease in aggregate as \\ increases, individual coefficients, such as rating and income, may occasionally increase as X increases.', 
    ]),
    "\n".join([
         'The image contains three plots that visualize the relationship between wages and three different factors: age, year, and education level.\n\n1. **Left Plot (Age vs. Wage)**:\n   - **X-axis**: Age, ranging from approximately 20 to 80.\n   - **Y-axis**: Wage, ranging from 0 to 300.\n   - **Data points**: Scatter plot with gray dots representing individual data points.\n   - **Trend Line**: A blue line represents a smoothed trend. The trend shows that wages increase with age, peaking around the age of 50, and then slightly decline as age increases past 60.\n\n2. **Middle Plot (Year vs. Wage)**:\n   - **X-axis**: Year, ranging from 2003 to 2009.\n   - **Y-axis**: Wage, ranging from 0 to 300.\n   - **Data points**: Scatter plot with gray dots representing individual data points.\n   - **Trend Line**: A blue line represents a smoothed trend. The trend shows a slight increase in wages over the years from 2003 to 2009.\n\n3. **Right Plot (Education Level vs. Wage)**:\n   - **X-axis**: Education Level, with categories from 1 to 5.\n   - **Y-axis**: Wage, ranging from 0 to 300.\n   - **Box Plots**: Five box plots, each representing a different education level. \n     - The boxes represent the interquartile range (IQR), with the lower and upper edges of the boxes showing the 25th and 75th percentiles, respectively.\n     - The horizontal line within each box represents the median wage.\n     - Whiskers extend to 1.5 times the IQR from the edges of the box, and outliers are shown as individual points beyond the whiskers.\n   - **Trend**: The plots show that higher education levels are associated with higher median wages, with the highest education level (5) having the highest wages and the widest distribution.\n\nOverall, the image effectively demonstrates how wages are influenced by age, year, and education level, showing clear trends and variability within each factor.',
    ]),
    "\n".join([
        'The image consists of three scatter plots, each showing the relationship between sales and different advertising mediums: TV, Radio, and Newspaper.\n\n1. **First Scatter Plot (TV vs. Sales)**:\n   - The x-axis represents the amount spent on TV advertising.\n   - The y-axis represents sales.\n   - There is a clear positive correlation between the amount spent on TV advertising and sales, as indicated by the upward-sloping blue regression line. Higher TV advertising expenditures are generally associated with higher sales.\n   - The data points are relatively close to the regression line, suggesting a strong linear relationship.\n\n2. **Second Scatter Plot (Radio vs. Sales)**:\n   - The x-axis represents the amount spent on radio advertising.\n   - The y-axis represents sales.\n   - There is also a positive correlation between radio advertising expenditure and sales, as shown by the upward-sloping blue regression line.\n   - The data points are more scattered compared to the TV vs. Sales plot, indicating a weaker linear relationship.\n\n3. **Third Scatter Plot (Newspaper vs. Sales)**:\n   - The x-axis represents the amount spent on newspaper advertising.\n   - The y-axis represents sales.\n   - There is a weak positive correlation between newspaper advertising expenditure and sales, as shown by the slightly upward-sloping blue regression line.\n   - The data points are widely scattered around the regression line, indicating a weak or less consistent relationship between newspaper advertising and sales.\n\nOverall, the scatter plots suggest that TV advertising has the strongest positive impact on sales, followed by radio advertising, and then newspaper advertising, which shows the weakest relationship.',
    ]),
    "\n".join([
        'The image appears to be a map with a scatter plot of data points represented by two distinct colors: orange and blue. The map is divided into sections by a black boundary line and a dashed purple line.\n\n- **Orange Circles**: These are densely scattered throughout the top left region of the map, within the boundaries formed by both the black line and the dashed purple line. The orange circles are concentrated more heavily in the central area of this region and become sparser as they spread outwards.\n\n- **Blue Circles**: These circles are predominantly located in the bottom right region of the map, outside the main boundary line but within the dashed purple line. There is a noticeable cluster of blue circles along the boundary line, particularly near the bottom center of the map, with a more dispersed distribution towards the outer areas.\n\nThe black boundary line appears to outline a specific area, possibly a geographic region, while the dashed purple line might represent a larger encompassing area. The grid overlay might be used for reference or measurement purposes. The distribution of the data points suggests some form of spatial analysis, possibly indicating the occurrence or concentration of certain events, phenomena, or objects within the mapped area.',
        'KNN: K=1\n\nKNN: K=100\n\nFIGURE 2.16. <A comparison of the KNN decision boundaries (solid black curves) obtained using K = 1 and K = 100 on the data from Figure 2.13. With K = 1, the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. The Bayes decision boundary is shown as a purple dashed line.\n\nError Rate — Training Errors -| — Test Errors . T T T T T T T 0.01 0.02 0.05 0.10 0.20 0.50 1.00 0.00 41/K\n\nFIGURE 2.17. The KNN training error rate (blue, 200 observations) and test error rate (orange, 5,000 observations) on the data from Figure 2.13, as the level of flexibility (assessed using 1/K on the log scale) increases, or equivalently as the number of neighbors K decreases. The black dashed line indicates the Bayes error rate. The jumpiness of the curves is due to the small size of the training data set.\n\nvarious methods for estimating test error rates and thereby choosing the optimal level of flexibility for a given statistical learning method.\n\n2.3 Lab: Introduction to Python',
    ]),
    "\n".join([
        'The provided image consists of two three-dimensional surface plots side by side. \n\nBoth plots feature a smooth, bell-shaped surface centered on a grid plane. The surfaces exhibit a gradient color scheme that transitions from blue at the base to yellow and then to pink at the peak, indicating varying elevations.\n\nEach plot is labeled with the axes \\(X_1\\) and \\(X_2\\) along the horizontal plane, with the vertical axis representing the height of the surface. The surfaces suggest a peak in the middle, which smoothly descends towards the edges, forming a symmetric, rounded hill in both plots. The perspective is set to show a three-dimensional view, making the elevation changes clearly visible.\n\nThe main difference between the two plots is the orientation or slight variation in the shape of the surfaces, with the peak on the left plot appearing slightly more pointed compared to the peak in the right plot, which is more spread out.',
    ]),
    "\n".join([
        'By contrast, the model containing only TV as a predictor had an R? of 0.61 (Table 3.2). Adding radio to the model leads to a substantial improve- ment in R?. This implies that a model that uses TV and radio expenditures to predict sales is substantially better than one that uses only TV advertis- ing. We could further quantify this improvement by looking at the p-value for the radio coefficient in a model that contains only TV and radio as predictors.\n\nThe model that contains only TV and radio as predictors has an RSE of 1.681, and the model that also contains newspaper as a predictor has an RSE of 1.686 (Table 3.6). In contrast, the model that contains only TV has an RSE of 3.26 (Table 3.2). This corroborates our previous conclusion that a model that uses TV and radio expenditures to predict sales is much more accurate (on the training data) than one that only uses TV spending. Furthermore, given that TV and radio expenditures are used as predictors, there is no point in also using newspaper spending as a predictor in the model. The observant reader may wonder how RSE can increase when newspaper is added to the model given that RSS must decrease. In general RSE is defined as\n\nRSE = (3.25)',
        'TABLE 3.4. For the Advertising data, least squares coefficient estimates of the multiple linear regression of number of units sold on TV, radio, and newspaper advertising budgets.',
        "<table><thead><tr><th></th><th>Coefficient</th><th>Std. error</th><th>t-statistic</th><th>p-value</th></tr></thead><tbody><tr><td>Intercept</td><td>2.939</td><td>0.3119</td><td>9.42</td><td>&lt; 0.0001</td></tr><tr><td>TV</td><td>0.046</td><td>0.0014</td><td>32.81</td><td>&lt; 0.0001</td></tr><tr><td>radio</td><td>0.189</td><td>0.0086</td><td>21.89</td><td>&lt; 0.0001</td></tr><tr><td>newspaper</td><td>—0.001</td><td>0.0059</td><td>—0.18</td><td>0.8599</td></tr></tbody></table>",   
    ]),
    "\n".join([
        "8.6.2 Simple Linear Regression\n\nIn this section we will construct model matrices (also called design matri- ces) using the ModelSpec() transform from ISLP.models.\n\nWe will use the Boston housing data set, which is contained in the ISLP package. The Boston dataset records medv (median house value) for 506 neighborhoods around Boston. We will build a regression model to pre- dict medv using 13 predictors such as rmvar (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940), and lstat (percent of households with low socioeconomic status). We will use statsmodels for this task, a Python package that implements several com- monly used regression methods.\n\nWe have included a simple loading function load_data() in the ISLP pack- age: load_data()\n\nIn [8]:| Boston = load_data(\"Boston\") Boston.columns\n\nOut[8]: Index([\'crim\', \'zn\', \'indus\', \'chas\', \'nox\', \'rm\', \'age\', \'dis\', ‘\'rad\', \'tax\', \'ptratio\', \'black\', \'lstat\', \'medv\'], dtype=\'object\')\n\nType Boston? to find out more about these data.\n\nWe start by using the sm.0LS() function to fit a simple linear regression model. Our response will be medv and 1stat will be the single predictor. For this model, we can create the model matrix by hand. sm.OLS()",
        "In [9]: Out [9] : X = pd.DataFrame({'intercept': np.ones(Boston.shape[0]), 'lstat': Boston['lstat']}) X[:4] intercept Ilstat (0) 1.0 4.98 1 1.0 9.14 2 1.0 4.03 3 1.0 2.94\n\nWe extract the response, and fit the model.\n\nIn [10]: y = Boston['medv'] model = sm.OLS(y, X) results = model.fit()\n\nNote that sm.0LS() does not fit the model; it specifies the model, and then model.fit() does the actual fitting.\n\nOur ISLP function summarize() produces a simple table of the parame- ter estimates, their standard errors, t-statistics and p-values. The function takes a single argument, such as the object results returned here by the fit method, and returns such a summary.\n\n[11]: [11]: summarize (results) coef std err t Plt intercept 34.5538 0.563 61.415 0.0 lstat -0.9500 0.039 -24.528 0.0\n\nIn\n\nOut\n\nBefore we describe other methods for working with fitted models, we outline a more useful and general framework for constructing a model ma- trix X.", 
    ]),
    "\n".join([
         '<table><thead><tr><th>summarize</th><th>(results)</th><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td>coef</td><td>std err</td><td>t</td><td>P&gt;itl</td></tr><tr><td>intercept</td><td>41.6173</td><td>4.936</td><td>8.431</td><td>0.000</td></tr><tr><td>crim</td><td>-0.1214</td><td>0.033</td><td>-3.678</td><td>0.000</td></tr><tr><td>zn</td><td>0.0470</td><td>0.014</td><td>3.384</td><td>0.0014</td></tr><tr><td>indus</td><td>0.0135</td><td>0.062</td><td>0.217</td><td>0.829</td></tr><tr><td>chas</td><td>2.8400</td><td>0.870</td><td>3.264</td><td>0.0014</td></tr><tr><td>nox</td><td>-18.7580</td><td>3.851</td><td>-4.870</td><td>0.000</td></tr><tr><td>rm</td><td>3.6581</td><td>0.420</td><td>8.705</td><td>0.000</td></tr><tr><td>age</td><td>0.0036</td><td>0.013</td><td>0.271</td><td>0.787</td></tr><tr><td>dis</td><td>-1.4908</td><td>0.202</td><td>-7.394</td><td>0.000</td></tr><tr><td>rad</td><td>0.2894</td><td>0.067</td><td>4.325</td><td>0.000</td></tr><tr><td>tax</td><td>-0.0127</td><td>0.004</td><td>-3.337</td><td>0.001</td></tr><tr><td>ptratio</td><td>-0.9375</td><td>0.132</td><td>-7.091</td><td>0.000</td></tr><tr><td>lstat</td><td>-0.5520</td><td>0.051</td><td>-10.897</td><td>0.000</td></tr></tbody></table>',
    ]),
    "\n".join([
        '<table><thead><tr><th>Name</th><th>Definition</th><th>Synonyms</th></tr></thead><tbody><tr><td>False Pos. rate</td><td>FP/N |</td><td>Type I error, 1—Specificity</td></tr><tr><td>True Pos. rate</td><td>TP/P |</td><td>1—Type II error, power, sensitivity, recal</td></tr><tr><td>Pos. Pred. value</td><td>TP/P* |</td><td>Precision, 1—false discovery proportion</td></tr><tr><td>Neg. Pred. value</td><td>TN/N*</td><td></td></tr></tbody></table>',
    ]),
    "\n".join([
        'Number of Bikers\n\n100 200 300 400 500 600 Log(Number of Bikers) 0 L Hour Hour\n\nFIGURE 4.14. Left: On the Bikeshare dataset, the number of bikers is dis- played on the y-axis, and the hour of the day is displayed on the x-axis. Jitter was applied for ease of visualization. For the most part, as the mean number of bikers increases, so does the variance in the number of bikers. A smoothing spline fit is shown in green. Right: The log of the number of bikers is now displayed on the y-axis.\n\nSome of the problems that arise when fitting a linear regression model to the Bikeshare data can be overcome by transforming the response; for instance, we can fit the model\n\nP log(¥) = $0 X58; +e. j=l',
        'The provided image consists of two scatter plots, each depicting a different representation of the number of bikers over the course of a day, with the x-axis representing the hour of the day (ranging from 0 to 24 hours).\n\n**Left Scatter Plot:**\n- The y-axis indicates the actual number of bikers, ranging from 0 to 600.\n- Each blue dot represents the number of bikers counted at a specific hour.\n- There is a green smoothed line (likely a trend line) that shows the general trend of biker activity throughout the day.\n- The plot shows a low number of bikers in the early hours (midnight to around 5 AM), followed by a gradual increase starting around 5 AM.\n- The number of bikers peaks between 7 AM and 9 AM, then slightly decreases.\n- There is another peak in the late afternoon, around 5 PM, before the number of bikers decreases again as the evening progresses.\n\n**Right Scatter Plot:**\n- The y-axis indicates the logarithm (base 10) of the number of bikers, ranging from 0 to 6.\n- Each blue dot represents the logarithm of the number of bikers counted at a specific hour.\n- Similarly, a green smoothed line is shown to represent the general trend.\n- The logarithmic scale provides a clearer picture of the number of bikers during low-activity hours.\n- The trend is similar to the first plot: a low count of bikers in the early hours, a peak during the morning commute, a decline, followed by another peak in the late afternoon, and then a decline into the evening.\n\nBoth plots effectively show the daily pattern of biker activity, highlighting peak times during morning and evening commutes.', 
    ]), 
    "\n".join([
       'Sometimes we have a good idea of the variance of each response. For example, the ith response could be an average of n; raw observations. If each of these raw observations is uncorrelated with variance o?, then their average has variance 0? = o7/nj. In this case a simple remedy is to fit our model by weighted least squares, with weights proportional to the inverse variances—i.e. w; = n; in this case. Most linear regression software allows for observation weights.\n\n4. Outliers\n\nAn outlier is a point for which y; is far from the value predicted by the\n\nhetero- scedasticity\n\nweighted least squares\n\nontlien *) er\n\n26 of 5 oa o 4 g 24 go é a ©0 go 277 oO ° =z oo o G0 Mo g 4 oO 3 Og.000°. ° 3 ass og] BOF GBH oe 8 5° 8 0° 74 0 80k ° o og ‘ lO o lO ° 20 2 4 6 20 2 4 6 xX Fitted Values Fitted Values\n\n.\n\nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between —3 and 3.\n\nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.',
    ]),
    "\n".join([
       '6. Col inearity\n\nCollinearity refers to the situation in which two or more predictor varia les are closely related to one another. The concept of collinearity is illustrated in Figure 3.14 using the Credit data set. In the left-hand panel of Fig- ure 3.14, the two predictors limit and age a tionship. In contrast, in the rig ht-hand pane limit and rating are very highly correlated that they are collinear. The pr the regression context, since i vidual effects of collinear varia limit and rating tend to increa: determine how each one separa esence of col ely is associa ppear to have no obvious rela- of Figure 3.14, the predic with each other, and we say inearity can pose problems in Ors can be difficult to separate out the indi- bles on the response. In other words, since se or decrease together, it can be difficult to ed with the response, balance.\n\nFigure 3.15 illustrates some of the difficulties that can result from collinear- ity. The left-hand panel of Figure 3.15 is a contour plot of the RSS (3.22) associated with different possi le coefficien of balance on limit and age. estimates for the regression Each ellipse represents a set of coefficients\n\ncollinearity\n\no 4 o 21.8- 21.5 + my 21.25 ( r° us| 2 o _| i y 4 Y ° » | f T T T T T T T T 016 0.17 «0.18 0.19 -0.1 0.0 0.4 0.2 Buimit Buimit',
    ]),
    "\n".join([
       'In contrast, the right-hand panel of Figure 3.15 displays contour plots of the RSS associated with possible coefficient estimates for the regression of balance onto limit and rating, which we know to be highly collinear. Now the contours run along a narrow valley; there is a broad range of values for the coefficient estimates that result in equal values for RSS. Hence a small change in the data could cause the pair of coefficient values that yield the smallest RSS—that is, the least squares estimates—to move anywhere along this valley. This results in a great deal of uncertainty in the coefficient estimates. Notice that the scale for the limit coefficient now runs from roughly —0.2 to 0.2; this is an eight-fold increase over the plausible range of the limit coefficient in the regression with age. Interestingly, even though the limit and rating coefficients now have much more individual uncertainty, they will almost certainly lie somewhere in this contour valley. For example, we would not expect the true value of the limit and rating coefficients to be —0.1 and 1 respectively, even though such a value is plausible for each coefficient individually.\n\nSince collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for B; to grow. Recall that the t-statistic for each predictor is calculated by dividing 6; by its standard',
    ]),
    "\n".join([
       '<table><thead><tr><th></th><th></th><th>Coefficient</th><th>Std. error</th><th>t-statistic</th><th>p-value</th></tr></thead><tbody><tr><td rowspan="2">Model 1</td><td>Intercept</td><td>—173.411</td><td>43.828</td><td>—3.957</td><td>&lt; 0.0001</td></tr><tr><td>limit</td><td>0.173</td><td>0.005</td><td>34.496</td><td>&lt; 0.0001</td></tr><tr><td rowspan="3">Model 2.</td><td>Intercept</td><td>—377.537</td><td>45.254</td><td>—8.343</td><td>&lt; 0.0001</td></tr><tr><td>rating</td><td>2.202</td><td>0.952</td><td>2.312</td><td>0.0213</td></tr><tr><td>limit</td><td>0.025</td><td>0.064</td><td>0.384</td><td>0.7012</td></tr></tbody></table>',
    ]),
    "\n".join([
      'power\n\nmulti-\n\ncollinearity\n\nvariance\n\ninflation\n\nfactor\n\nIn the Credit data, a regression of balance on age, rating, and limit indicates that the predictors have VIF values of 1.01, 160.67, and 160.59. As we suspected, there is considerable collinearity in the data!\n\nWhen faced with the problem of collinearity, there are two simple solu- ions. The first is to drop one of the problematic variab variable provides about the response is redundant in without the rating predictor, then the resulting VIF he minimum possible value of 1, and the R? drops les from the regres- sion. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this he presence of the other variables. For instance, if we regress balance onto age and limit, values are close to from 0.754 to 0.75. So dropping rating from the set of predictors has effectively solved the collinearity problem without compromising the fit. The second solution is o combine the collinear variables together into a single predictor. For in- stance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.',
    ]),
    "\n".join([
      'power\n\nmulti-\n\ncollinearity\n\nvariance\n\ninflation\n\nfactor\n\nIn the Credit data, a regression of balance on age, rating, and limit indicates that the predictors have VIF values of 1.01, 160.67, and 160.59. As we suspected, there is considerable collinearity in the data!\n\nWhen faced with the problem of collinearity, there are two simple solu- ions. The first is to drop one of the problematic variab variable provides about the response is redundant in without the rating predictor, then the resulting VIF he minimum possible value of 1, and the R? drops les from the regres- sion. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this he presence of the other variables. For instance, if we regress balance onto age and limit, values are close to from 0.754 to 0.75. So dropping rating from the set of predictors has effectively solved the collinearity problem without compromising the fit. The second solution is o combine the collinear variables together into a single predictor. For in- stance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness.',
    ]),
    "\n".join([
       "In\n\n[3]: from statsmodels.stats.outliers_influence \\ import variance_inflation_factor as VIF from statsmodels.stats.anova import anova_lm\n\nAs one of the import statements above is quite a long line, we inserted a line break \\ to ease readability.\n\nWe will also use some functions written for the labs in this book in the ISLP package.\n\nIn [4]: from ISLP import load_data from ISLP.models import (ModelSpec as MS, summarize, poly)\n\nInspecting Objects and Namespaces\n\nThe function dir() provides a list of objects in a namespace.\n\nIn [5]:\n\ndir()\n\nOut [5]: ('In', _builtin__' _builtins__',\n\nnamespace\n\nstatsmodels\n\ndir()\n\n117\n\n‘poly', ‘quit', 'sm', ‘summarize']\n\nThis shows you everything that Python can find at the top level. There are certain objects like __builtins__ that contain references to built-in functions like print ().\n\nEvery python object has its own notion of namespace, also accessible with dir(). This will include both the attributes of the object as well as any methods associated with it. For instance, we see 'sum' in the listing for an array.\n\nIn (6]:|A = np.array([3,5,11]) dir(A)\n\nOut[6]: ... 'strides', ‘sum', ‘swapaxes',\n\nThis indicates that the object A.sum exists. In this case it is a method that can be used to compute the sum of the array A as can be seen by typing A.sum?.\n\nIn [7]: | A.sum()\n\nOut [7]: 19",
    ]),
    "\n".join([
       'intercept 1.0\n\nlstat\n\n5\n\n1\n\n1.0\n\n10\n\n2\n\n1.0\n\n15\n\nNext we compute the predictions at newX, and view them by extracting the predicted_mean attribute.\n\nIn [17]:| new_predictions = results.get_prediction(newX) ; new_predictions.predicted_mean\n\nOut [17]: array ([29.80359411, 25.05334734, 20.30310057])\n\nWe can produce confidence intervals for the predicted values.\n\nIn [18]: | new_predictions.conf_int (alpha=0.05)\n\nOut [18]: array ([[29.00741194, (24.47413202, (19.73158815,\n\n30.59977628]\n\n25.63256267], 20.87461299]])\n\n,\n\nPrediction intervals are computing by setting obs=True:\n\nIn [19]: | new_predictions.conf_int(obs=True, alpha=0.05)\n\nOut [19]: array ([[17.56567478, 42.04151344], (12.82762635, 37.27906833] , { 8.0777421 , 32.52845905]])\n\nFor instance, the 95% confidence interval associated with an lstat value of 10 is (24.47, 25.63), and the 95% prediction interval is (12.82, 37.28). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of 25.05 for medv when 1stat equals 10), but the latter are substantially wider.\n\nNext we will plot medv and 1stat using DataFrame.plot.scatter(), and plot wish to add the regression line to the resulting plot. scatter\n\n()',
    ]),
    "\n".join([
      'Scenario 6: The observations were generated from a normal distribution with a different diagonal covariance matrix for each class. However, the sample size was very small: just n = 6 in each class. Naive Bayes performed very well, because its assumptions are met. LDA and logistic regression performed poorly because the true decision boundary is non-linear, due to the unequal covariance matrices. QDA performed a bit worse than naive Bayes, because given the very small sample size, the former incurred too much variance in estimating he correlation between the predictors within each class. KNN’s performance also suffered due to the very small sample size.\n\nThese six examples illustrate that no one method will dominate the oth- ers in every situation. When the true decision boundaries are the LDA and logistic regression approaches will inear, then end to perform well. When the boundaries are moderately non-linear, QDA or naive Bayes may give better results. Finally, for much more complicated decision boundaries, non-parametric approach such as KNN can be superior. But smoothness for a non-parametric approach mus he level of be chosen carefully. In the next chapter we examine a number of approaches for choosing the correct level of smoothness and, in general, for selecting the best overall method.',
    ]),
    "\n".join([
       '4.6 Generalized Linear Models\n\nIn Chapter 3, we assumed that the response Y is quantitative, and ex- plored the use of least squares linear regression to predict Y. Thus far in this chapter, we have instead assumed that Y is qualitative. However, we may sometimes be faced with situations in which Y is neither qualitative nor quantitative, and so neither linear regression from Chapter 3 nor the classification approaches covered in this chapter is applicable.\n\nAs a concrete example, we consider the Bikeshare data set. The response is bikers, the number of hourly users of a bike sharing program in Wash- ington, DC. This response value is neither qualitative nor quantitative: instead, it takes on non-negative integer values, or counts. We will consider predicting bikers using the covariates mth (month of the year), hr (hour of the day, from 0 to 23), workingday (an indicator variable that equals 1 if it is neither a weekend nor a holiday), temp (the normalized temperature, in Celsius), and weathersit (a qualitative variable that takes on one of four possible values: clear; misty or cloudy; light rain or light snow; or heavy rain or heavy snow.)\n\nIn the analyses that follow, we will treat mnth, hr, and weathersit as qualitative variables.',
       '4.6.1 Linear Regression on the Bikeshare Data\n\nTo begin, we consider predicting bikers using linear regression. The results are shown in Table 4.10.\n\nWe see, for example, that a progression of weather from clear to cloudy results in, on average, 12.89 fewer bikers per hour; however, if the weather progresses further to rain or snow, then this further results in 53.60 fewer bikers per hour. Figure 4.13 displays the coefficients associated with mith\n\ncounts\n\n2 84 & 2 & 4 \\ 5 84 Bo Bg @ 87 3 6 oF & 4 8 4 7 TOT TT TT TT TT TT T T T T JFMAMJJASOND 5 10 15 20 Month Hour',
       'Furthermore, it is reasonable to suspect that when the expected value of bikers is small, the variance of bikers should be small as well. For instance, at 2 AM during a heavy December snow storm, we expect that extremely few people will use a bike, and moreover that there should be little variance associated with the number of users during those conditions. This is borne out in the data: between 1 AM and 4 AM, in December, January, and February, when it is raining, there are 5.05 users, on average, with a standard deviation of 3.73. By contrast, between 7 AM and 10 AM, in April, May, and June, when skies are clear, there are 243.59 users, on average, with a standard deviation of 131.7. The mean-variance relationship is displayed in the left-hand panel of Figure 4.14. This is a major violation of the assumptions of a linear model, which state that Y = ve Xj Bj +e, where € is a mean-zero error term with variance a? that is constant, and not a function of the covariates. Therefore, the heteroscedasticity of the data calls into question the suitability of a linear regression model.\n\nFinally, the response bikers is integer-valued. But under a linear model, Y=Po+ vie Xj8; +, where ¢ is a continuous-valued error term. This means that in a linear model, the response Y is necessarily continuous- valued (quantitative). Thus, the integer nature of the response bikers sug- gests that a linear regression model is not entirely satisfactory for this data set.',
       'Number of Bikers\n\n100 200 300 400 500 600 Log(Number of Bikers) 0 L Hour Hour\n\nFIGURE 4.14. Left: On the Bikeshare dataset, the number of bikers is dis- played on the y-axis, and the hour of the day is displayed on the x-axis. Jitter was applied for ease of visualization. For the most part, as the mean number of bikers increases, so does the variance in the number of bikers. A smoothing spline fit is shown in green. Right: The log of the number of bikers is now displayed on the y-axis.\n\nSome of the problems that arise when fitting a linear regression model to the Bikeshare data can be overcome by transforming the response; for instance, we can fit the model\n\nP log(¥) = $0 X58; +e. j=l',
       'Transforming the response avoids the possibility of negative predictions, and it overcomes much of the heteroscedasticity in the untransformed data, as is shown in the right-hand panel of Figure 4.14. However, it is not quite a satisfactory solution, since predictions and inference are made in terms of the log of the response, rather than the response. This leads to challenges in interpretation, e.g. “a one-unit increase in Xj is associated with an increase in the mean of the log of Y by an amount 8, ”. Furthermore, a log transformation of the response cannot be applied in settings where the response can take on a value of 0. Thus, while fitting a linear model to a transformation of the response may be an adequate approach for some count-valued data sets, it often leaves something to be desired. We will see in the next section that a Poisson regression model provides a much more natural and elegant approach for this task.\n\n4.6.2 Poisson Regression on the Bikeshare Data\n\nTo overcome the inadequacies of linear regression for analyzing the Bikeshare data set, we will make use of an alternative approach, called Poisson regression. Before we can talk about Poisson regression, we must first in- troduce the Poisson distribution.\n\nSuppose that a random variable Y takes on nonnegative integer values, ie. Y € {0,1,2,...}. If Y follows the Poisson distribution, then\n\nek k} Pr(Y =k)= for k = 0,1,2,.... (4.35)\n\n@>'
    ]),
    "\n".join([
      'TABLE 4.11. Results for a Poisson regression model fit to predict bikers in the Bikeshare data. The predictors mnth and hr are omitted from this table due to space constraints, and can be seen in Figure 4.15. For the qualitative variable weathersit, the baseline corresponds to clear skies.\n\n0.0 L ~e -04 -0.2 L L Coefficient 2 1 Ll _* Month Hour',
      '<table><thead><tr><th></th><th>Coefficient</th><th>Std. error</th><th>z-statistic</th><th>p-value</th></tr></thead><tbody><tr><td>Intercept</td><td>4.12</td><td>0.01</td><td>683.96</td><td>0.00</td></tr><tr><td>workingday</td><td>0.01</td><td>0.00</td><td>7.5</td><td>0.00</td></tr><tr><td>temp</td><td>0.79</td><td>0.01</td><td>68.43</td><td>0.00</td></tr><tr><td>weathersit [cloudy/misty]</td><td>-0.08</td><td>0.00</td><td>-34.53</td><td>0.00</td></tr><tr><td>weathersit [light rain/snow]</td><td>-0.58</td><td>0.00</td><td>-141.91</td><td>0.00</td></tr><tr><td>weathersit [heavy rain/snow]</td><td>-0.93</td><td>0.17</td><td>-5.55</td><td>0.00</td></tr></tbody></table>',
    ]),
    "\n".join([
       'Coefficient\n\nFIGURE 4.15. A Poisson regression model was fit to predict bikers in the Bikeshare data set. Left: The coefficients associated with the month of the year. Bike usage is highest in the spring and fall, and lowest in the winter. Right: The coefficients associated with the hour of the day. Bike usage is highest during peak commute times, and lowest overnight.\n\nand lowest during the winter and in the early morning hours. Moreover, bike usage increases as the temperature increases, and decreases as the weather worsens. Interestingly, the coefficient associated with workingday is statistically significant under the Poisson regression model, but not under the linear regression model.\n\nSome important distinctions between the Poisson regression model and the linear regression model are as follows:\n\n« Interpretation: To interpret the coefficients in the Poisson regression model, we must pay close attention to (4.37), which states that an increase in Xj by one unit is associated with a change in E(Y) = A by a factor of exp(S;). For example, a change in weather from clear to cloudy skies is associated with a change in mean bike usage by a factor of exp(—0.08) = 0.923, i.e. on average, only 92.3% as many people will use bikes when it is cloudy relative to when it is clear. If the weather worsens further and it begins to rain, then the mean bike usage will further change by a factor of exp(—0.5) = 0.607, ie. on average only 60.7% as many people will use bikes when it is rainy relative to when it is cloudy.',
        '¢ Mean-variance relationship: As mentioned earlier, under the Poisson model, \\ = E(Y) = Var(Y). Thus, by modeling bike usage with a Poisson regression, we implicitly assume that mean bike usage in a given hour equals the variance of bike usage during that hour. By contrast, under a linear regression model, the variance of bike usage always takes on a constant value. Recall from Figure 4.14 that in the Bikeshare data, when biking conditions are favorable, both the mean and the variance in bike usage are much higher than when conditions are unfavorable. Thus, the Poisson regression model is able to handle the mean-variance relationship seen in the Bikeshare data in a way that the linear regression model is not.°\n\n¢ nonnegative fitted values: There are no negative predictions using the Poisson regression model. This is because the Poisson model itself only allows for nonnegative values; see (4.35). By contrast, when we fit a linear regression model to the Bikeshare data set, almost 10% of the predictions were negative.',
    ]),
    "\n".join([
      "exponential family\n\nexponential\n\nGamma\n\nnegative binomial\n\ngeneralized linear model\n\nfrom sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression\n\nNow we are ready to load the Smarket data.\n\nIn [3]: Smarket = load_data('Smarket') Smarket\n\nThis gives a truncated listing of the data, which we do not show here. We can see what the variable names are.\n\nIn [4]: Smarket .columns\n\nOut [4]: Index(['Year', 'Lagi', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume', 'Today', 'Direction'], dtype='object')\n\nWe compute the correlation matrix using the corr() method for data frames, which produces a matrix that contains all of the pairwise corre- lations among the variables. (We suppress the output here.) The pandas library does not report a correlation for the Direction variable because it is qualitative.\n\nIn [5]: Smarket .corr()\n\nAs one would expect, the correlations between the lagged return variables and today’s return are close to zero. The only substantial correlation is between Year and Volume. By plotting the data we see that Volume is in- creasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.\n\nIn [6]: Smarket.plot (y='Volume') ;",
    ]),
    "\n".join([
      "4.7.2 Logistic Regression\n\nNext, we will fit a logistic regression model in order to predict Direction using Lagi through Lag5 and Volume. The sm.GLM() function fits gener- alized linear models, a class of models that includes logistic regression. Alternatively, the function sm.Logit() fits a logistic regression model di- rectly. The syntax of sm.GLM() is similar to that of sm.0LS(), except that we must pass in the argument family=sm.families.Binomial() in order to tell statsmodels to run a logistic regression rather than some other type of generalized linear model.\n\nIn\n\n[7]: allvars = Smarket.columns.drop(['Today', 'Direction', 'Year']) design = MS(allvars) X = design. fit_transform(Smarket) y = Smarket.Direction == 'Up' glm = sm.GLM(y, XxX, family=sm.families.Binomial ()) results = glm.fit() summarize (results)\n\ncorr()\n\nsm.GLM() generalized linear model\n\n4.7 Lab: Logistic Regression, LDA, QDA, and KNN 175\n\nOut [7]:",
    ]),
    "\n".join([
       "train_test_ split()\n\nindexable\n\nrate\n\nOut [58] : Truth No Yes Predicted No 880 58 Yes 53 9\n\nIt turns out that KNN with A = 1 does far better than random guessing among the customers that are predicted to buy insurance. Among 62 such customers, 9, or 14.5%, actually do purchase insurance. This is double the rate that one would obtain from random guessing.\n\nIn [59] : | 9/(53+9)\n\nOut [59]: 0.145\n\nTuning Parameters\n\nThe number of neighbors in KNN is referred to as a tuning parameter, also referred to as a hyperparameter. We do not know a priori what value to use. It is therefore of interest to see how the classifier performs on test data as we vary these parameters. This can be achieved with a for loop, described in Section 2.3.8. Here we use a for loop to look at the accuracy of our classifier in the group predicted to purchase insurance as we vary the number of neighbors from 1 to 5:\n\nIn [60]:|\n\nfor K in range(1,6): knn = KNeighborsClassifier(n_neighbors=K) knn_pred = knn.fit(X_train, y_train) .predict (X_test) C = confusion_table(knn_pred, y_test) templ = ('K={0:d}: # predicted to rent: {1:>2},' + ' # who did rent {2:d}, accuracy {3:.1%}') pred = C.loc['Yes'].sum() did_rent = C.loc['Yes','Yes'] print (templ. format ( K, pred, did_rent, did_rent / pred))",
    ]),
    "\n".join([
      'Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an\n\n?Though forward stepwise selection considers p(p + 1)/2 +1 models, it performs a guided search over model space, and so the effective model space considered contains substantially more than p(p + 1)/2 + 1 models.\n\nbackward stepwise selection\n\nefficient alternative to best subset selection. However, unlike forward step- wise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at- a-time. Details are given in Algorithm 6.3.\n\nAlgorithm 6.3 Backward stepwise selection\n\n1. Let M, denote the full model, which contains all p predictors.\n\n2. Fork =p,p—1,...,1:\n\n(a) Consider all k models that contain all but one of the predictors in Mg, for a total of k — 1 predictors.\n\n(b) Choose the best among these k models, and call it M,—1. Here best is defined as having smallest RSS or highest R?.\n\n3. Select a single best model from among Mo,...,M, using the pre- diction error on a validation set, C, (AIC), BIC, or adjusted R?. Or use the cross-validation method.\n\nLike forward stepwise selection, the backward selection approach searches through only 1+ p(p+1)/2 models, and so can be applied in settings where p is too large to apply best subset selection.’ Also like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the p predictors.',
    ]),
    "\n".join([
      'Comparing the Lasso and Ridge Regression\n\nIt is clear that the lasso has a major advantage over ridge regression, in that it produces simpler and more interpretable models that involve only a subset of the predictors. However, which method leads to better prediction accuracy? Figure 6.8 displays the variance, squared bias, and test MSE of the lasso applied to the same simulated data as in Figure 6.5. Clearly the lasso leads to qualitatively similar behavior to ridge regression, in that as increases, the variance decreases and the bias increases. In the right-hand\n\n50 L 50 L 40 L 40 L 20 L 20 L 10 L Squared 30 fi Mean Squared Error 30 fi 10 L TTT 11 1 1 1 T T T 002 0.10 050 200 10.00 50.00 0.0 0.2 04 0.6 08 1.0 d R? on Training Data\n\nError',
    ]),
    "\n".join([
       'An Overview of Principal Components Analysis\n\nPCA is a technique for reducing the dimension of an n x p data matrix X. The first principal component direction of the data is that along which the observations vary the most. For instance, consider Figure 6.14, which shows population size (pop) in tens of thousands of people, and ad spending for a particular company (ad) in thousands of dollars, for 100 cities.° The green solid line represents the first principal component direction of the data. We can see by eye that this is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance; projecting the observations onto any other line would yield projected observations with lower variance. Projecting a point onto a line simply involves finding the location on the line which is closest to the point.\n\nThe first principal component is displayed graphically in Figure 6.14, but how can it be summarized mathematically? It is given by the formula\n\nZ, = 0.839 x (pop — pop) + 0.544 x (ad — ad). (6.19)',
      'PCA provides a tool to do just this. It finds a low-dimensional represen- tation of a data set that contains as much as possible of the variation. The idea is that each of the n observations lives in p-dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of in- teresting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a linear combination of the p features. We now explain the manner in which these dimensions, or principal components, are found.\n\nThe first principal component of a set of features X,,X2,...,Xp is the normalized linear combination of the features\n\nZ = uX1 + bX +-++ + bpiXp (12.1)\n\nthat has the largest variance. By normalized, we mean that een oy =1. We refer to the elements ¢11,...,@p1 as the loadings of the first principal component; together, the loadings make up the principal component load- ing vector, $1 = (¢11 a1 ... pi)". We constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.',
      '12.2. Principal Components Analysis\n\nPrincipal components are discussed in Section 6.3.1 in the context of principal components regression. When faced with a large set of corre- lated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. The principal component directions are presented in Section 6.3.1 as directions in feature space along which the original data are highly variable. These directions also define lines and subspaces that are as close as possible to the data cloud. To perform principal components regression, we simply use principal components as predictors in a regression model in place of the original larger set of vari- ables.\n\nPrincipal components analysis (PCA) refers to the process by which prin- cipal components are computed, and the subsequent use of these compo- nents in understanding the data. PCA is an unsupervised approach, since it involves only a set of features X,, X2,..., Xp, and no associated response Y. Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization (visualization of',
    ]),
    "\n".join([
      'Step 1: Define the Null and Alternative Hypotheses\n\nIn hypothesis testing, we divide the world into two possibilities: the hypothesis and the alternative hypothesis. The hypothesis, denoted Ho, is the default state of belief about the world.* For instance, hypotheses associated with the two questions posed earlier in this chapter are as follows:\n\n1. The true coefficient 3; in a linear regression of Y onto X1,...,Xp equals zero.\n\n2. There is no difference between the expected blood pressure of mice in the control and treatment groups.\n\nThe hypothesis is boring by construction: it may well be true, but we might hope that our data will tell us otherwise.\n\nThe alternative hypothesis, denoted H,, represents something different and unexpected: for instance, that there is a difference between the ex- pected blood pressure of the mice in the two groups. Typically, the alter- native hypothesis simply posits that the hypothesis does not hold: if the hypothesis states that there is no difference between A and B, then the alternative hypothesis states that there is a difference between A and B.',
      'It is important to note that the treatment of Hp and H, is asymmetric. Ho is treated as the default state of the world, and we focus on using data to reject Ho. If we reject Ho, then this provides evidence in favor of Hy. We can think of rejecting Hp as making a discovery about our data: namely, we are discovering that Hp does not hold! By contrast, if we fail to reject Ho, then our findings are more nebulous: we will not know whether we failed to reject Hp because our sample size was too small (in which case testing Ho again on a larger or higher-quality dataset might lead to rejection), or whether we failed to reject Ho because Hp really holds.\n\nStep 2: Construct the Test Statistic\n\nNext, we wish to use our data in order to find evidence for or against the hypothesis. In order to do this, we must compute a test statistic, denoted T, which summarizes the extent to which our data are consistent with Ho. The way in which we construct T depends on the nature of the hypothesis that we are testing.\n\nTo make things concrete, let xj,..., vw denote the blood pressure mea- surements for the n, mice in the treatment group, and let 2{,...,2%,, denote the blood pressure measurements for the n, mice in the control group, and be = E(X*), we = E(X°). To test Ho : ut = fe, we make use of a two-sample t-statistic,’ defined as\n\n4H is pronounced “H naught” or “H zero”.',
    ]),
    "\n".join([
       'TABLE 13.1. A summary of the possible scenarios associated with testing the hypothesis Ho. Type I errors are also known as false positives, and Type IT errors as false negatives.\n\nwhat value of the test statistic shou what a p-value gives us. In other words, a p-value allows us to trans: our test statistic, which is measured on some arbitrary and uninterpreta scale, into a number between 0 and d be expected, under Ho. This is exac’ 1 that can be more easily interpre ly orm le ed.\n\nStep 4: Decide Whether to Reject he Null Hypothesis\n\nOnce we have computed a p-value corresponding to Ho, it remains us to decide whether or not to reject Ho. (We do not usually talk a “accepting” Ho: instead, we talk a value indicates that such a large value of the test statistic is unli occur under Ho, and thereby provides evidence against Ho. If the is sufficiently small, then we will want to reject Ho (and, there: a “discovery”). But how small is small enough to reject Ho? y or out out “failing to reject” Hp.) A small p- to p-val fore, make ue\n\nIt turns out that the answer to this question is very much in he eyes of the beholder, or more specifically, the data analyst. The smaller the p- value, the stronger the evidenc reject Ho if the p-value is below expect to see such a small p-val! e against Ho. In some fields, it is typical to 0.05; this means that, if Hp holds, we would ue no more than 5% of the time.° However, in other fields, a much higher burden of proof is required: for example, in some areas of physics, it is typical to reject Ho only if the p-value is below 10-°!',
        'In the example displayed in Figure 13.1, if we use a threshold of 0.05 as our cut-off for rejecting the nul contrast, if we use a threshold These ideas are formalized in t hypothesis, then we will reject the null. By of 0.01, then we will fail to reject the null. he next section.\n\n13.1.2 Type I and Type II Errors\n\nIf the hypothesis holds, then we say that it is a true hypothesis; otherwise, it is a false hypothesis. For instance, if we test Ho : tt = [Me as in Section 13.1.1, and there is indeed no difference in the population mean blood pressure for mice in the treatment group and mice in the control group, then Hp is rue; otherwise, it is false. Of course, we do not know a priori whether Ho is true or whether it is false: this is why we need to conduct a hypothesis test!\n\n°Though a threshold of 0.05 to reject Ho is ubiquitous in some areas of science, we advise against blind adherence to this arbitrary choice. Furthermore, a data analyst. should typically report the p-value itself, rather than just whether or not it exceeds a specified threshold value.',
    ]),
    "\n".join([
       'It is worth noting that unlike p-values, for which a threshold of 0.05 is typically viewed as the minimum standard of evidence for a “positive” result, and a threshold of 0.01 or even 0.001 is viewed as much more com- pelling, there is no standard accepted threshold for FDR control. Instead, the choice of FDR threshold is typically context-dependent, or even dataset- dependent. For instance, the genomic researcher in the previous example might seek to control the FDR at a threshold of 10% if the planned follow-\n\n151f R = 0, then we replace the ratio V/R with 0, to avoid computing 0/0. Formally, FDR = E(V/R|R > 0) Pr(R > 0).\n\nup analysis is time-consuming or expensive. Alternatively, a much larger threshold of 30% might be suitable if she plans an inexpensive follow-up analysis.\n\n13.4.2 The Benjamini—Hochberg Procedure\n\nWe now focus on the task of controlling the FDR: that is, deciding which hypotheses to reject while guaranteeing that the FDR, E(V/R), is less than or equal to some pre-specified value qg. In order to do this, we need some way to connect the p-values, p1,...,Pm, from the m hypotheses to the desired FDR value, q. It turns out that a very simple procedure, outlined in Algorithm 13.2, can be used to control the FDR.\n\nAlgorithm 13.2 Benjamini-Hochberg Procedure to Control the FDR\n\n1. Specify q, the level at which to control the FDR.\n\n2. Compute p-values, pi,..., Pm, for the m hypotheses\n\n3. Order the m p-values so that pi) < pz) S +++ < Pim):\n\n4. Define',
        'L=max{j : py) < qj/m}.\n\n(13.10)\n\n5. Reject all hypotheses Ho; for which p; < pz).\n\nAlgorithm 13.2 is known as the Benjamini—Hochberg procedure. The crux of this procedure lies in (13.10). For example, consider again the first five managers in the Fund dataset, presented in Table 13.3. (In this example, m = 5, although typically we control the FDR in settings involving a much greater number of hypotheses.) We see that pi1) = 0.006 < 0.05 x 1/5, D2) = 0.012 < 0.05 x 2/5, pis) = 0.601 > 0.05 x 3/5, pray = 0.756 > 0.05 x 4/5, and pis) = 0.918 > 0.05 x 5/5. Therefore, to control the FDR at 5%, we reject the hypotheses that the first and third fund managers perform no better than chance.\n\nAs long as the m p-values are independent or only mildly dependent, then the Benjamini-Hochberg procedure guarantees!® that\n\nFDR <q.\n\nIn other words, this procedure ensures that, on average, no more than a fraction q of the rejected hypotheses are false positives. Remarkably, this holds regardless of how many hypotheses are true, and regardless of the distribution of the p-values for the hypotheses that are false. Therefore, the Benjamini—Hochberg procedure gives us a very easy way to determine, given a set of m p-values, which hypotheses to reject in order to control the FDR at any pre-specified level q.\n\n16 However, the proof is well beyond the scope of this book.',
        'There is a fundamental difference between the Bonferroni procedure of Section 13.3.2 and the Benjamini-Hochberg procedure. In the Bonferroni procedure, in order to control the FWER for m hypotheses at level a, we must simply reject hypotheses for which the p-value is below a/m. This threshold of a/m does not depend on anything about the data (beyond the value of m), and certainly does not depend on the p-values themselves. By contrast, the rejection threshold used in the Benjamini— Hochberg procedure is more complicated: we reject all hypotheses for which the p-value is less than or equal to the Lth smallest p-value, where L is itself a function of all m p-values, as in (13.10). Therefore, when con- ucting the Benjamini—Hochberg procedure, we cannot plan out in advance what threshold we will use to reject p-values; we need to first see our data. For instance, in the abstract, there is no way to know whether we will reject a hypothesis corresponding to a p-value of 0.01 when using an FDR threshold of 0.1 with m = 100; the answer depends on the values of the other m— 1 p-values. This property of the Benjamini—Hochberg procedure is shared by the Holm procedure, which also involves a data-dependent p-value threshold.',
    ]),
    "\n".join([
       '<table><thead><tr><th>Linkage</th><th>Description</th></tr></thead><tbody><tr><td>Complete</td><td>Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dis- similarities.</td></tr><tr><td>Single</td><td>Minimal intercluster dissimilarity. Compute all pairwise dis- similarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.</td></tr><tr><td>Average</td><td>Mean intercluster dissimilarity. Compute all pairwise dis- similarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.</td></tr><tr><td>Centroid</td><td>Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.</td></tr></tbody></table>',
     'TABLE 12.3. A summary of the four most commonly-used types of linkage in hierarchical clustering.\n\nof the hierarchical clustering algorithm will depend on the type of linkage used, as well as on the choice of dissimilarity measure. Hence, the resulting dendrogram typically depends quite strongly on the type of linkage used, as is shown in Figure 12.14.\n\nChoice of Dissimilarity Measure\n\nThus far, the examples in this chapter have used Euclidean distance as the dissimilarity measure. But sometimes other dissimilarity measures might be preferred. For example, correlation-based distance considers two obser- vations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance. This is an unusual use of correlation, which is normally computed between vari- ables; here it is computed between the observation profiles for each pair of observations. Figure 12.15 illustrates the difference between Euclidean and correlation-based distance. Correlation-based distance focuses on the shapes of observation profiles rather than their magnitudes.\n\nThe choice of dissimilarity measure is very important, as it has a strong effect on the resulting dendrogram. In general, careful attention should be paid to the type of data being clustered and the scientific question at hand. These considerations should determine what type of dissimilarity measure is used for hierarchical clustering.',
    ]),
    "\n".join([
       'We have described some of the issues ciated with clustering. However, clustering can be a very useful and valid statistical tool if used properly. We mentioned that small decisions in how clustering is performed, such as how the data are standardized and what type of linkage is used, can have a large effect on the results. Therefore, we recommend performing clustering with different choices of these parameters, and looking at the full set of results in order to see what patterns consistently emerge. Since clustering can be non-robust, we recommend clustering subsets of the data in order to get a sense of the robustness of the clusters obtained. Most importantly, we must be careful about how the results of a clustering analysis are reported. These results should not be taken as the absolute truth about a data set. Rather, they should constitute a starting point for the development of a scientific hypothesis and further study, preferably on an independent data set. w',
    ]),
    "\n".join([
      'TABLE 10.1. Test error rate on the MNIST data, for neural networks with two forms of regularization, as well as multinomial logistic regression and linear dis- criminant analysis. In this example, the extra complexity of the neural network leads to a marked improvement in test error.\n\nFIGURE 10.5. A sample of images from the CIFAR100 database: a collection of natural images from everyday life, with 100 different classes represented.\n\nAdding the number of coefficients in W1, W2 and B, we get 235,146 in all, more than 33 times the number 785 x 9 = 7,065 needed for multinomial logistic regression. Recall that there are 60,000 images in the training set. While this might seem like a large training set, there are almost four times as many coefficients in the neural network model as there are observations in the training set! To avoid overfitting, some regularization is needed. In this example, we used two forms of regularization: ridge regularization, which is similar to ridge regression from Chapter 6, and dropout regularization. We discuss both forms of regularization in Section 10.7.',
       '<table><thead><tr><th>Method</th><th>Test Error</th></tr></thead><tbody><tr><td>Neural Network + Ridge Regularization</td><td>2.3%</td></tr><tr><td>Neural Network + Dropout Regularization</td><td>1.8%</td></tr><tr><td>Multinomial Logistic Regression</td><td>7.2%</td></tr><tr><td>Linear Discriminant Analys</td><td>12.7%</td></tr></tbody></table>',
    ]),
    "\n".join([
      '°This used to be called weight sharing in the early years of neural networks.\n\n¢ If we use K different convolution filters at this first hidden layer, we get K two-dimensional output feature maps, which together are treated as a single three-dimensional feature map. We view each of the K output feature maps as a separate channel of information, so now we have A channels in contrast to the three color channels of the original input feature map. The three-dimensional feature map is just like the activations in a hidden layer of a simple neural network, except organized and produced in a spatially structured way.\n\n« We typically apply the ReLU activation function (10.5) to the con- volved image. This step is sometimes viewed as a separate layer in the convolutional neural network, in which case it is referred to as a detector layer.\n\n10.8.2 Pooling Layers\n\nA pooling layer provides a way to condense a large image into a smaller summary image. While there are a number of possible ways to perform pooling, the maz pooling operation summarizes each non-overlapping 2 x 2 block of pixels in an image using the maximum value in the block. This reduces the size of the image by a factor of two in each direction, and it also provides some location invariance: i.e. as long as there is a large value in one of the four pixels in the block, the whole block registers as a large value in the reduced image.\n\nHere is a simple example of max pooling:\n\n3 4 2 4}° 0 on Max pool rPNwWr HHOW Nwe',
    ]),

]

answers = [

]

contexts = [

]



if __name__ == '__main__':
  pass 
 
    