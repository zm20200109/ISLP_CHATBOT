{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"R52mFCzHdENj"},"outputs":[],"source":["import transformers\n","import torch\n","import os\n","from transformers import AutoTokenizer, AutoModelForCausalLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cB4pI2LDdENk"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_ollama.llms import OllamaLLM\n","from langchain_openai.llms import OpenAI\n","from langchain_openai.chat_models import ChatOpenAI\n","from langchain_openai.embeddings import OpenAIEmbeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YF6FnULdENl"},"outputs":[],"source":["from dotenv import  load_dotenv\n","load_dotenv()\n","\n","OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAXzCD7CdENl"},"outputs":[],"source":["import os\n","import pdfminer\n","#from pdfminer.utils import  open_filename\n","from unstructured.partition.pdf import partition_pdf\n","from unstructured.staging.base import elements_to_json\n","import pytesseract\n","os.environ['PATH'] += r';C:\\Program Files\\Tesseract-OCR' # dodajem tesseract u Path varijable\n","from pdf_parsing_and_chunking import parse_pdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pXdKZ0WdENl"},"outputs":[],"source":["chunks = parse_pdf() # ovde je parsiran tekst"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPDk9Aq_dENl"},"outputs":[],"source":["import pickle\n","\n","#with open(\"chunks.pkl\",\"wb\") as file:\n","#    pickle.dump(chunks,file)\n","\n","with open('results/chunked_elements_1.pkl', 'rb') as file: # test_chunks.pkl, test_chunks_max_1600.pkl\n","    chunks = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UplMcTB2dENm","outputId":"bc7b1779-1d8a-43f2-a45a-57eecd4f65f4"},"outputs":[{"data":{"text/plain":["1310"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["len(chunks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWhRuwB7dENm"},"outputs":[],"source":["raw_pdf_elements = chunks"]},{"cell_type":"markdown","metadata":{"id":"QUk3QOnMdENm"},"source":["**Set of categories from pdf document:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCB5j-nqdENn"},"outputs":[],"source":["from collections import defaultdict\n","categories = defaultdict(int)\n","for chunk in raw_pdf_elements:\n","    category = chunk.category\n","    categories[category] += 1\n","\n","set_of_categories = set(categories.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5gISYxndENn","outputId":"445a4a2d-9388-4955-af74-17cac53a4093"},"outputs":[{"data":{"text/plain":["defaultdict(int, {'CompositeElement': 1206, 'Table': 104})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["categories"]},{"cell_type":"markdown","metadata":{"id":"LLMsBu-wdENn"},"source":["**Now we will create DATA MODEL with 2 fields: first one is type - type of pdf documents' element and text - text from element.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKWIgZt4dENo"},"outputs":[],"source":["from pydantic import BaseModel\n","from typing import Any\n","from pydantic import Field\n","\n","class Element(BaseModel):\n","    type:str = Field(description=\"Type of pdf documents' element.\")\n","    text:Any = Field(description=\"This is text content of pdf document's element\")\n","    page_no:int = Field(description=\"Page number of the original document from where chunk belongs.\")\n","\n","table_elements = list()\n","text_elements = list()\n","\n","for element in raw_pdf_elements:\n","    if \"CompositeElement\" == element.category:\n","        text_elements.append(Element(type=\"text\",text=element.text,page_no=dict(element.metadata.fields)['page_number']))\n","    elif \"Table\" == element.category:\n","        table_elements.append(Element(type=\"table\",text=element.metadata.text_as_html,page_no=dict(element.metadata.fields)['page_number'])) # metadata.text_as_html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK8yV6IHdENo"},"outputs":[],"source":["with open('results/raw_txt_elements.pkl', 'wb') as file:\n","    pickle.dump(text_elements,file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgIe37NPdENo"},"outputs":[],"source":["with open('results/raw_table_elements.pkl', 'wb') as file:\n","    pickle.dump(table_elements,file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8QUHYYMUdENo"},"outputs":[],"source":["with open('results/raw_txt_elements.pkl','rb') as fl1:\n","    text_elements = pickle.load(fl1)\n","with open('results/raw_table_elements.pkl','rb') as fl2:\n","    table_elements = pickle.load(fl2)"]},{"cell_type":"markdown","metadata":{"id":"RL70WAd2dENo"},"source":["**For text and tables we will create summaries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djOXOjBldENo"},"outputs":[],"source":["OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n","chat = ChatOpenAI(openai_api_key=OPEN_API_KEY,model=\"gpt-3.5-turbo\",temperature=0) # gpt-4o-mini\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mrp1Z8jrdENp","outputId":"bd391601-14ef-460f-f649-263c5841f7a9"},"outputs":[{"data":{"text/plain":["AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-dd5839e1-0cfb-43f5-b022-27e41b9275a3-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18})"]},"execution_count":350,"metadata":{},"output_type":"execute_result"}],"source":["chat.invoke(\"Hello!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DoO7Cbe-dENp"},"outputs":[],"source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.messages import AIMessage\n","\n","def parse(ai_message: AIMessage) -> str:\n","    \"\"\"Parse the AI message.\"\"\"\n","    return ai_message.content\n","\n","prompt_text = \"\"\" You are an assistant who's task is to summarize tables and chunks made of text. \\\n","Retrieve the summary response in the following format: ''\n","Give precise and informative summary of table or text. Table or text element \\\n","is provided to you from following content: {element}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(prompt_text)\n","\n","summarize_chain = {\"element\":lambda x: x} | prompt | chat | parse"]},{"cell_type":"markdown","metadata":{"id":"wieEnw5-dENp"},"source":["**txt representations of formulas created by LLM**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UH87LKHHdENp"},"outputs":[],"source":["import pickle\n","\n","with open(\"../data/formulas/latex_formulas.pkl\",\"rb\") as file:\n","    formulas = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbRHRIWqdENp"},"outputs":[],"source":["problem_example = r\"\\delta_k(x) = x^T \\Sigma^{{-1}} \\mu_k - \\frac{{1}}{{2}} \\mu_k^T \\Sigma^{{-1}} \\mu_k + \\log \\pi_k \"\n","\n","formulas_template = f\"\"\"You are an assistant who has a task to describe LaTex formula content by words.\n","This is example of your task: \\n\n","Problem: {problem_example}\"\"\" + \"\"\"\n","Solution: delta k(x) = x transpose Sigma inverse mu k - 1/2 mu k transpose Sigma inverse mu k + log pi k\n","You will execute your task on this formula:{formula} and return solution in format: Formula -> solution.\n","\"\"\"\n","formulas_prompt = ChatPromptTemplate.from_template(formulas_template)\n","describe_formula_chain = formulas_prompt | chat | StrOutputParser()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3UvkCuE7dENp","outputId":"e2d6862e-6fd7-4c17-ac4a-0fa84c4c24d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Formula -> delta k(x) = x transpose Sigma inverse mu k - 1/2 mu k transpose Sigma inverse mu k + log pi k.\n","Formula -> X equals the matrix with elements x subscript 11, x subscript 12, and so on up to x subscript 1p in the first row; x subscript 21, x subscript 22, and so on up to x subscript 2p in the second row; and continuing with n rows, where each row contains elements x subscript n1, x subscript n2, and so on up to x subscript n p.\n","Formula: x_{i}=\\left({\\frac{x_{i1}}{x_{i2}}}\\right)  \n","Solution: x i equals the fraction of x i1 over x i2.\n","Formula -> x equals a column vector with elements x two prime, x two prime, and so on, up to x n prime.\n","Formula: \\mathbf{X}=\\left(\\mathbf{x}_{1}\\quad\\mathbf{x}_{2}\\quad\\mathbf{\\epsilon}\\cdot\\mathbf{\\epsilon}\\ \\cdot\\mathbf{\\epsilon}\\cdot\\mathbf{\\epsilon}\\quad\\mathbf{x}_{p}\\right)  \n","Solution: X equals the matrix formed by x one, x two, epsilon dot epsilon dot epsilon dot epsilon, and x p.\n","Formula -> X transpose = ( x 11 & x 21 & dots & x n1 \\\\ x 12 & x 22 & dots & x n2 \\\\ vdots & vdots & ddots & vdots \\\\ x 1p & x 2p & dots & x n p )\n","Formula -> x sub j equals the column vector with elements x sub 1 j, x sub 2 j, and so on, up to x sub n j.\n","Formula -> y equals the column vector containing y sub 1, y sub 2, and continuing down to y sub n.\n","Formula -> a equals the column vector consisting of a1, a2, and continuing down to an, where the entries are arranged vertically.\n","Formula -> A equals the binomial coefficient of 1 over 3, followed by a matrix with 2 in the first row and 4 in the second row, and B equals the binomial coefficient of 5 over 7, followed by a matrix with 6 in the first row and 8 in the second row.\n","Formula: \\dot{Y}=\\hat{f}(X)  \n","Solution: Y dot equals f hat of X\n","Formula -> E of (Y minus Y dot) squared equals E of [f of X plus epsilon minus hat f of X] squared equals [f of X minus hat f of X] squared plus the variance of epsilon, which is reducible.\n","Formula -> f of X equals beta zero plus beta one times X one plus beta two times X two plus dot dot dot plus beta p times X p.\n","Formula -> Y is approximately equal to beta zero plus beta one times X one plus beta two times X two plus dot dot dot plus beta p times X p.\n","Formula -> A B equals the binomial coefficient with 1 on top and 3 on the bottom, followed by an array containing 2 and the fraction with 5 on top and 7 on the bottom, then another array with 6, 6 on the first row and 0 on the second row, equals the binomial coefficient with 1 times 5 plus 2 times 7 on top and 3 times 6 plus 4 times 8 on the bottom, equals the binomial coefficient with 19 on top and 43 on the bottom, followed by an array containing 22.\n","Formula -> log of the fraction of the probability of Y equals k given X equals x over the probability of Y equals K given X equals x.\n","Formula: \\operatorname*{Pr}(Y=k|X=x)={\\frac{\\pi_{k}\\times f_{k1}(x_{1})\\times f_{k2}(x_{2})\\times\\cdot\\cdot\\times f_{k p}(x_{p})}{\\sum_{l=1}^{K}\\pi_{l}\\times f_{l1}(x_{1})\\times f_{l2}(x_{2})\\times\\cdot\\cdot\\cdot\\times f_{l p}(x_{p})}}  \n","Solution: probability of Y equals k given X equals x equals pi k times f k1 of x1 times f k2 of x2 times dot dot dot times f kp of xp divided by the sum from l equals 1 to K of pi l times f l1 of x1 times f l2 of x2 times dot dot dot times f lp of xp.\n","Formula -> log(Pr(Y=k|X=x) / Pr(Y=K|X=x)) = a_k + sum(j=1 to p) b_kj x_j + sum(j=1 to p) sum(l=1 to p) c_kj x_l.\n","Formula -> log of the fraction of the probability of Y equals k given X equals x over the probability of Y equals K given X equals x equals beta k zero plus the sum from j equals 1 to P of beta k j times x j.\n","Formula -> Pr(Y equals k) equals e raised to the power of negative lambda times lambda raised to the power of k divided by k factorial for k equals 0, 1, 2, and so on.\n","Formula -> hat f sub k j of x sub j equals to a piecewise function where it is 0.32 if x sub j equals 1, and 0.13 if x sub j equals 2.\n","Formula -> MSE equals one over n times the sum from i equals one to n of the quantity y sub i minus the estimated function f hat of x sub i, all squared.\n","Formula -> log of lambda of X1 through Xp equals beta zero plus beta one times X1 plus ... plus beta p times Xp.\n","Formula -> lambda of X sub 1, up to X sub p equals e raised to the power of beta sub 0 plus beta sub 1 times X sub 1 plus dot dot dot plus beta sub p times X sub p.\n","Formula -> ell(beta_0, beta_1, ..., beta_p) = product from i equals 1 to n of (e raised to the power of negative lambda of x_i times lambda of x_i raised to the power of y_i divided by y_i factorial)\n","Formula -> E of Y given X1, and so on up to chi p equals beta zero plus beta one times X1 plus, and so on, plus beta p times Xp.\n","Formula -> f of Y given X1, up to Xp equals the probability of Y equals 1 given X1, up to Xp equals the exponential of beta zero plus beta one times X1 plus, continuing with beta p times Xp, divided by one plus the exponential of beta zero plus beta one times X1 plus, continuing with beta p times Xp.\n","Formula -> E of Y given X1, dot, dot, dot, Xp equals lambda of X1, dot, dot, dot, Xp equals e raised to the power of beta zero plus beta one X1 plus dot, dot, dot, plus beta p Xp.\n","Formula -> Gamma raised to the nu times nabla with respect to m equals the fraction of i over mathfrak greater than dot dot dot, summed from i equals 1 to infinity of MS E sub i.\n","Formula -> CV sub n equals one over n times the sum from i equals one to n of Err sub 1.\n","Formula -> C V_{<n>} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_{i} - \\hat{y}_{i}}{1 - h_{i}} \\right)^{2}  \n","Solution -> C V sub n equals 1 over n times the sum from i equals 1 to n of the quantity y sub i minus y hat sub i divided by 1 minus h sub i all squared.\n","Formula -> log of (p divided by (1 minus p)) equals (Lambda zero divided by Lambda one) times X one plus beta two times X one squared plus beta three times X two plus beta four times X two squared.\n","Formula -> hat u = (vartheta V squared - theta V) / (vartheta X squared + vartheta V squared - bar theta X gamma)\n","Formula -> hat theta equals hat theta subscript Y superscript l bar minus hat bar phi subscript X times y all over hat theta subscript X superscript l bar plus hat bar theta subscript Y superscript l bar minus bar chi times hat bar theta subscript X times y.\n","Formula -> alpha bar equals one divided by one factorial times l bracket sum from r equals one to l old of hat alpha sub r equals l sub alpha bar times j bar 996.\n","Formula -> square root of the fraction with 1 in the numerator and 1000 minus 1 in the denominator, multiplied by the sum from r equals 1 to lolo of the quantity (hat alpha sub r minus hat alpha) squared, equals 0.083.\n","Formula -> E left parenthesis y subscript 0 minus hat f left parenthesis x subscript 0 right parenthesis right parenthesis squared equals variance of hat f left parenthesis x subscript 0 right parenthesis plus left bracket bias of hat f left parenthesis x subscript 0 right parenthesis right bracket squared plus variance of epsilon.\n","Formula -> hat s E subscript B of hat a equals the square root of one over B minus one times the sum from r equals one to B of the quantity hat alpha star r r minus one over B times the sum from r prime equals one to B of hat a r r prime all squared.\n","Formula -> CV sub k equals 1 over k times the sum from i equals 1 to g of MSE.\n","Formula -> C sub p equals one over n times the quantity of R S plus two d delta squared.\n","Formula -> Lambda given l is less than or approximately equal to the sum of the absolute value of R multiplied by the double angle bracket notation plus the product of Lambda and the square of delta given l.\n","Formula -> BIC equals one over partial Sigma times the sum of RSS plus log Sigma to the gf of pi times a times a hat sigma squared.\n","Formula -> left absolute value of the inner product of the absolute value of the hyperbolic sine evaluated at index i prime evaluated at index j, norm at index i, right absolute value at index i, left absolute value of the double integral from i to j equivalent to the square root of the union from i equals 1 to infinity of the absolute value of the absolute value of the absolute value of the absolute value of p evaluated at index i equals the absolute value of one-fifth evaluated at index i divided by the product over k of the transpose of the absolute value of the square root of the absolute value of the absolute value of pi divided by the product over i equals the product over i, right absolute value evaluated from i equals 1 to 1.\n","Formula -> RSS = sum from i equals 1 to n of (y sub i minus beta hat sub 0 minus sum from j equals 1 to p of beta hat sub j times x sub i j) squared\n","Formula -> sum from i equals 1 to n of the quantity y sub i minus beta sub 0 minus sum from j equals 1 to p of beta sub j times x sub i j, all squared, plus lambda times sum from j equals 1 to p of beta sub j squared equals RSS plus lambda times sum from j equals 1 to p of beta sub j squared.\n","Formula: \\tilde{x}_{i j}=\\frac{\\stackrel{\\leftrightarrow}{\\sqrt{\\frac{1}{n}}\\sum_{i=1}^{n}(x_{i j}-\\overline{{{x}}}_{j})^{2}}}  \n","Solution: x tilde i j equals the square root of one over n times the sum from i equals one to n of x i j minus x overline j squared, with a double arrow above the square root.\n","Formula: \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}|\\beta_{j}x_{i j}\\right)^{2}+\\lambda\\sum_{j=1}^{p}|\\beta_{j}| = \\mathrm{RSS}+\\lambda\\sum_{j=1}^{p}|\\dot{p}\\rangle  \n","Solution: sum from i equals 1 to n of the quantity y i minus beta zero minus sum from j equals 1 to p of the absolute value of beta j times x i j, all squared, plus lambda times sum from j equals 1 to p of the absolute value of beta j equals RSS plus lambda times sum from j equals 1 to p of the inner product of p.\n","Formula -> one over n times the sum from i equals 1 to n of the indicator function of y sub i not equal to y hat sub i.\n","Formula -> Ave of the indicator function of y zero not equal to dot y zero.\n","Formula -> sum from 1 to n of the maximum over 1 of the gradient raised to the power P of the maximum over D divided by j equals 1, underbrace of the gradient raised to the power P over K, where the gradient is equal to the gradient of the sum from j equals 1 to P of the integral of the tilde p sub j not equal to j less than or equal to the gradient of delta divided by the gradient less than or equal to 0.\n","Formula -> sum from i equals 1 to n of (y sub i minus theta sub j) squared.\n","Formula -> b sub i equals y sub j.\n","Formula -> millimilize { sum from i equals 1 to n of ( y sub i minus beta bar zero minus sum from j equals 1 to p of beta bar j times x sub i j ) squared } subject to sum from j equals 1 to p of absolute value of beta bar j less than or equal to i, where i is greater than j equals 1.\n","Formula: \\sum_{i=1}^{\\mathsf{P}}\\left(|j_{j}-{\\hat{\\mathsf{P}}}_{j}\\right)^{i}+\\lambda\\sum_{i=1}^{\\mathsf{P}}\\delta_{j}^{i}  \n","Solution: sum from i equals 1 to P of the absolute value of j sub j minus hat P sub j raised to the power of i plus lambda times the sum from i equals 1 to P of delta sub j raised to the power of i.\n","Formula: \\sum_{i=1}^{\\mathsf{P}}\\left(\\left|j_{j}-{\\frac{\\beta}{\\beta_{j}}}\\right|^{2}+\\lambda\\sum_{j=1}^{\\mathsf{P}}\\left|{\\hat{\\boldsymbol{p}}}\\right| \\right)  \n","Solution: sum from i equals 1 to P of the absolute value of j sub j minus beta over beta sub j squared plus lambda times sum from j equals 1 to P of the absolute value of p hat.\n","Formula -> beta j superscript R equals y j divided by the quantity one plus lambda.\n","Formula -> hat beta j superscript L equals left brace y j superscript J minus lambda over 2 if y j is greater than lambda over 2; hat gamma J superscript 0 equals lambda over 2 if y j is less than negative lambda over 2; dot dot dot dot phantom minus ~~~.\n","Formula: p(\\beta|X,Y) \\propto f(Y|X,\\beta)p(\\beta|X) = f(Y|X,\\beta)p(\\beta)  \n","Solution: p beta given X, Y is proportional to f of Y given X, beta times p of beta given X equals f of Y given X, beta times p of beta.\n","Formula: Y = \\beta_{0} + X_{1} \\beta_{1} + \\cdots + X_{\\mathrm{p}} \\beta_{p} + \\epsilon  \n","Solution: Y equals beta zero plus X one beta one plus dot dot dot plus X p beta p plus epsilon.\n","Formula -> psi psi\n","Formula -> Pr of Y equals j given X equals x zero\n","Formula -> vert y sub i equals theta bar sub 0 plus the sum from m equals 1 to M of theta bar sub m z sub tm plus epsilon sub i, where i equals 1, dots, p sub i.\n","Formula -> sum from m equals 1 to M of theta m z i m equals sum from m equals 1 to M of theta m sum from j equals 1 to P of phi j m x i j equals sum from j equals 1 to P of sum from m equals 1 to M of theta m phi j m x i j equals sum from j equals 1 to P of beta j x i j.\n","Formula -> Psi l of the absolute value of theta l, bar equivalent to zero minus, is equivalent to the fraction where the numerator is t bar chi times the sum of the absolute value of hat theta plus hat theta one times bar theta one squared plus the absolute value of hat theta two raised to the three halves plus dot dot dot plus the absolute value of hat theta one times bar theta one raised to the fourth power, and the denominator is the absolute value plus the absolute value of bar theta times the sum of hat theta zero squared plus hat theta two squared plus bar theta four squared times bar theta i squared plus dot dot dot plus hat theta zero to the fourth power times bar theta i to the fourth power.\n","Formula: Z_{m}=\\sum_{i=1}\\phi_{j m}\\lambda  \n","Solution: Z m equals the sum from i equals 1 of phi j m times lambda.\n","Formula -> C zero of X equals indicator function of X less than c flat, C one of X equals indicator function of X between c one and c three, C two of X equals indicator function of X between c two and c three, C K of X equals indicator function of X greater than or equal to c K.\n","Formula -> u = hat B zero + beta one C one of x one + beta two C two of x one + ... + beta K C K of x one + epsilon one.\n","Formula -> 1 minus expected value of the maximum over j of the probability of Y equals j given X.\n","Formula -> y sub i equals beta sub 0 plus beta sub 1 times C sub 1 of x sub i plus beta sub 2 times C sub 2 of x sub i plus dot dot dot plus beta sub K times C sub K of x sub i plus epsilon sub i.\n","Formula: \\operatorname*{Pr}[\\vartheta_{i}>\\ 2^{\\gamma_{1}}]|z_{i}\\rangle={\\frac{\\operatorname{tarp}\\{\\Lambda_{i}+\\Im_{1}\\bar{c},1\\{x_{i}\\}\\rangle+\\;\\cdots\\not=\\beta_{K}\\bar{c},K\\{x_{i}\\}\\}}{1+\\mathrm{taxp}[\\bar{\\Lambda}_{k}+\\bar{\\Lambda}_{1}\\bar{\\zeta}_{1}[x_{i}\\}+\\:\\cdots\\:\\mp\\beta_{K}\\bar{\\zeta}_{K}[x_{k}]\\}}}\n","\n","Solution: probability of theta i greater than 2 raised to the gamma one given z i equals the fraction where the numerator is the tarp of Lambda i plus imaginary one bar c, one of x i not equal to beta K bar c, K of x i and the denominator is one plus the tax of bar Lambda k plus bar Lambda one bar zeta one of x i plus additional terms minus beta K bar zeta K of x k.\n","Formula: y_{i}=\\tilde{\\bf n}_{0}+\\beta_{1}b_{1}\\langle x_{i}\\rangle+\\tilde{\\bf n}_{2}b_{2}\\langle{\\bf x}_{i}\\rangle+\\beta_{2}b_{3}\\langle x_{i}\\rangle+\\prod\\beta_{R}\\delta_{N}\\langle{\\bf x}_{i}\\rangle+\\epsilon_{i}b_{2}\\rangle  \n","Solution: y i equals tilde n zero plus beta one b one times angle bracket x i angle bracket plus tilde n two b two times angle bracket x i angle bracket plus beta two b three times angle bracket x i angle bracket plus product beta R delta N times angle bracket x i angle bracket plus epsilon i b two angle bracket.\n","Formula -> y sub i equals vector p sub i0 cubed plus j sub 1 x sub i plus vector p sub 2 x sub two thirds squared plus vector p sub 1 x sub i cubed plus c sub i\n","Formula -> tilde q i equals tilde beta bar d plus beta i b bar 1 of T i plus beta bar j 2 star b bar 2 of x i plus dot dot dot left right arrow beta bar p R plus bar k star of bar x i plus bar a i.\n","Formula -> J sub id of L and xi equals the positive part of bar x minus xi to the power of one, which is defined as follows: it equals the first case where the expression in curly braces is the one raised to the power of one if z is greater than the overline of xi, otherwise it equals zero in the case of other conditions.\n","Formula -> left brace l subscript i superscript A right brace minus or plus integral subscript i vertical bar 2 epsilon subscript i right arrow left dot right dot integral subscript bar L Psi right dot right dot right dot right dot right dot left dot right brace Psi right bracket Xi subscript i right dot left dot left brace fraction l vertical bar bar L right brace right dot scriptscriptstyle Gamma right brace right dot vertical bar fraction l bar L over epsilon subscript i right dot Gamma right dot fraction l left arrow bar zeta subscript i over vertical bar bar l right dot Psi right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot vertical bar fraction l over vertical bar bar Psi right dot left dot left dot Gamma right brace right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right dot right\n","Formula: \\sum_{i=1}\\langle{|j|}-\\beta({|z_{i}|})\\rangle^{i}+\\lambda\\int\\phi^{T}({|z|}^{2}\\mathrm{d})  \n","Solution: sum from i equals 1 of the angle bracket of absolute value of j minus beta of absolute value of z sub i close angle bracket raised to the power of i plus lambda times the integral of phi transpose of absolute value of z squared d.\n","Formula -> R sub 3 equals S dot V\n","Formula: \\sum K_{\\mathrm{to}}(y_{i}-{\\bar{\\Lambda}}_{0}-{\\bar{\\beta}}_{1}\\mathbf{r}_{i})^{2}  \n","Solution: sum K to of (y i minus Lambda bar zero minus beta bar one times r i) squared\n","Formula -> f hat of t subscript i j equals hat big wedge of the union of hat psi subscript i hat j subscript i hat j subscript i hat i subscript i.\n","Formula -> g sub i = beta zero + beta one z sub i one + beta two x sub i two + ... + beta p z sub i p + epsilon sub i\n","Formula -> phi i = hat phi i j + sum from j equals 1 to s of the integral of the average of E i j plus the average of i j plus the average of i i plus the average of i i k plus i, j plus the average of j j times the average of i i j plus i.  \n","phi i = hat lambda k j + hat j of i i k plus hat j k of i i k plus i, j plus gamma j times the average of i i k plus i.\n","Formula -> log(v of X divided by 1 of (p of X divided by 1 of 1 of N)) equals v divided by Lambda zero plus beta i times X one plus beta two times X two plus commas.\n","Formula -> d f sub A equals the sum from i equals 1 to n of the elements of S sub A indexed by i.\n","Formula: \\log\\left(\\frac{{\\bf\\bar{p}}^{i j}(X)}{1-p_{i}(X)}\\right)=\\tilde{\\bf\\bar{\\tilde{p}}_{0}}+f_{1}\\langle X_{1}\\rangle+f_{2}(X_{2}\\rangle+\\cdot\\cdot,\\leftrightarrow f_{\\beta}\\langle X_{\\beta}\\rangle  \n","Solution: log of (bar p raised to the power of i j of X divided by 1 minus p sub i of X) equals tilde bar tilde p sub 0 plus f sub 1 times average of X sub 1 plus f sub 2 times average of X sub 2 plus dot dot dot, corresponding to f sub beta times average of X sub beta.\n","Formula: \\log\\left(\\frac{{P}^{i}\\!\\!\\slash^{i}X\\!\\!\\slash}{1-{P}^{i}\\!\\!\\slash^{i}}{\\!\\!\\int}=\\tilde{\\beta}_{0}+\\beta\\!\\!\\beta_{1}\\times\\ y\\!\\!\\cos\\!\\!\\delta\\!\\!\\slash{x}+f_{2}(\\!\\small\\pm\\xi\\!\\!\\slash_{0}\\!\\!\\slash)+f_{3}\\!\\!\\langle\\!\\cos\\!\\!\\langle\\!\\slash\\mathrm{s}\\!\\mid\\!\\cdots\\!\\!\\int\\!\\!\\int_{0}^{}\\!\\!\\!\\int_{0}\\!\\!\\!\\int\\!\\!\\epsilon\\in\\!\\!\\dim\\!\\!\\!\\operatorname*{det}\\!\\!\\slash\\cdot\\mathrm{d}\\!\\!\\left\\Vert\\!\\!\\slash\\boldmath\\Pi\\right)\\ . \n","\n","Solution: log of the fraction of P raised to the power i divided by 1 minus P raised to the power i equals beta tilde zero plus beta times beta one times y times cosine delta divided by x plus function f two of plus or minus xi divided by zero plus function f three times the cosine of angle s and so on integrated over the range from zero to zero and including epsilon in the dimension of the determinant times the differential of the norm of the bold symbol Pi.\n","Formula -> hat RSS caj lambda equals the sum from i equals 1 to n of the expression 3 lambda minus hat y lambda to the power of negative one, with an inner product notation, squared equals the sum from i equals 1 to infinity of the expression y i minus hat g lambda with an inner product notation, divided by i minus the expression involving S lambda and hat i lambda i, all squared.\n","Formula -> serenener goes to ... sum over i greater than or equal to z minus in fHat of the set where j sub i equals hat hat y sub S1 squared plus sum over i greater than or equal to plus minus sum over k plus minus sum over i greater than or equal to z plus not equal to H sub plus 1 of the set where j sub i i equals hat of the absolute value of hat y sub 1 raised to the power of s over 2.\n","Formula: \\sum_{n=n=1}^{n+\\varepsilon_{1}\\operatorname{cundersensenter}\\to\\varepsilon_{0}\\operatorname{crig}_{2}}_{\\left\\{y_{i}=\\dot{\\hat{y}}_{K_{m}}\\right\\}^{2}+\\alpha\\left|T\\right|}  \n","Solution: sum from n equals 1 to n plus epsilon one under the condition that epsilon zero crig two of the set where y i equals dot y hat K m squared plus alpha times the absolute value of T.\n","Formula: {\\hat{E}}=1-\\operatorname*{max}_{k}(\\beta_{\\mathrm{ink}})  \n","Solution: E hat equals 1 minus the maximum over k of beta ink.\n","Formula: G = \\sum_{n=1}^{n} \\beta_{n}(1-i\\epsilon_{n k})  \n","Solution: G equals the sum from n equals 1 to n of beta n times the quantity one minus i times epsilon n k.\n","Formula: U = -\\sum_{i=1}^{K}{\\hat{\\mu}}_{\\mathrm{at}}\\left|\\psi_{\\mathrm{g}}\\right|  \n","Solution: U equals negative sum from i equals 1 to K of hat mu at times absolute value of psi g.\n","Formula -> ell of X equals beta zero plus the sum from one to n of X.\n","Formula -> I of X equals the sum from us to N of epsilon us, times the indicator function of X being in I.\n","Formula -> Gamma zero star Gamma zero star, iota j to the j\n","Formula -> absolute value of nu subscript tr multiplied by the average of t, circled with the fraction of the magnitude of the vector sigma over the magnitude of the vector sigma, all multiplied by beta times the average of t.\n","Formula -> vertical bar with subscript laj, vertical bar with subscript k, vertical bar with subscript k equals one over beta times the sum from i equals one to P of vertical bar with hat a subscript i vertical bar.\n","Formula -> m approaches pi prime times epsilon prime sin d R sub s of j, s equals the set of X such that X sub s prime is greater than or equal to s.\n","Formula -> Pr of Y equals j given X equals x zero equals one over K times the sum over i in the set N zero of the indicator function I of y i equals j.\n","Formula -> hat beta 1 equals the sum from i equals 1 to n of (x sub i minus x bar) times (y sub i minus y bar) divided by the sum from i equals 1 to n of (x sub i minus x bar) squared, hat beta 0 equals y bar minus hat beta 1 times x bar.\n","Formula -> y sub i left arrow Y sub i equals lambda hat J raised to the power b of z sub i.\n","Formula: \\dot{\\bar{\\Gamma}}(x)=\\sum_{\\Lambda=1}^{n^{\\mu}}\\Lambda_{\\bar{\\Lambda}}\\dot{\\bar{\\Gamma}}^{\\mathrm{i}}(\\d y  \n","Solution: dot bar Gamma of x equals the sum from Lambda equals 1 to n mu of Lambda sub bar Lambda times dot bar Gamma to the power of i of dy\n","Formula: \\sum_{i=1}^{M} [i] * \\sum_{i=1}^{n} j_{i}  \n","Solution: sum from i equals 1 to M of i times sum from i equals 1 to n of j sub i\n","Formula -> i hat lambda of z equals the sum from k equals 0 to hat lambda of j hat k zero of z, hat g zero one j equals i plus hat z one prime dot dot dot, hat B.\n","Formula -> sum from n to n of the absolute value of t sub i, multiplied by the sum over i and i.\n","Formula: i(t) = l / (B - L) * Σ (from k = 1 to B) ∫ (from u) (∇)  \n","Solution: i t equals l divided by B minus L times the sum from k equals 1 to B of the integral from u of the gradient.\n","Formula: |*s_{1}+s_{1}+s_{1}-  \n","Solution: absolute value of s one plus s one plus s one minus\n","Formula -> i i plus beta i X I plus beta 2 X Z comma equivalent to p hat i comma X j equals empty set\n","Formula: \\left\\langle\\left\\langle J\\right|\\left.\\right\\rangle\\left[\\left\\langle\\bar{z}{\\cal\\right\\rangle}\\right|\\mp\\left.\\int\\left\\langle\\bar{z}{\\cal\\right\\rangle}+\\right\\rangle\\right\\rangle^{\\hat{0}}\\left(\\bar{z}{\\cal\\right]  \n","Solution: double angle bracket J with angle bracket bar z curly with angle bracket minus or plus integral of angle bracket bar z curly plus angle bracket double angle bracket to the power of hat zero with bar z curly bracket.\n","Formula: Y = \\beta_{0} + \\beta_{1} X^{\\ast} + \\epsilon  \n","Solution: Y equals beta zero plus beta one times X star plus epsilon.\n","Formula -> zero plus j hat norm L plus theta hat norm t u hat plus i hat not equal to theta hat norm H u hat y tensor product norm i with u hat tensor product i.\n","Formula: \\dot{v}_{\\mathrm{il}}+\\dot{y}_{\\mathrm{i}}X_{\\mathrm{1}}+\\beta_{\\mathrm{2}}X_{\\mathrm{z}}+\\cdots\\circ\\nu\\dots\\dot{\\beta}_{\\mathrm{s}}X_{\\mathrm{s}}\\geqslant\\dot{\\mathrm{b}}  \n","Solution: v dot il plus y dot i times X1 plus beta 2 times Xz plus dots circ nu dots beta dot s times Xs is greater than or equal to b dot.\n","Formula -> partial derivative with respect to zero plus beta one times vector one at index one i plus beta two times vector one at index d i plus star, plus beta beta times vector two at absolute value of i is greater than empty set.\n","Formula -> wi(3_0 + beta cubed 1 x i1 + beta 22 cubed T i2 + ... not I p T i p) greater than or equal to M, V, i, V.\n","Formula -> psi evaluated at the vertical bar down arrow, summation over the vertical bar greater than the vertical bar 2, summation over the vertical bar of vector L greater than 1, 1, plus 9 divided by beta vertical bar times product over the vertical bar.\n","Formula -> limit as k approaches delta prime given k prime conditioned on dot dot dot, bar theta conditioned on k prime conditioned on dot dot dot, hat theta conditioned on k prime conditioned on dot dot dot, hat theta conditioned on k, right angle bracket, hat Psi.\n","Formula -> hat theta zero plus beta evaluated at two times triangle twelve plus dot dot dot plus hat beta zero times triangle one is greater than or equal to M times one equals nu, hat vartheta plus sum from i equals one to n of hat vartheta i times c.\n","Formula: \\dot{t}_{0}+\\dot{y}_{1}X_{1}+\\dot{y}_{2}X_{\\bar{z}}+\\l^{\\prime}\\cdot\\l+\\bar{\\Lambda}_{b}X_{\\l s}\\l\\ll\\hat{0}  \n","Solution: dot t zero plus dot y one times X one plus dot y two times X bar z plus lambda prime dot lambda plus bar Lambda b times X lambda s times lambda is much less than hat zero.\n","Formula -> c bar sub i, Z sub f double prime given Phi equals the sum over vector c equals 1 and i not equal to 1 to bullet of Lambda times Lambda times Lambda times Lambda times bar Lambda sub i times bar Lambda star.\n","Formula -> f of x equals beta hat zero plus the sum from s equals one to s of alpha sub s times the inner product of x and x sub i.\n","Formula -> absolute value of n times absolute value of n times one over mu factorial times one over mu factorial.\n","K[x_{1},x_{2},y_{3}] -> K of x one, x two, y three.\n","Formula -> N of I1, I1 dagger equals the sum from i greater than or equal to 1.\n","Formula -> Lambda of L1 and L1 to the power of i equals the absolute value of the sum from i equals 1 to n of L sub i times the absolute value of L sub i to the power of i, all raised to the power of i.\n","Formula -> x equals tilde R zero plus the sum of d k for all x in the set x where x equals x and j.\n","Formula -> hat U subscript 1, hat i subscript 1, hat i subscript 1 equals hat A, such that hat i subscript 1 is equivalent to hat A times negative hat Lambda, summed from i equals 1 to i of hat I subscript i times hat j vector.\n","Formula -> limit as the determinant of Sigma approaches infinity of the maximum of the sum from i equals 1 to n tilde of the maximum of zero and one minus the union from i equals 1 to n tilde of Pi inajk of the maximum of zero and one minus the union from j equals 1 to p tilde of the maximum of one plus lambda times the sum from j equals lambda to p tilde of the square of hat theta j.\n","Formula -> min f sub i,j downarrow, j downarrow of (i times square root of N, j) plus i times P of i\n","Formula -> negative of a long sequence of characters or symbols that are not specified in the provided content.\n","Formula -> angle bracket of angle bracket X equals bar Lambda sub bar Lambda star times the sum from k equals bar Lambda to K of bar beta sub k times angle bracket X angle bracket equals bar Lambda sub bar Lambda star times the sum from k equals bar k equals bar Lambda to bar Lambda of bar beta sub k comma bar beta times angle bracket of bar bold bar bold bar Phi underline Lambda underline Lambda angle bracket.\n","Formula -> dot d sub k equals hat h sub k of V equals g of open bracket u sub k0 plus sum from n equals 1 to infinity of u sub k j times N sub i close bracket.\n","\\hat{\\langle|}\\hat{\\lambda}\\rangle=\\hat{\\Psi_{\\hat{0}}}\\hat{\\mp\\sum_{\\hat{m}}}\\hat{\\hat{\\nu}_{\\hat{\\imath}}}\\hat{\\hat{\\nu}_{\\hat{\\imath}}}\\hat{\\hat{\\nu}_{\\hat{\\imath}}}\\hat{\\hat{\\nu}_{\\hat{\\imath}}}\\hat{\\hat{\\nu}_{\\hat{\\imath}}}\\hat{\\hat{\\nu}_{\\hat{\\imath}}}\\hat{\\hat{\\nu}_{\\hat{\\imath}}}\\hat{\\hat{\\nu}_{\\hat{\\imath}}}\\hat{\\hat{\\nu}_{\\hat{\\imath}}}\\hat{\\hat{\\nu}_{\\hat{\\jmath}}}}\\hat{\\hat{\\ |}}\\hat{\\hat{\\nu}_{\\hat{\\jmath}}}\\hat{\\hat{\\nu}_{\\hat{\\nu}}}}\\hat{\\hat{\\ |}}\\hat{\\hat{\\nu}_{\\hat{\\nu}}}\\hat{\\hat{\\nu}_{\\hat{\\nu}}}\\hat{\\hat{\\bf P}}\\hat{\\hat{\\hat{\\nu}}}\\hat{\\hat{\\hat{\\hat{\\nu}}}}\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\nu}}}}}\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\nu}}}}}}}}}\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\hat{\\nu}}}}}}}}_{\\hat{\\hat{\\hat{\\hat{ -> solution: hat left angle | hat lambda hat right angle equals hat Psi hat 0 hat sum over hat m hat nu hat i hat nu hat i hat nu hat i hat nu hat i hat nu hat i hat nu hat i hat nu hat i hat nu hat i hat nu hat j hat | hat nu hat j hat nu hat nu | hat nu hat nu hat nu hat P hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu hat nu\n","Formula -> g(z) = e raised to the power of z divided by 1 plus e raised to the power of z = 1 divided by 1 plus e raised to the power of negative z.\n","Formula -> g with a tilde over the union symbol equals the set containing the following: g if there is a curved cross product with the union symbol, and so on, of Phi models the star symbol, star star symbol, and five star symbols.\n","Formula -> absolute value of h subscript 1 times absolute value of N equals the inner product of the quantity in brackets, which is the absolute value of the sum of N subscript 1 and the estimate of lambda subscript 2 raised to the power of 3; absolute value of h subscript estimate 2 times absolute value of N equals the inner product of the quantity in brackets, which is the estimate of 0 plus the estimate of lambda subscript 1 minus the estimate of lambda subscript estimate 2 raised to the power of 3.\n","Formula -> left double vertical bar left double vertical bar left double vertical bar left double vertical bar hat M parallel right angle bracket right double vertical bar left angle bracket hat n right parallel right double vertical bar right double vertical bar dagger left vertical bar hat n right vertical bar left vertical bar hat n right vertical bar right double vertical bar left vertical bar hat j right vertical bar left vertical bar hat j right vertical bar left vertical bar hat j right vertical bar left vertical bar hat j right vertical bar left vertical bar hat j right vertical bar left vertical bar hat j right vertical bar right angle bracket right double vertical bar, right double vertical bar, right double vertical bar hat hat j right angle bracket.\n","Formula -> X = (1/2) * [(1/4) * (N + X1 + X2)^2 - (1/2)]^2 = X1 * X2.\n","Formula: \\sum_{i=1}^{n}(y_{i}-f(x_{i}))^{i}  \n","Solution: sum from i equals 1 to n of (y sub i minus f of x sub i) raised to the power of i\n","Formula -> mathcal Z sub s u = beta sub pri6 cubed + sum from sigma prime equals 1 to K sub 2 of tilde beta sub raf times hat Lambda sub tilde t to the power of 2 times left brace mathcal W right brace = beta sub prini6 cubed + sum from alpha equals 1 to K sub 2 of tilde beta sub raf times tilde Lambda sub tilde t to the power of 2.\n","Formula -> left angle bracket in bar X right angle bracket equals bar Pi subscript i of bar Y equals fraction e raised to the power of bar Z subscript n over bar eta right angle bracket equals fraction e raised to the power of bar Z subscript n over negative fraction bar eta over m minus bar eta times bar xi subscript i.\n","Formula: \\sum_{i=1,\\infty}^{1}\\rangle_{i,j}^{1}|i\\rangle|j_{i}|i\\rangle  \n","Solution: sum from i equals 1 to infinity of the inner product of i and j, with the ket i and the ket j subscript i, and the ket i.\n","Formula -> A subscript Lambda with superscript {1} equals the fraction with 1 in the numerator and a subscript k with superscript {1} in the denominator, multiplied by the set X. This equals g of the expression omega subscript Lambda i bar 0 with superscript {1} not connected to the summation from lambda equals 1 to infinity of omega subscript bar k, bar 3 with superscript {1} times X subscript j.\n","Formula -> lag of the product of the probability of Y equals 1 given X divided by the product of the probability of Y given the absolute value of X equals the modified Z1 equals the modified Z1. This is equal to the set where the modified beta for 1, i, delta equals the modified beta for 1, i, delta plus the sum over K2, K2, kappa2, and epsilon equals the modified beta for 1, i, delta equals the modified beta for 1, delta, where lambda i equals the modified beta for 1, delta star given the modified delta i equals the modified beta for 1, i, delta star, and lambda i to the power of the modified delta.\n","Formula -> d bar k equals bar g of the derivative of k prime bar k sub k of beta dot minus the sum of the derivative plus the fraction of the derivative of mu prime over k, summed over bar prime prime, times the sum from a equals 1 to beta times bar K of bar a times iota sub k s times A bar k times epsilon minus the underline of 1 sub bar n, right angle bracket.\n","Formula -> O sub l equals hat phi sub l plus the sum from k equals 1 to n of hat phi sub k l.\n","Formula: (Y - O_{L})^{2}  \n","Solution: Y minus O sub L, all squared.\n","Formula -> A subscript ell in curly braces d equals the integral from ell to d of the set X, equals g of alpha subscript ell H in square brackets plus the sum from lambda less than or equal to 1 in square brackets of omega subscript ell L in curly braces times the set X in curly braces times the set X in curly braces.\n","Formula -> left bracket, array, column, v sub L plus 1, v sub L plus 2, v sub L plus 3, v sub L plus 3, right bracket, double space, bold M equals left bracket, array, row, 1, v sub L, v sub L minus 1, dot dot dot, v sub 1, right bracket, row, 1, v sub L plus 1, v sub L plus 1, dot dot dot, v sub 2, right bracket, row, vertical dots, vertical dots, vertical dots, right bracket, row, vertical dots, v sub T minus 2, v sub T minus 2, dot dot dot, v sub T minus L, right bracket, period.\n","Formula -> nabla R of theta to the power of m equals the partial derivative of R with respect to theta evaluated at theta equals theta to the power of m.\n","Formula -> theta raised to the power of m plus 1 is updated to theta raised to the power of m minus rho times the gradient of R evaluated at theta raised to the power of m.\n","Formula -> R sub i of theta equals one half times the quantity y sub i minus beta zero minus the sum from k equals one to K of beta sub k times g of the quantity w sub k zero plus the sum from j equals one to p of w sub k j times x sub i j, all squared.\n","Formula -> sum from i equals 1 to n of (y sub i minus o sub i L) squared equals sum from i equals 1 to n of the bigl (y sub i minus the bigl (beta sub 0 plus sum from k equals 1 to K of beta sub k g of (w sub k 0 plus sum from j equals 1 to p of w sub k j x sub i L j plus sum from s equals 1 to K of w sub k s a sub i comma L minus 1 comma s) bigr) squared.\n","Formula -> partial R sub i with respect to beta sub k equals partial R sub i with respect to f sub theta of x sub i times partial f sub theta of x sub i with respect to beta sub k equals negative of y sub i minus f sub theta of x sub i times g of z sub i k.\n","Formula -> partial R sub i with respect to w sub k j equals partial R sub i with respect to f sub theta of x sub i times partial f sub theta of x sub i with respect to g of z sub i k times partial g of z sub i k with respect to x sub i k times partial z sub i k with respect to w sub k j equals negative of (y sub i minus f sub theta of x sub i) times beta sub k times g prime of z sub i k times x sub i j.\n","Formula -> R(theta; lambda) = -sum from i equals 1 to n of sum from m equals 0 to 9 of y subscript i m times log of f subscript m of x subscript i plus lambda times sum over j of theta subscript j squared.\n","Formula -> Y equals the minimum of T and C.\n","Formula -> delta equals a piecewise function where delta is 0 if T is less than or equal to C, and delta is 1 if T is greater than C.\n","\\mathbf{S}(t)=\\operatorname*{Pr}(T>t) -> S of t equals the probability that T is greater than t.\n","Formula -> Pr of T greater than d sub k equals Pr of T greater than d sub k given T greater than d sub k minus 1 times Pr of T greater than d sub k minus 1 plus Pr of T greater than d sub k given T less than or equal to d sub k minus 1 times Pr of T less than or equal to d sub k minus 1.\n","Formula -> S of d sub k equals the probability of T being greater than d sub k equals the probability of T being greater than d sub k given that T is greater than d sub k minus 1 times the probability of T being greater than d sub k minus 1.\n","Formula -> S of d sub k equals the probability of T being greater than d sub k given that T is greater than d sub k minus 1, multiplied by S of d sub k minus 1.\n","Formula -> variance of mu hat equals standard error of mu hat squared equals sigma squared divided by n.\n","Formula -> S of d sub k equals the probability of T being greater than d sub k given that T is greater than d sub k minus 1, multiplied by the probability of T being greater than d sub 2 given that T is greater than d sub 1, multiplied by the probability of T being greater than d sub 1.\n","Formula -> hat Pr(T > d_i | T > d_{j-1}) = (r_i - q_i) / r_j.\n","Formula -> hat S of d sub k equals the product from i equals 1 to k of the fraction where the numerator is r sub j minus q sub j and the denominator is r sub j.\n","Formula -> W equals (X minus mu) divided by the square root of the variance of X.\n","Formula -> variance of the sum from k equals 1 to K of q one k is approximately equal to the sum from k equals 1 to K of the variance of q one k equals the sum from k equals 1 to K of q k times the fraction of r one k over r k times one minus the fraction of r one k over r k times the quantity r k minus q k divided by r k minus one.\n","Formula -> W equals the sum from k equals 1 to K of q one k minus mu k divided by the square root of the sum from k equals 1 to K of the variance of q one k, equals the sum from k equals 1 to K of q one k minus q k divided by r k times r one k, divided by the square root of the sum from k equals 1 to K of q k times r one k over r k times one minus r one k over r one times r k minus q k over r k minus one.\n","Formula: h(t) = \\operatorname*{lim}_{\\Delta t\\rightarrow0}{\\frac{\\operatorname*{Pr}(t<T\\leq t+\\Delta t|T>t)}{\\Delta t}}  \n","Solution: h of t equals the limit as delta t approaches zero of the probability of t less than T less than or equal to t plus delta t given T greater than t divided by delta t.\n","Formula -> n = n subscript n raised to the power of n subscript 1 times 6.\n","Formula -> h of t is approximately equal to the probability of t being less than T and T being less than or equal to t plus Delta t given that T is greater than t, divided by Delta t.\n","Formula -> f of t equals the limit as Delta t approaches zero of the fraction of the probability of t being less than T and T being less than or equal to t plus Delta t, divided by Delta t.\n","Formula -> SE hat beta 0 squared equals sigma squared times open bracket 1 over n plus vec x squared over sum from i equals 1 to n of x i minus vec x squared close bracket, SE hat beta 1 squared equals sigma squared over sum from i equals 1 to n of x i minus vec x squared.\n","Formula -> L equals the product from i equals 1 to n of f of y sub i raised to the power of delta sub i multiplied by S of y sub i raised to the power of one minus delta sub i, which equals the product from i equals 1 to n of h bar of y sub i raised to the power of delta sub i multiplied by S of y sub i.\n","Formula -> h of t given x sub i equals h zero of t times the exponential of the sum from j equals 1 to p of x sub i j times beta sub j.\n","Formula -> variance of q one k equals q k times r one k divided by r k times one minus r one k divided by r k times r k minus q k all divided by r k star minus one.\n","Formula: P L(\\beta) = \\prod_{i:\\delta_{i}=1}\\frac{\\exp\\left(\\sum_{j=1}^{p}x_{i j}\\beta_{j}\\right)}{\\sum_{i^{\\prime}:y_{i^{\\prime}}\\geq y_{i}}\\exp\\left(\\sum_{j=1}^{p}x_{i^{\\prime}j}\\beta_{j}\\right)}  \n","Solution: P L beta equals the product over i such that delta i equals 1 of the exponential of the sum from j equals 1 to p of x i j beta j divided by the sum over i prime such that y i prime is greater than or equal to y i of the exponential of the sum from j equals 1 to p of x i prime j beta j.\n","Formula -> absolute value of log of the product for all i such that delta i equals 1 of the fraction where the numerator is the exponential of the sum from j equals 1 to p of x i j beta j and the denominator is the sum for all i prime such that y i prime is greater than y i of the exponential of the sum from j equals 1 to p of x i prime j beta j, plus lambda times P of beta.\n","Formula: C = \\frac{\\sum_{i,i^{\\prime};y_{i}>y_{\\nu}}I(\\hat{\\eta}_{i^{\\prime}}>\\hat{\\eta}_{i})\\delta_{i^{\\prime}}}{\\sum_{i,i^{\\prime};y_{i}>y_{i^{\\prime}}}\\delta_{i^{\\prime}}}  \n","Solution: C equals the sum over i and i prime where y i is greater than y nu of the indicator function I of hat eta i prime being greater than hat eta i multiplied by delta i prime, divided by the sum over i and i prime where y i is greater than y i prime of delta i prime.\n","Formula -> RSE = square root of (RSS divided by (n minus 2))\n","Formula: Z_{1}=\\phi_{11}X_{1}+\\phi_{21}X_{2}+\\cdot\\cdot\\cdot+\\phi_{p1}X_{p}  \n","Solution: Z one equals phi eleven X one plus phi twenty one X two plus dot dot dot plus phi p one X p\n","Formula -> dot c sub i1 equals phi sub 11 x sub i1 plus phi sub 21 x sub i2 plus dot dot dot plus phi sub p1 x sub i p\n","Formula: \\sum_{i^{\\prime}:y_{i^{\\prime}}\\ge y_{i}}h_{0}(y_{i})\\exp\\left(\\sum_{j=1}^{p}x_{i^{\\prime}j}\\beta_{j}\\right)  \n","Solution: sum over i prime where y i prime is greater than or equal to y i of h zero of y i times the exponential of the sum from j equals 1 to p of x i prime j times beta j.\n","Formula -> max over phi from 11 to phi p1 of the expression 1 over n times the sum from i equals 1 to n of the square of the sum from j equals 1 to p of phi j1 times x i j, subject to the constraint that the sum from j equals 1 to p of phi j1 squared equals 1.\n","Formula -> z subscript 12 equals phi subscript 12 times x subscript 11 plus phi subscript 22 times x subscript 12 plus dot dot dot plus phi subscript p2 times x subscript 1p.\n","Formula -> h zero of y sub i times the exponential of the sum from j equals one to p of x sub i j times beta sub j divided by the sum over i prime where y sub i prime is greater than or equal to y sub i of h zero of y sub i times the exponential of the sum from j equals one to p of x sub i j times beta sub j equals the exponential of the sum from j equals one to p of x sub i j times beta sub j divided by the sum over i prime where y sub i prime is greater than or equal to y sub i of the exponential of the sum from j equals one to p of x sub i prime j times beta sub j.\n","Formula -> hat beta 1 plus/minus 2 times SE of hat beta 1.\n","Formula -> sum over A in the tilde of E to the power n times M, and bar B in the overbrace of A equals 1 to the power n, of sinh i times bar z dot A, of the sum from j equals 1 to M of the sum from i equals 1 to n of a subscript i m times b subscript j m, all squared.\n","Formula -> sum from i equals 1 to p of sum from j equals 1 to n of the quantity x subscript i j minus the sum from m equals 1 to M of z subscript i m times phi subscript j m, all squared.\n","Formula -> sum from j equals 1 to p of variance of X sub j equals sum from i equals 1 to p of one over n times sum from i equals 1 to n of x sub i j squared.\n","Formula -> one over r times the sum from i equals 1 to n of the derivative of y squared with respect to t, evaluated at i equals 1 to n, times the sum from bar i equals 1 to p of the sum from j equals 1 to p of the derivative of phi sub j m times x sub i j, all squared.\n","Formula -> fraction of the sum from l equals 1 to n of z l m squared over the sum from j equals 1 to p of the sum from l equals 1 to n of z bar i j squared is equivalent to the fraction of the sum from l equals 1 to n of the sum from j equals 1 to p of the inner sum of the sum from bar j equals 1 to the sum from bar j equals 1 of m squared i bar j squared over the sum from bar j equals 1 to p of the sum from i equals 1 to bar j of z bar i bar j squared.\n","Formula: \\sum_{i=1}^{p}{\\frac{1}{n}}\\sum_{i=1}^{n}{x_{i j}^{2}}=\\ \\sum_{m=1}^{M}{\\frac{1}{n}}\\sum_{i=1}^{n}{z_{i m}^{2}}\\;+{\\frac{1}{n}}\\sum_{i=1}^{p}{\\sum_{i=1}^{n}}\\left({x_{i j}}-\\sum_{m=1}^{M}{z_{i m}\\phi_{j m}}\\right)^{2}  \n","Solution: sum from i equals 1 to p of 1 over n times sum from i equals 1 to n of x sub i j squared equals sum from m equals 1 to M of 1 over n times sum from i equals 1 to n of z sub i m squared plus 1 over n times sum from i equals 1 to p of sum from i equals 1 to n of x sub i j minus sum from m equals 1 to M of z sub i m phi sub j m squared.\n","Formula -> absolute value of the sum from j equals 1 to p of the sum from j equals 1 to n of the quantity x subscript i j minus the sum from m equals 1 to M of bar z subscript i m times phi subscript j m, all squared, divided by the sum from j equals 1 to p subscript j of the sum from i subscript 1 equals 1 to n of bar x subscript i j squared equals 1 minus the ratio of residual sum of squares to total sum of squares.\n","Formula -> left parenthesis sum over (i,j) in G left parenthesis x subscript i j minus sum from m equals 1 to M of a subscript i m b subscript j m right parenthesis squared right parenthesis\n","Formula -> minimize sum of {sum from j equals 1 to P of sum from i equals 1 to n of (Lambda divided by hat x subscript i j minus sum from m equals 1 to M of a subscript i m times b subscript j m) squared}\n","Formula -> tilde x subscript i j equals x subscript i j if the pair i, j is in the set O, otherwise it equals tilde x subscript j.\n","Formula -> T_{i j} approximately equals the sum from m equals 1 to M of z_{i m} times delta_{j m}.\n","Formula -> minimize the sum of W of C sub k for k equals 1 to K.\n","Formula -> W of C sub k equals one over the absolute value of C sub k times the sum over i and i prime in C sub k of the sum over i from one to p of the quantity x sub i j minus x sub i prime j squared.\n","Formula -> minimize with respect to C1, ..., CK the sum from k equals 1 to K of 1 over the size of Ck times the sum over i and i prime in Ck times the sum from j equals 1 to P of the square of (x1j minus x i prime j).\n","Formula: \\hat{\\beta}_{0}\\pm2\\cdot\\mathrm{SE}(\\hat{\\beta}_{0})  \n","Solution: beta hat zero plus or minus 2 times standard error of beta hat zero.\n","Formula -> min over C1, ..., CK of the sum from k equals 1 to K of 1 divided by the size of Ck times the sum over i and i prime in Ck of the sum from j equals 1 to P of the quantity (xij minus xi prime j) squared.\n","Formula -> one over the square root of one sub k, with the condition that l is in the range of one over l, times the sum from j equals one to the product from j equals one to p not included of the expression bar one sub i j minus bar L sub i j raised to the power of j, equivalent to the tilde over i, times the product over i in the set T of the expression bar one sub i j minus bar L sub k j raised to the power of j.\n","Formula -> T equals the difference between the time derivative of mu t and the time derivative of mu c, divided by s times the square root of the sum of the reciprocals of n t and n c.\n","Formula: \\hat{\\mu}_{i}={\\textstyle\\frac{1}{n}}\\sum_{i=1}^{n_{i}}{x_{i}^{t}}\\,\\hat{\\mu}_{c}={\\textstyle\\frac{1}{{\\textstyle\\mathrm{n}}.}}\\sum_{\\mathrm{i=1}}^{n_{c}}{x_{1}^{c}}  \n","Solution: mu hat i equals 1 over n times the sum from i equals 1 to n i of x i transpose, mu hat c equals 1 over n times the sum from i equals 1 to n c of x 1 c.\n","Formula -> s equals the absolute value of the fraction where the numerator is the product of n sub t minus 1 and s sub t squared plus the product of n sub c minus 1 and s sub c squared, and the denominator is n sub t plus n sub c minus 2.\n","Formula -> H zero: beta one equals zero, H a: beta one not equal to zero.\n","Formula: Y = 2 + 3X + \\epsilon  \n","Solution: Y equals 2 plus 3 times X plus epsilon.\n","Formula -> FWER equals the probability that V is greater than or equal to 1.\n","Formula -> FWER alpha equals 1 minus the product from i equals 1 to m of 1 minus alpha equals 1 minus 1 minus alpha raised to the power of m.\n","Formula -> FWER equals the probability of falsely rejecting at least one null hypothesis, which is equal to the probability of the event where zero vector from j to m multiplied by A sub j occurs, and this is less than or equal to the sum from i equals 1 to m of the probability of A sub j.\n","Formula -> Gamma Psi Gamma P of o given eta is a subset of eta times mu under Omega equals 0.\n","Formula -> L equals the minimum of j such that p sub j is greater than alpha divided by m plus 1 minus j.\n","Formula: H_{0}:{\\frac{1}{2}}\\left(\\mu_{1}+\\mu_{3}\\right)={\\frac{1}{3}}\\left(\\mu_{2}+\\mu_{4}+\\mu_{5}  \n","Solution: H zero equals one half times the sum of mu one and mu three equals one third times the sum of mu two, mu four, and mu five.\n","L = max{j: p_(j) < q j/m} -> L equals the maximum value of j such that p sub j is less than q times j divided by m.\n","Formula -> tilde psi X minus tilde mu Y, tilde theta times square root of one over h X plus one over h Y.\n","Formula -> FWER(alpha) = 1 - Pr(V = 0) = 1 - Pr(done falsely reject any null hypotheses) = 1 - Pr(product from j equals 1 to m of {abe not falsely reject H0j})\n","Formula -> fraction of the sum from zero to one of the indicator function of i multiplied by the indicator function of three where the absolute value is greater than the absolute value of T, all divided by B.\n","Formula -> FDR equals the expected value of V divided by R, which is approximately equal to the expected value of V divided by R.\n","Formula -> p sub j equals the sum from i prime equals 1 to m of the sum from b equals 1 to B of the absolute value of v dot prime star given that j is distributed according to B m.\n","Formula -> R equals the sum from j equals 1 to m of the indicator function that is 1 if the condition that i is in the set T of j and the absolute value is greater than or equal to c is satisfied, and 0 otherwise.\n","Formula -> hat mu sub X equals 1 over n squared X times the sum from i equals 1 to n sub X of x sub i, hat mu sub Y equals 1 over n sub Y times the sum from i equals 1 to n sub Y of y sub i, s equals the square root of the fraction (n sub X minus 1) times s sub X squared plus (n sub Y minus 1) times s sub Y squared all over (n sub X plus n sub Y minus 2).\n","Formula -> V hat = sum from b equals 1 to D of sum from B equals 1 to D of sum over B of left bracket left vertical bar T to the power of left parenthesis comma right angle bracket asterisk right angle bracket greater than or equal to asterisk right bracket equals N.\n","Formula -> p.v a l u e = (sum from b equals 1 to B of l with the condition that the absolute value of T times b is greater than or equal to the absolute value of T) divided by B\n","Formula -> RSE equals the square root of one over n minus two, multiplied by the sum from i equals one to the circle of the difference between y sub i and y hat sub i, all squared.\n","Formula -> RSS = sum from i equals 1 to n of (y sub i minus y hat sub i) squared.\n","Formula -> R squared equals the total sum of squares minus the residual sum of squares divided by the total sum of squares equals one minus the residual sum of squares divided by the total sum of squares.\n","Formula -> Cor(X, Y) = sum from i equals 1 to n of (x sub i minus x bar) times (y sub i minus y bar) divided by the square root of the sum from i equals 1 to n of (x sub i minus x bar) squared times the square root of the sum from i equals 1 to n of (y sub i minus y bar) squared.\n","Formula: Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\cdots + \\beta_{p} X_{p} + \\epsilon  \n","Solution: Y equals beta zero plus beta one times X one plus beta two times X two plus dot dot dot plus beta p times X p plus epsilon.\n","Formula -> RSS = sum from i equals 1 to n of (y i minus y hat i) squared  \n","RSS = sum from i equals 1 to n of (y i minus beta hat 0 minus beta hat 1 x i1 minus beta hat 2 x i2 minus ... minus beta hat p x i p) squared.\n","Formula -> t equals the hat g subscript 1 minus 0 divided by the standard error of the hat beta subscript 1.\n","Formula -> H sub a: at least one beta j is non-zero.\n","Formula -> F equals the fraction where the numerator is the total sum of squares minus the residual sum of squares divided by p, and the denominator is the residual sum of squares divided by n minus p minus 1.\n","Formula -> E{RSS/(n-p-1)} = sigma squared\n","Formula -> E{(TSS - RSS)/p} = sigma squared.\n","Formula -> H zero: beta p minus q plus one equals beta p minus q plus two equals dot dot dot equals beta p equals zero.\n","Formula -> F equals the fraction where the numerator is the difference between RSS zero and RSS divided by q, and the denominator is RSS divided by the quantity n minus p minus 1.\n","Formula -> H zero: beta one equals beta two equals ... equals beta p equals zero\n","Formula -> p(X) = beta zero + beta one X.\n","Formula -> p(X) equals e raised to the power of beta zero plus beta one times X, divided by one plus e raised to the power of beta zero plus beta one times X.\n","Formula -> p(X) divided by 1 minus p(X) equals e raised to the power of beta zero plus beta one times X.\n","Formula -> log of the fraction of p of X over 1 minus p of X equals beta zero plus beta one times X.\n","Formula -> ell(beta 0, beta 1) = product for i where y i equals 1 of p(x i) times product for i prime where y i prime equals 0 of (1 minus p(x i prime)).\n","Formula -> log of (p of X divided by 1 minus p of X) tilde equals beta zero plus beta one times X one plus dot dot dot plus beta p times X p.\n","Formula -> p(X) = e raised to the power of beta zero plus beta one times X one plus ... plus beta p times X p, divided by one plus e raised to the power of beta zero plus beta one times X one plus ... plus beta p times X p.\n","Formula -> hat p of X equals e raised to the power of hat beta zero plus hat beta one times X, divided by one plus e raised to the power of hat beta zero plus hat beta one times X, equals e raised to the power of negative 10.6513 plus 0.0055 times 1.000, divided by one plus e raised to the power of negative 10.6513 plus 0.0055 times 1.000, equals 0.00576.\n","Formula -> hat p of X equals e raised to the power of negative 10.869 plus 0.00574 times 1.500 plus 0.003 times 40.6468 times 1, all divided by 1 plus e raised to the power of negative 10.869 plus 0.003 times 40 minus 0.6468 times 1, equals 0.058.\n","Formula: \\hat{p}(X)=\\frac{e^{-10.869+0.00574\\times1.500+0.003\\times40.6468\\times0}}{1+e^{-10.869+0.00574\\times1.500+0.003\\times40-0.6468\\times0}}=0.105  \n","Solution: p hat of X equals e raised to the power of negative 10.869 plus 0.00574 times 1.500 plus 0.003 times 40.6468 times 0, all divided by one plus e raised to the power of negative 10.869 plus 0.00574 times 1.500 plus 0.003 times 40 minus 0.6468 times 0, which equals 0.105.\n","Formula -> Pr(Y equals k given X equals x) equals the fraction with e raised to the power of beta k zero plus beta k one x one plus dot dot dot plus beta k p x p in the numerator and one plus the sum from l equals one to K of e raised to the power of beta l zero plus beta l one x one plus dot dot dot plus beta l p x p in the denominator.\n","Formula -> Pr of Y equals K given X equals x equals 1 divided by 1 plus the sum from l equals 1 to K minus 1 of e raised to the power of beta l0 plus beta l1 times x1 plus dot dot dot plus beta lp times xp.\n","Formula -> log of the fraction of the probability of Y equals k given X equals x over the probability of Y equals K given X equals x equals beta k zero plus beta k one times x one plus dot dot dot plus beta k p times x p.\n","Formula -> hat Pr(default equals Ves) equals e raised to the power of negative 3.5041 plus 0.4049 times 1, divided by 1 plus e raised to the power of negative 3.5041 plus 0.4049 times 1 equals 0.0431, hat Pr(default equals Ir given default equals Ies) equals e raised to the power of negative 3.5041 plus 0.4049 times 1, divided by 1 plus e raised to the power of negative 3.5041 plus 0.4049 times 0 equals 0.0292.\n","Formula -> log of the probability of Y equals k given X equals x divided by the probability of Y equals k prime given X equals x equals beta k zero minus beta k prime zero plus beta k one minus beta k prime one times x one plus dot dot dot plus beta k p minus beta k prime p times x p.\n","Formula -> Pr(Y=k|X=x) = (pi_k f_k(x)) / (sum_{l=1}^{K} pi_l f_l(x))\n","Formula -> f sub k of x equals one over the square root of two pi times sigma k times the exponential of negative one over two times sigma k squared times the quantity x minus mu k squared.\n","Formula -> p sub k of x equals pi sub k times one over the square root of two pi sigma times the exponential of negative one over two sigma squared times the quantity x minus mu sub k squared, all divided by the sum from l equals one to K of pi sub l times one over the square root of two pi sigma times the exponential of negative one over two sigma squared times the quantity x minus mu sub l squared.\n","Formula: \\delta_{k}(x)=x\\cdot{\\frac{\\mu_{k}}{\\sigma^{2}}}-{\\frac{\\mu_{k}^{2}}{2\\sigma^{2}}}+\\log(\\pi_{k})  \n","Solution: delta k(x) = x times mu k divided by sigma squared minus mu k squared divided by 2 sigma squared plus log pi k\n","Formula -> Pr of Y equals stroke given X equals x divided by Pr of Y equals epileptize seizure given X equals x.\n","Formula: x = \\frac{\\mu_{1}^{2}-\\mu_{2}^{2}}{2(\\mu_{1}-\\mu_{2})} = \\frac{\\mu_{1}+\\mu_{2}}{2}  \n","Solution: x equals the quantity mu one squared minus mu two squared divided by two times the quantity mu one minus mu two, which is equal to the quantity mu one plus mu two divided by two.\n","Formula -> Pr of Y equals k given X equals x is equal to the fraction with e raised to the power of beta k zero plus beta k one x one plus dot dot dot plus beta k p x p in the numerator and the sum from l equals 1 to K of e raised to the power of beta l zero plus beta l one x one plus dot dot dot plus beta l p x p in the denominator.\n","Formula -> hat mu k = 1 over n k times the sum of x i for all i where y i equals k, hat sigma squared equals the sum from i equals k to K of (x i minus hat mu k) squared.\n","Formula: \\hat{\\delta}_{k}(x)=x\\cdot\\frac{\\hat{\\mu}_{k}}{\\hat{\\sigma}^{2}}-\\frac{\\hat{\\mu}_{k}^{2}}{2\\hat{\\sigma}^{2}}+\\log(\\hat{\\pi}_{k})  \n","Solution: delta hat k of x equals x times hat mu k divided by hat sigma squared minus hat mu k squared divided by 2 hat sigma squared plus log of hat pi k.\n","Formula -> f of x equals one divided by the quantity two pi raised to the power of p over two times the absolute value of Sigma raised to the power of one half, multiplied by the exponential of negative one half times the quantity x minus mu transpose Sigma inverse times the quantity x minus mu.\n","Formula -> delta k(x) = x transpose Sigma inverse mu k - 1/2 mu k transpose Sigma inverse mu k + log pi k\n","Formula: \\( x^{T}\\Sigma^{-1}\\mu_{k}-\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}=x^{T}\\Sigma^{-1}\\mu_{l}-\\frac{1}{2}\\mu_{l}^{T}\\Sigma^{-1}\\mu_{l} \\)  \n","Solution: x transpose Sigma inverse mu k minus 1/2 mu k transpose Sigma inverse mu k equals x transpose Sigma inverse mu l minus 1/2 mu l transpose Sigma inverse mu l.\n","Formula -> delta k(x) = -1/2 times (x minus mu k) transpose Sigma k inverse (x minus mu k) minus 1/2 log of the absolute value of Sigma k plus log pi k = -1/2 x transpose Sigma k inverse x plus x transpose Sigma k inverse mu k minus 1/2 mu k transpose Sigma k inverse mu k minus 1/2 log of the absolute value of Sigma k plus log pi k.\n","Formula -> f sub k of x equals f sub k1 of x sub 1 times f sub k2 of x sub 2 times dot dot dot times f sub kp of x sub p.\n","Formula -> hat pi k = n k / n.\n"]}],"source":["formula_descriptions = []\n","for formula in formulas:\n","    answer = describe_formula_chain.invoke({\"formula\":formula[1]})\n","    print(answer)\n","    formula_descriptions.append((formula[0],answer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-gNb__PdENq"},"outputs":[],"source":["with open(\"results/formula_descriptions.pkl\",\"wb\") as file:\n","    pickle.dump(formula_descriptions,file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdUlgyqXdENq"},"outputs":[],"source":["with open(\"results/formula_descriptions.pkl\",\"rb\") as file:\n","    formula_descriptions = pickle.load(file)"]},{"cell_type":"markdown","metadata":{"id":"kag_2Gv4dENq"},"source":["**Sumarizacija tekstualnog sadržaja**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4M9eCxRfdENq"},"outputs":[],"source":["text_originals = [element.text for element in text_elements]\n","text_summaries = summarize_chain.batch(text_originals,{\"max_concurrency\":5})"]},{"cell_type":"markdown","metadata":{"id":"5I7FjPkPdENq"},"source":["**Sumarizacija tabelarnog sadržaja**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iScuvQMMdENq"},"outputs":[],"source":["table_originals = [table.text for table in table_elements]\n","table_summaries = summarize_chain.batch(table_originals,{\"max_concurrency\":5})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AdecG29qdENq"},"outputs":[],"source":["with open('results/chunking_strategy_4/4_table_summaries.pkl','rb') as fl1:\n","    table_summaries = pickle.load(fl1)\n","with open('results/chunking_strategy_4/4_txt_summaries.pkl','rb') as fl2:\n","    text_summaries = pickle.load(fl2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqLHB2FmdENq"},"outputs":[],"source":["import pickle\n","with open(\"results/chunking_strategy_4/4_txt_summaries.pkl\",\"wb\") as file:\n","    pickle.dump(text_summaries,file)\n","\n","with open(\"results/chunking_strategy_4/4_table_summaries.pkl\",\"wb\") as file2:\n","    pickle.dump(table_summaries,file2)"]},{"cell_type":"markdown","metadata":{"id":"eZTSU76qdENq"},"source":["**Kreiranje summary-ja za slike**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_Kk3lJgdENr"},"outputs":[],"source":["import io\n","import base64\n","from PIL import Image\n","from langchain_core.messages import HumanMessage\n","import uuid\n","from langchain.retrievers.multi_vector import MultiVectorRetriever, SearchType\n","from langchain.storage import InMemoryStore\n","from langchain_core.documents import Document\n","from langchain_chroma import Chroma\n","\n","\n","\n","gpt_vision_llm = ChatOpenAI(model=\"gpt-4o\",openai_api_key=OPEN_API_KEY) # gpt-4-vision-preview je zamenjen gpt-4o modelom\n","\n","def image_to_base64(img_path):\n","    with Image.open(img_path) as image:\n","        buffered = io.BytesIO()\n","        image.save(buffered,format=image.format)\n","        img_str = base64.b64encode(buffered.getvalue())\n","        return img_str.decode('utf-8')\n","\n","\n","def get_jpg_files(folder_path):\n","    jpg_files = []\n","    for root, dirs, files in os.walk(folder_path):\n","        for file in files:\n","            if file.lower().endswith('.jpg'):\n","                jpg_files.append(os.path.join(root,file))\n","    return jpg_files\n","\n","def get_decoded_representations():\n","    representations = list()\n","    folder_path = \"figures/\" # ovaj direktorijum je izbacen iz koda, slike su prebačene u bucket\n","    jpg_files = get_jpg_files(folder_path)\n","    for jpg_path in jpg_files:\n","        print(jpg_path)\n","        representations.append((jpg_path,image_to_base64(jpg_path)))\n","    return representations\n","representations = get_decoded_representations()\n","\n","image_summaries = []\n","\n","for representation in representations:\n","    summary = gpt_vision_llm.invoke([\n","    HumanMessage(\n","        content=[\n","            {\"type\":\"text\",\"text\":\"Please give me a summary of the image provided. Be descriptive.\"},\n","            {\n","                \"type\":\"image_url\",\n","                \"image_url\":{\n","                    \"url\":f\"data:image/jpeg;base64,{representation[1]}\"\n","                }\n","            }\n","            ]\n","        )\n","    ])\n","    image_summaries.append((representation[0],summary.content))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvBIS4UkdENr"},"outputs":[],"source":["with open(\"results/1_image_summaries.pkl\",\"wb\") as file2:\n","    pickle.dump(image_summaries,file2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2sWGNKZdENr"},"outputs":[],"source":["with open('results/1_image_summaries.pkl','rb') as fl3:\n","    image_summaries = pickle.load(fl3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2u--SvLhdENr","outputId":"de7d59f9-51d4-4b6d-f6ff-2446ee7be6b6"},"outputs":[{"data":{"text/plain":["('figures/figure-101-32.jpg',\n"," 'The image is a matrix of scatter plots showing pairwise relationships between six different variables: Balance, Age, Cards, Education, Income, Limit, and Rating. Each variable is plotted against each other variable in the dataset. \\n\\n- **Diagonal (from top left to bottom right)**: These squares are blank or self-referential, providing no information as they represent each variable plotted against itself.\\n\\n- **Balance**: \\n  - Positively correlated with Income, Limit, and Rating, as seen from the upward trends in the corresponding scatter plots.\\n  - No clear pattern with Age, Cards, and Education.\\n\\n- **Age**:\\n  - No clear correlation with Balance, Cards, Education, Income, Limit, or Rating.\\n\\n- **Cards**:\\n  - No clear pattern with Balance, Age, Education, Income, Limit, or Rating.\\n\\n- **Education**:\\n  - No clear pattern with Balance, Age, Cards, Income, Limit, or Rating.\\n\\n- **Income**:\\n  - Positively correlated with Balance, Limit, and Rating.\\n  - No clear pattern with Age, Cards, and Education.\\n\\n- **Limit**:\\n  - Strongly positively correlated with Balance, Income, and Rating.\\n  - No clear pattern with Age, Cards, and Education.\\n\\n- **Rating**:\\n  - Strongly positively correlated with Balance, Income, and Limit.\\n  - No clear pattern with Age, Cards, and Education.\\n\\nThe scatter plots show that the variables Limit and Rating have a very strong positive correlation, while other pairs like Balance with Income and Balance with Limit also display positive correlations. Age, Cards, and Education do not show strong correlations with any other variables.')"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["image_summaries[0]"]},{"cell_type":"markdown","metadata":{"id":"PFXqVxfodENr"},"source":["**Kreiranje MultiVector retrievera koji za docstore ima InMemoryStore za originale dokumenata, a za vectorstore ima Chroma Vectorstore**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmokop-edENr","outputId":"6617c6ae-ae7e-486d-d99b-d1b26dd3dc38"},"outputs":[{"name":"stdout","output_type":"stream","text":["1310 loaded from results/chunked_elements_1.pkl.\n","104 loaded from results/1_table_summaries.pkl.\n","1206 loaded from results/1_txt_summaries.pkl.\n","221 loaded from results/1_image_summaries.pkl.\n","268 loaded from results/formula_descriptions.pkl.\n","1799 documents stored.\n"]}],"source":["from chroma_db_connection import create_retriever\n","\n","chunks_path = \"results/chunked_elements_1.pkl\" # chunk_by_title strategy, combine_text_under_n_chars=1000,max_characters=1600, multipage_sections=True, new_after_n_chars=1200, overlap=True,\n","txt_summaries_path = \"results/1_txt_summaries.pkl\"\n","tbl_summaries_path = \"results/1_table_summaries.pkl\"\n","retriever = create_retriever(chunks_path,txt_summaries_path,tbl_summaries_path,\"similarity\",5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1wtD01_dENw"},"outputs":[],"source":["from chroma_db_connection import delete_collection\n","delete_collection(retriever)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQQJP8yHdENw","outputId":"0675bb74-6390-43a6-9a54-fb2123fc2f73"},"outputs":[{"data":{"text/plain":["[(Document(metadata={'doc_id': 'ee4b571b-4087-49d7-bb4f-eae3a132f98d'}, page_content='The text discusses the use of F-statistics and t-statistics in regression analysis to evaluate the significance of predictors in a model. It explains that the F-statistic is calculated using the residual sum of squares from two models: one that includes all variables except a subset (g) and another that includes all variables. The t-statistics and p-values for individual predictors, as shown in Table 3.4, indicate their relationship with the response variable while controlling for other predictors. For example, it is noted that TV and radio are related to sales, but newspaper is not when controlling for the other two. The text emphasizes the importance of the overall F-statistic, particularly in models with many predictors, as relying solely on individual p-values can be misleading.'),\n","  0.17132410407066345),\n"," (Document(metadata={'doc_id': '82975cea-7cbc-4008-8acb-e1a6a65664da'}, page_content='The text discusses the relationship between the response variable and predictors in regression analysis. In simple linear regression, the relationship is assessed by checking if the regression coefficient is zero. In multiple regression, the hypothesis tested is whether all regression coefficients are zero (Ho: β1 = β2 = ... = βp = 0) against the alternative that at least one coefficient is non-zero (H1). The F-statistic is calculated using the formula: F = (TSS - RSS) / p / (RSS / (n - p - 1)). TSS represents total sum of squares, and RSS represents residual sum of squares. If the null hypothesis is true, the F-statistic is expected to be close to 1, while if the alternative hypothesis is true, it is expected to be greater than 1.'),\n","  0.18957510590553284),\n"," (Document(metadata={'doc_id': '329916b0-1a0a-4a42-a1e5-7fb8fa6dbccf'}, page_content='The text discusses the limitations of using the F-statistic for testing associations between predictors and response variables when the number of predictors (p) exceeds the number of observations (n). In such cases, fitting a multiple linear regression model using least squares becomes impossible, rendering the F-statistic and other previously discussed concepts inapplicable. Instead, alternative methods like forward selection may be employed in high-dimensional settings, which will be explored further in Chapter 6.'),\n","  0.209781676530838),\n"," (Document(metadata={'doc_id': '6d316ee9-855f-4040-a808-b4f4fc138b89'}, page_content='The text discusses the relationship between advertising budgets (TV, newspaper, and radio) and the number of units sold, as analyzed through a least squares regression model. It highlights the importance of the sample size (n) and the number of predictors (p) in determining the significance of the F-statistic. A larger sample size allows for a smaller F-statistic to provide evidence against the null hypothesis (Ho), while a smaller sample size requires a larger F-statistic for the same evidence. The F-statistic follows an F-distribution when the null hypothesis is true and errors are normally distributed. For the advertising data, the p-value associated with the F-statistic is nearly zero, indicating strong evidence that at least one advertising medium is linked to increased sales. Additionally, the text mentions the possibility of testing specific subsets of coefficients to determine their significance.'),\n","  0.23675239086151123)]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["retriever.vectorstore.similarity_search_with_score(\"What is the F-statistic, and how is it used in multiple regression to test model significance?\",k=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9In1DS4mdENw","outputId":"aec6e7e8-66b3-44af-8a23-6d0924f7ccca"},"outputs":[{"data":{"text/plain":["['Ho: Bp—g+1 = Bp—qt2 = +++ = Bp = 9,\\n\\nwhere for convenience we have put the variables chosen for omission at the end of the list. In this case we fit a second model that uses all the variables except those last g. Suppose that the residual sum of squares for that model is RSSo. Then the appropriate F-statistic is\\n\\n_ (RSSo — RSS)/q ~ RSS/(n— p—1) F (3.24)\\n\\nNotice that in Table 3.4, for each individual predictor a t-statistic and a p-value were reported. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these is exactly equivalent’ to the F- test that omits that single variable from the model, leaving all the others in—i.e. g=1 in (3.24). So it reports the partial effect of adding that variable to the model. For instance, as we discussed earlier, these p-values indicate that TV and radio are related to sales, but that there is no evidence that newspaper is associated with sales, when TV and radio are held fixed.\\n\\nGiven these individual p-values for each variable, why do we need to look at the overall F-statistic? After all, it seems likely that if any one of the p-values for the individual variables is very small, then at least one of the predictors is related to the response. However, this logic is flawed, especially when the number of predictors p is large.',\n"," 'One: Is There a Relationship Between the Response and Predictors?\\n\\nRecall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether $; = 0. In the multiple regression setting with p predictors, we need to ask whether all of the regression coefficients are zero, i.e. whether 6, = 62 = --- = 6p = 0. As in the simple linear regression setting, we use a hypothesis test to answer this question. We test the hypothesis,\\n\\nHo : 61 = 62 =+--= By =0\\n\\nversus the alternative\\n\\nHI, : at least one {; is non-zero.\\n\\nThis hypothesis test is performed by computing the F’-statistic,\\n\\n__ (TSS — RSS) /p re RSS/(n —p-1)’ (3.23)\\n\\nwhere, as with simple linear regression, TSS = 7(y; — y)? and RSS = (yi - Hi). If the linear model assumptions are correct, one can show that\\n\\nE{RSS/(n — p—1)} = 0?\\n\\nand that, provided Ho is true,\\n\\nE{(TSS — RSS)/p} = 0”.\\n\\nHence, when there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1. On the other hand, if H, is true, then E{(TSS — RSS)/p} > o?, so we expect F to be greater than 1.',\n"," 'The approach of using an F-statistic to test for any association between he predictors and the response works when p is relatively small, and cer- ainly small compared to n. However, sometimes we have a very large num- ber of variables. If p > n then there are more coefficients 6; to estimate than observations from which to estimate them. In this case we cannot even fit the multiple linear regression model using least squares, so the F- statistic cannot be used, and neither can most of the other concepts that we have seen so far in this chapter. When p is large, some of the approaches iscussed in the next section, such as forward selection, can be used. This high-dimensional setting is discussed in greater detail in Chapter 6. t. t',\n"," 'TABLE 3.6. More information about the least squares model for the regression of number of units sold on TV, newspaper, and radio advertising budgets in the Advertising data. Other information about this model was displayed in Table 3.4.\\n\\nconclude that there is a relationship? It turns out that the answer depends on the values of n and p. When n is large, an F-statistic that is just a little larger than 1 might still provide evidence against Hp. In contrast, a larger F-statistic is needed to reject Ho if n is small. When Hp is true and the errors ¢; have a normal distribution, the F-statistic follows an F-distribution.® For any given value of n and p, any statistical software package can be used to compute the p-value associated with the F-statistic using this distribution. Based on this p-value, we can determine whether or not to reject Ho. For the advertising data, the p-value associated with the F-statistic in Table 3.6 is essentially zero, so we have extremely strong evidence that at least one of the media is associated with increased sales.\\n\\nIn (3.23) we are testing Ho that all the coefficients are zero. Sometimes we want to test that a particular subset of q of the coefficients are zero. This corresponds to a hypothesis',\n"," 'The F-statistic for the multiple linear regression model obtained by re- gressing sales onto radio, TV, and newspaper is shown in Table 3.6. In this example the F-statistic is 570. Since this is far larger than 1, it provides compelling evidence against the hypothesis Ho. In other words, the large F-statistic suggests that at least one of the advertising media must be related to sales. However, what if the F-statistic had been closer to 1? How large does the F-statistic need to be before we can reject Hp and\\n\\nF-statistic']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["retriever.invoke(\"What is the F-statistic, and how is it used in multiple regression to test model significance?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCOouhRQdENw","outputId":"a5edaf13-06a3-40c9-9de3-c8e2a58bfce7"},"outputs":[{"data":{"text/plain":["1799"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["len(retriever.vectorstore.get()['documents'])"]},{"cell_type":"markdown","metadata":{"id":"csPODw8cdENx"},"source":["**Document grader**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8F9vT7WxdENx"},"outputs":[],"source":["# data model for structured output\n","class GradeDocument(BaseModel):\n","    \"\"\"Checks if document is relevant for asked question.\"\"\"\n","    binary_score: str = Field(\n","        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n","    )\n","\n","structured_llm_grader = chat.with_structured_output(GradeDocument)\n","system_grader = \"\"\"\n","You are a grader assessing relevance of a retrieved document to a user question. \\n\n","    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n","    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n","    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\n","\"\"\"\n","grade_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system_grader),\n","        (\"human\",\"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n","    ],\n",")\n","retrieval_grader = grade_prompt | structured_llm_grader\n","\n","def grade_retrieved_documents(input_dictionary:dict):\n","    print(\"INPUT\",input_dictionary)\n","    question = input_dictionary['question']\n","    documents = input_dictionary['context_documents']\n","    filtered_documents = []\n","    print(f\"U funkciju za ocenjivanje dokumenata uslo je:{len(documents)}\")\n","    for d in documents:\n","        #print(\"DOKUMENT:\",d,\"TIP:\",type(d))\n","        score = retrieval_grader.invoke({\"question\":question,\"document\":d})\n","        if score.binary_score == \"yes\":\n","            filtered_documents.append(d)\n","        else:\n","            continue\n","    print(f\"Broj dokumenata koji pripadaju datom kontekstu:{len(filtered_documents)}\")\n","    print(filtered_documents)\n","    return {\"context_documents\":filtered_documents,\"question\":question}"]},{"cell_type":"markdown","metadata":{"id":"MNFbD190dENx"},"source":["**Multiple perspectives**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuWIRhs1dENx"},"outputs":[],"source":["from langchain_core.runnables import Runnable\n","from  langchain.load import dumps,loads\n","from operator import itemgetter\n","from langchain_core.runnables import RunnableLambda\n","\n","def get_unique_union(documents: list[list]):\n","    \"\"\"Method creates set of retrieved documents.\"\"\"\n","    all_docs = [dumps(doc) for sublist in documents for doc in sublist]\n","    unique_docs = list(set(all_docs))\n","    return [loads(doc) for doc in unique_docs]\n","\n","class MultiQuery(BaseModel):\n","    \"\"\"Make a list of different perspectives of initial query.\"\"\"\n","    queries: list[str] = Field(\n","        description=\"Given a user question make five different perspectives of initial query.\"\n","    )\n","\n","def reciprocal_rank_fusion(results: list[list], k=60):\n","    \"\"\"This method ranks documents based on the formula for fused scores and their individual rank from each question and optional parameter k\"\"\"\n","    fused_scores = {}\n","    for documents in results:\n","        for rank, document in enumerate(documents):\n","            document_string = dumps(document)\n","            if document_string not in fused_scores: # ako ne postoji ovaj dokument, dodati ga u listu\n","                fused_scores[document_string] = 0\n","            previous_score = fused_scores[document_string] # svakako je prethodni skor 0 ili >0\n","            fused_scores[document_string] += 1/(rank + k)\n","\n","    # sada je potrebno sortirati dokumente po formuli od najveceg skora do najmanjeg\n","    reranked_results = [(loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda el: el[1], reverse=True)]\n","    return reranked_results\n","\n","system = \"\"\"You are an AI language model assistant. Your task is to generate five\n","different versions of the given user question to retrieve relevant documents from a vector\n","database. By generating multiple perspectives on the user question, your goal is to help\n","the user overcome some of the limitations of the distance-based similarity search.\n","Provide these alternative questions separated by newlines. Original question: {question}\n","\"\"\"\n","\n","fusion_template = ChatPromptTemplate.from_messages([\n","    (\"system\",system),\n","    (\"human\",\"{question}\")\n","])\n","\n","rag_fusion_llm = chat.with_structured_output(MultiQuery)\n","query_generation_chain = (\n","    fusion_template\n","    | rag_fusion_llm\n",")\n","\n","multi_query_retrieval_with_rag_fusion_chain = (query_generation_chain\n","                                               |(lambda x: x.queries)\n","                                               | retriever.map()\n","                                               | reciprocal_rank_fusion\n","                                               )\n","\n","final_chain_template = \"\"\"\n","You don't have any knowledge in statistics and machine learning fields.\n","You are only allowed to answer the questions based on the data from provided context:\n","Also, you are not allowed to correct answers based on your knowledge not provided from the context.\n","Your answer must be completely created from the retrieved context.\n","You are not allowed to correct context provided to you based on your previous knowledge.\n","If answer is not contained in your context, you will tell to the user that you don't know the answer and sugest to the user to be more specific.\n","When you answer the question, you need to reference the explicit part of allowed context where answer came from, especialy if user asks you to provide formulas!\n","\n","\\n\n","Allowed context for answering the user question:\n","\\n\n","\\n\n","{context_documents}\n","\\n\n","\\n\n","Question: {question}\n","\"\"\"\n","final_prompt = ChatPromptTemplate.from_template(final_chain_template)\n","final_rag_chain = (\n","    {\"context_documents\":multi_query_retrieval_with_rag_fusion_chain,\n","     \"question\":itemgetter(\"question\")}\n","    | RunnableLambda(grade_retrieved_documents) # retrieval check, validacija da li je pitanje utemeljeno sa kontekstom\n","    | final_prompt\n","    | chat\n","    | StrOutputParser()\n",")\n","retriever.search_kwargs = {\"k\":20}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ayKwXmjAdENx","outputId":"c884393d-e4e3-4885-dc2a-af13318d44f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["INPUT {'context_documents': [('TABLE 4.7. Important measures for classification and diagnostic testing, derived from quantities in Table 4.6.\\n\\nalternative (non-null) hypothesis. In the context of the Default data, “+” indicates an individual who defaults, and “—” indicates one who does not.\\n\\nTable 4.7 lists many of the popular performance measures that are used in this context. The denominators for the false positive and true positive rates are the actual population counts in each class. In contrast, the denominators for the positive predictive value and the negative predictive value are the total predicted counts for each class.', 0.08075130698081517), ('qualitative, 2, 27, 91, 135, 167, 202 variable, 91-94 quantitative, 2, 27, 91, 135, 167, 202 radial kernel, 381, 383, 390 random forest, 11, 331, 343, 346— 347, 354, 360-361 random seed, 46 re-sampling, 577-582 recall, 155 receiver operating characteristic (ROC), 154, 382-383 recommender systems, 516 rectified linear unit, 401 recurrent neural network, 416-427 recursive binary splitting, 334, 337, 338 reducible error, 17, 90 regression, 2, 11, 27 local, 289, 290, 304-305 iecewise polynomial, 294-295 olynomial, 289-292, 299 spline, 289, 294 tree, 331-337, 358-360 regularization, 230, 240, 406, 484— 486 ReLU, 401 resampling, 201-214 residual, 71, 81 plot, 100 standard error, 75, 77-78, 88- 89, 109 studentized, 104 sum of squares, 71, 79, 81 residuals, 263, 348 response, 15 ridge regression, 11, 240-244, 385, 484 risk set, 473\\n\\nrobust, 374, 376, 535 ROC curve, 154, 382-383, 486— 487 R?, 77-80, 88, 109, 238 rug plot, 314\\n\\nscale equivariant, 242 Scheffé’s method, 572 scree plot, 512, 514-515', 0.08046398046398046), ('accuracy, 415\\n\\nactivation, 400 activation function, 401 additive, 11, 94-98, 110-111 additivity, 305, 306 adjusted R?, 87, 231, 232, 236- 238 Advertising data set, 15, 16, 19, 69, 71-73, 77, 78, 80, 82, 83, 85, 87-90, 95, 96, 109— 111 agglomerative clustering, 525 Akaike information criterion, 87, 231, 232, 236-238 alternative hypothesis, 76, 559 analysis of variance, 312 ANOVA, 587 area under the curve, 155, 486— 487 argument, 40 array, 42 attribute, 42 AUC, 155 Auto data set, 12, 66, 98-101, 129, 197, 202-207, 327, 398 auto-correlation, 421 autoregression, 423 axes, 48 backfitting, 307, 328\\n\\nauto-correlation, 421 autoregression, 423 axes, 48\\n\\nbackpropagation, 429 backward stepwise selection, 87, 234-235\\n\\nbackward stepwise selection, 87, 234-235 bag-of-n-grams, 415 bag-of-words, 414 bagging, 11, 24, 331, 343-346, 354, 360-361 BART, 343, 350, 353, 354, 362- baseline, 93, 145, 161 basis function, 293-294, 296 Bayes classifier, 35-37, 147 decision boundary, 148 error, 35-37 Bayes’ theorem, 146, 250 Bayesian, 250-251, 353 Bayesian additive regression trees, 331, 343, 350, 350, 353, 354, 362-363 Bayesian information criterion, 87, 231, 232, 236-238 Benjamini—Hochberg procedure, 575— 577 Bernoulli distribution, 172 best subset selection, 231, 246 bias, 31-34, 74, 90, 159, 405 bias-variance decomposition, 32', 0.07744189653427455), ('logistic regression\\n\\nlinear\\n\\ndiscriminant.\\n\\nanaly\\n\\nquadratic discriminant.\\n\\nanalysis\\n\\nnaive Bayes\\n\\nK-nearest\\n\\nneighbors\\n\\ngeneralized linear\\n\\nmodels\\n\\nPoisson\\n\\nregression\\n\\n1. A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\\n\\n2. An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.\\n\\n3. On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.\\n\\nJust as in the regression setting, in the classification setting we have a set of training observations (x1, y1),---,(%n,Yn) that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.', 0.07564449556119726), ('elbow, 514 semi-supervised learning, 27 sensitivity, 153, 155, 156 separating hyperplane, 367-372 Seq2Seq, 425 sequence, 41 shrinkage, 230, 240, 484-486 penalty, 240 sigmoid, 40 signal, 252 signature, 45 singular value decomposition, 539 slack variable, 375 slice, 51 slope, 71, 72 Smarket data set, 2, 3, 12, 173, 184, 196 smoother, 308 smoothing spline, 290, 300-303 soft margin classifier, 372-374 soft-thresholding, 250 softmax, 145, 405 sparse, 244, 252 sparse matrix format, 414 sparsity, 244 specificity, 153, 155, 156 spline, 289, 294-303 cubic, 296 linear, 296 natural, 297, 301 regression, 289, 294-299 smoothing, 30, 290, 300-303 thin-plate, 22 andard error, 75, 101 andardize, 185 atistical model, 1 ep function, 111, 289, 292-293 epwise model selection, 11, 231, 233 HRHRHRK DB\\n\\nochastic gradient descent, 429 ring, 41 ring interpolation, 490 ump, 349 subset selection, 230-240 subtree, 336 supervised learning, 25-27, 261 support vector, 371, 376, 385 classifier, 367, 372-377 machine, 5, 11, 24, 377-386 Ss Ss', 0.07471583707437754), ('It is also important to sures of model fit in the p> n, it is easy to obtain a useless model fore, one should never use sum of squared er: other traditional measures of model fit on t a good model fit in the in Figure 6.23, one can Reporting this fact might mis valid and useful model absolutely no evidence of a compelling model. report results on an indepen instance, the MSE or R? on an independen model fit, but the MSE on the training set cer’ be particularly care: high-dimensional se ne ‘ul in reporting errors and mea- ting. We have seen that when hat has zero residuals. There- rors, p-values, R? statistics, or raining data as evidence of high-dimensional se easily lead others int has been obtained, obtain a model ent test set, or ting. For instance, as we saw with R? = 1 when p > n. o thinking that a statistically whereas in fact this provides It is important to instead cross-validation errors. For test set is a valid measure of ainly is not.\\n\\nIn this lab we implement many of the techniques discussed in this chapter. We import some of our libraries at this top level.\\n\\n[1]: import numpy as np import pandas as pd from matplotlib.pyplot import subplots from statsmodels.api import OLS import sklearn.model_selection as skm import sklearn.linear_model as skl from from from from sklearn.preprocessing import StandardScaler ISLP import load_data ISLP.models import ModelSpec as MS functools import partial', 0.07327912853859557), ('2.2.3 The Classification Setting\\n\\nThus far, our discussion of model accuracy has been focused on the regres- sion setting. But many of the concepts that we have encountered, such as the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that y; is no longer quan- titative. Suppose that we seek to estimate f on the basis of training obser- vations {(71,y1),.. 1>Yn)}, Where now y1,.--,Yn are qualitative. The most common approach for quantifying the accuracy of our estimate f is the training error rate, the proportion of mistakes that are made if we apply\\n\\nbias-variance trade-off\\n\\nerror rate\\n\\nour estimate f to the training observations:\\n\\n1 = lui 4 Gi). (2.8) i=1\\n\\nHere %; is the predicted class label for the ith observation using f. And I(y; # GH) is an indicator variable that equals 1 if y; A Gj and zero if y; = G. If I(y; A 9) = 0 then the ith observation was classified correctly by our classification method; otherwise it was misclassified. Hence Equation 2.8 computes the fraction of incorrect classifications.\\n\\nEquation 2.8 is referred to as the training error rate because it is com- puted based on the data that was used to train our classifier. As in the regression setting, we are most interested in the error rates that result from applying our classifier to test observations that were not used in training. The test error rate associated with a set of test observations of the form (xo, yo) is given by', 0.062399843888819126), ('4.5.2 An Empirical Comparison\\n\\nWe now compare the empirical (practical) performance of logistic regres- sion, LDA, QDA, naive Bayes, and KNN. We generated data from six dif- erent scenarios, each of which involves a binary (two-class) classification problem. In three of the scenarios, the Bayes decision boundary is linear, and in the remaining scenarios it is non-linear. For each scenario, we pro- duced 100 random training data sets. On each of these training sets, we fit each method to the data and computed the resulting test error rate on a large test set. Results for the linear scenarios are shown in Figure 4.11, and the results for the non-linear scenarios are in Figure 4.12. The KNN method requires selection of AK, the number of neighbors (not to be con- used with the number of classes in earlier sections of this chapter). We performed KNN with two values of kK: K = 1, and a value of K that was chosen automatically using an approach called cross-validation, which we discuss further in Chapter 5. We applied naive Bayes assuming univariate Gaussian densities for the features within each class (and, of course — since this is the key characteristic of naive Bayes — assuming independence of the features).', 0.05811610397211803), (\"SimpleModule. classifi-\\n\\ncation()\\n\\n[42]: mnist_results = pd.read_csv(mnist_logger.experiment. metrics_file_path) fig, ax = subplots(1, 1, figsize=(6, 6)) summary_plot(mnist_results, ax, col='accuracy', ylabel='Accuracy') ax.set_ylim([0.5, 1]) ax.set_ylabel('Accuracy') ax.set_xticks(np.linspace(0, 30, 7).astype(int));\\n\\nIn\\n\\nOnce again we evaluate the accuracy using the test() method of our trainer. This model achieves 97% accuracy on the test data.\\n\\nIn\\n\\n[43]: mnist_trainer.test (mnist_module, datamodule=mnist_dm)\\n\\nOut [43] : [{'test_loss': 0.1471, 'test_accuracy': 0.9681}]\\n\\nTable 10.1 also reports the error rates resulting from LDA (Chapter 4) and multiclass logistic regression. For LDA we refer the reader to Section 4.7.3. Although we could use the sklearn function LogisticRegression() to fit multiclass logistic regression, we are set up here to fit such a model with torch. We just have an input layer and an output layer, and omit the hidden layers!\\n\\nIn [44]: class MNIST_MLR(nn. Module): def __init__(self): super (MNIST_MLR, self).__init__() self.linear = nn.Sequential(nn.Flatten(), nn.Linear(784, 10)) def forward(self, x): return self.linear(x) mlr_model = MNIST_MLR() mlr_module = SimpleModule.classification(mlr_model) mlr_logger = CSVLogger('logs', name='MNIST_MLR')\\n\\n[45]: mlr_trainer = Trainer(deterministic=True, max_epochs=30, callbacks=[ErrorTracker ()]) mlr_trainer.fit(mlr_module, datamodule=mnist_dm)\", 0.054761302294197026), ('/td><td></td><td>Generality</td><td></td><td></td><td>.</td><td>72</td></tr><tr><td>4.7</td><td>Lab:</td><td>Logistic Regression, LDA,</td><td></td><td></td><td>QDA,</td><td></td><td>and</td><td></td><td>KNN</td><td></td><td></td><td></td><td>......</td><td></td><td>73</td></tr></tbody></table>', 0.04504312675751676), ('classification error rate\\n\\nGini index\\n\\n= — Training — Cross-Validation — Test 08 i 0.2 0.0 L Tree Size\\n\\nMean Squared Error\\n\\nFIGURE 8.5. Regression tree analysis for the Hitters data. The training, cross-validation, and test MSE are shown as a function of the number of terminal nodes in the pruned tree. Standard error bands are displayed. The minimum cross-validation error occurs at a tree size of three.\\n\\nnode purity—a small value indicates that a node contains predominantly observations from a single class.\\n\\nAn alternative to the Gini index is entropy, given by\\n\\nK D=- Ss Pmk log Pmk- (8.7) k=l\\n\\nSince 0 < Pmxz < 1, it follows that 0 < —pms log Pm. One can show that the entropy will take on a value near zero if the pm,’s are all near zero or near one. Therefore, like the Gini index, the entropy will take on a small value if the mth node is pure. In fact, it turns out that the Gini index and the entropy are quite similar numerically.\\n\\nWhen building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.', 0.04387881031716648), ('<table><tbody><tr><td>Predicted</td><td>— or Null</td><td>True Neg. (TN)</td><td>False Neg. (FN)</td><td>N*</td></tr><tr><td>class</td><td>+ or Non-null</td><td>False Pos. (FP)</td><td>True Pos. (TP)</td><td>p*</td></tr></tbody></table>', 0.043269344498852697), (\"export_text ()\\n\\nIn order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data. This pattern is similar to that in Chapter 6, with the linear models replaced here by decision trees — the code for validation is almost identical. This approach leads to correct predictions for 68.5% of the locations in the test data set.\\n\\nvalidation = skm.ShuffleSplit(n_splits=1, test_size=200, random_state=0) results = skm.cross_validate(clf, D, High, cv=validation) results['test_score']\\n\\nIn [10]:\\n\\nOut [10] : array ([0.685])\\n\\nNext, we consider whether pruning the tree might lead to improved clas- sification performance. We first split the data into a training and test set. We will use cross-validation to prune the tree on the training set, and then evaluate the performance of the pruned tree on the test set.\", 0.04208494208494208), ('semi-\\n\\nsupervised learning\\n\\nquantitative qualitative categorical\\n\\nclass\\n\\nregression classification\\n\\nbinary\\n\\nbest, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.\\n\\nIn this section, we discuss some of the most important concepts that arise in selecting a statistical learning procedure for a specific data set. As the book progresses, we will explain how the concepts presented here can be applied in practice.\\n\\n2.2.1 Measuring the Quality of Fit\\n\\nIn order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by\\n\\nMSE = — 9 (yi — f(a)’, (2.5) i=1\\n\\nwhere f(a) is the prediction that f gives for the ith observation. The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.', 0.04134927849701681), ('<td></td><td></td><td></td><td>..........</td><td></td><td></td><td></td><td>56</td></tr><tr><td>44 4</td><td>Naive Bayes... 2...</td><td></td><td></td><td></td><td></td><td>0.0000.</td><td></td><td></td><td></td><td>0.000.</td><td></td><td>2.000.</td><td></td><td>58</td></tr><tr><td>Comparison &gt;</td><td>of Classification</td><td></td><td></td><td>Methods</td><td></td><td></td><td></td><td></td><td></td><td>..........</td><td></td><td></td><td></td><td>61</td></tr><tr><td rowspan=\"2\"></td><td>5.</td><td>An Analytical Comparison...</td><td></td><td></td><td></td><td></td><td>2...</td><td></td><td></td><td></td><td>......00.</td><td></td><td></td><td></td><td>61</td></tr><tr><td>4.5.2</td><td>An Empirical Comparison</td><td></td><td></td><td></td><td></td><td>..............</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>64</td></tr><tr><td rowspan=\"3\">4.6</td><td>Generalized</td><td>Linear Models ..</td><td></td><td></td><td></td><td></td><td>2.2.2...</td><td></td><td></td><td></td><td>0000000.%</td><td></td><td></td><td></td><td>67</td></tr><tr><td>6. Ss</td><td>Linear Regression on the</td><td></td><td></td><td>Bikeshare</td><td></td><td></td><td></td><td></td><td></td><td>Data...</td><td></td><td>..</td><td>.</td><td>67</td></tr><tr><td>4.6.2</td><td>Poisson Regression on</td><td></td><td>the</td><td></td><td></td><td>Bikeshare</td><td></td><td></td><td></td><td>Data</td><td></td><td>....</td><td>.</td><td>69</td></tr><tr><td></td><td>4.6.3.</td><td>Generalized Linear Models</td><td></td><td></td><td>in</td><td></td><td>Greater</td><td></td><td><', 0.04130672926447575), ('Figure 4.7 illus old value for the is to be expected known to have the lowest overa rates the trade-off error ra Bayes ¢. ,; since the used, the error rate among the indivi dashed line). As who default decreases stea who do not default increases. How can we best? Such a decision must be based on domain knowledge, such as detai he thresho! ily, but t information about the costs associate: hat results from mo: posterior probability of shown as a function of the threshol (4.26), minimizes the overal 1 error d is reduced, value. Using a threshold hown as a black so. assifier uses a threshold of e, S rate. But when a ne error ra decide which with default. hresho: uals who default is qui he error rate among individu e among the individu: hreshold value is g fo) id line. T 0.5 an e high (b no. d of 0.5 i ifyin, he thresh- default. Various error rates are in his als als ed\\n\\nThe ROC curve is a popular graphic for simul taneously disp. aying he two types of errors for all possible thresholds. The name “ROC” is historic, and comes from communications theory. It is an erating characteristics. Figure 4.8 displays the acronym for receiver op- ROC curve for the LDA classifier on the training data. The overall performance of a classifier, sum-', 0.030798389007344232), ('marized over all possible thresholds, is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. For this data the AUC is 0.95, which is close to the maximum of 1.0, so would be considered very good. We expect a classifier that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training). ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds. It turns out that the ROC curve for the logistic regression model of Section 4.3.4 fit to these data is virtually indistinguishable from this one for the LDA model, so we do not display it here.\\n\\nAs we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the sensitivity and one minus the specificity of our classifier. Since there is an almost bewildering array of terms used in this context, we now give a summary. Table 4.6 shows the possible results when applying a classifier (or diagnostic test) to a population. To make the connection with the epidemiology literature, we think of “+” as the “disease” that we are trying to detect, and “—” as the “non-disease” state. To make the connection to the classical hypothesis testing literature, we think of “—” as the hypothesis and “+” as the', 0.029709507042253523), ('criterion\\n\\nadjusted R?\\n\\n¢,\\n\\n30000 30000 L 25000 25000 L 20000 BIC 20000 L Adjusted R? 15000 15000 L 086 0.88 0.90 092 0.94 0.96 40000 10000 \\\\ * 111 111 2 4 6 8 10 2 4 6 8 10 2 4 6 8 10 Number of Predictors Number of Predictors Number of Predictors\\n\\nFIGURE 6.2. Cp, BIC, and adjusted R? are shown for the best models of each size for the Credit data set (the lower frontier in Figure 6.1). Cp and BIC are estimates of test MSE. In the middle plot we see that the BIC estimate of test error shows an increase after four variables are selected. The other two plots are rather flat after four variables are included.\\n\\nfor the corresponding decrease in training RSS. Though it is beyond the scope of this book, one can show that if 6? is an unbiased estimate of o? in (6.2), then C) is an unbiased estimate of test MSE. As a consequence, the C, statistic tends to take on a small value for models with a low test error, so when determining which of a set of models is best, we choose the model with the lowest C;, value. In Figure 6.2, Cp selects the six-variable model containing the predictors income, limit, rating, cards, age and student.\\n\\nThe AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model (6.1) with Gaussian errors, maximum likelihood and least squares are the same thing. In this case AIC is given by', 0.029411764705882353), ('Validation and Cross-Validation\\n\\nAs an alternative to the approaches just discussed, we can directly esti- mate the test error using the validation set and cross-validation methods discussed in Chapter 5. We can compute the validation set error or the cross-validation error for each model under consideration, and then select\\n\\n° 5 84 2 e4 a g 8 & 8 rr < 3 3 34 8 3] [4 se Se 2 2° s° s gs 1 z 2 24 g 24 3B $ fs £4 sf C84 Ee Syg i t e | nena 3] Nye a = Le Le 2 4 6 8 1 2 4 6 8 2 4 6 8 Number of Predictors Number of Predictors Number of Predictors\\n\\nFIGURE 6.3. For the Credit data set, three quantities are displayed for the best model containing d predictors, for d ranging from 1 to 11. The overall best model, based on each of these quantities, is shown as a blue cross. Left: Square root of BIC. Center: Validation set errors. Right: Cross-validation errors.', 0.02904040404040404), ('The MSE in (2.5) is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE. But in general, we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the pre- dictions that we obtain when we apply our method to previously unseen test data. Why is this what we care about? Suppose that we are interested in developing an algorithm to predict a stock’s price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don’t really care how well our method predicts last week’s stock price. We instead care about how well it will predict tomorrow’s price or next month’s price. On a similar note, suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learn- ing method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes.', 0.028850145288501453), ('After a bit more exploration with the lasso model, we identified an even simpler model with four variables. We then refit the linear model with these four variables to the training data (the so-called relaxed lasso), and achieved a test mean absolute error of 224.8, the overall winner! It is tempting to present the summary table from this fit, so we can see coefficients and p- values; however, since the model was selected on the training data, there would be selection bias. Instead, we refit the model on the test data, which was not used in the selection. Table 10.3 shows the results.\\n\\nWe have a number of very powerful tools at our disposal, including neural networks, random forests and boosting, support vector machines and gen- eralized additive models, to name a few. And then we have linear models, and simple variants of these. When faced with new data modeling and pre- diction problems, it’s tempting to always go for the trendy new methods. Often they give extremely impressive results, especially when the datasets are very large and can support the fitting of high-dimensional nonlinear models. However, if we can produce models with the simpler tools that\\n\\nOccam’s razor\\n\\nperform as well, they are likely to be easier to fit and understand, and po- tentially less fragile than the more complex approaches. Wherever possible, it makes sense to try the simpler models as well, and then make a choice based on the performance/complexity tradeoff.', 0.027783137179239824), ('factor, 92\\n\\nfactorial, 170 failure time, 470 false discovery proportion, 155, 573 discovery rate, 558, 573-577, 579-582 negative, 155, 562 positive, 155, 562, 563 positive rate, 155, 156, 382 amily-wise error rate, 565-573, 577 eature, 15 eature map, 406 eature selection, 230 eaturize, 414 eed-forward neural network, 400 figure, 48 fit, 21 fitted value, 101 flattening, 424 flexible, 21 floating point, 43 orward stepwise selection, 86, 87, 233-234, 268 ‘unction, 40 Fund data set, 12, 567-570, 572, 575, 576, 585, 588, 589 Gamma, 173 Gaussian (normal) distribution, 146,\\n\\nGamma, 173 Gaussian (normal) distribution, 146, 147, 150, 172, 561 generalized additive model, 5, 24, 162, 289, 290, 305-309, 319 generalized linear model, 5, 135, 167-174, 217 generative model, 146-161 Gini index, 337-339, 345, 346, 363 global minimum, 427 gradient, 428 gradient descent, 427\\n\\nHarrell’s concordance index, 487 hazard function, 476-478 baseline, 478 hazard rate, 476 Heart data set, 339, 340, 344-347, 352, 353, 382, 383 heatmap, 50 helper, 311', 0.027313266443701224), ('main effects, 96 majority vote, 344 Mallow’s C,, 87, 231, 232, 236— 238 Mantel-Haenszel test, 475 margin, 370, 385 marginal distribution, 158 Markov chain Monte Carlo, 353 matrix completion, 515 matrix multiplication, 10 maximal margin classifier, 367-372 hyperplane, 370 maximum likelihood, 139-141, 143, 70 mean squared error, 28 mesh, 53 method, 43 minibatch, 429 misclassification error, 35 missing at random, 515 missing data, 56, 515-520 mixed selection, 87 MNIST data set, 402-404, 406, 430, 431, 441, 444, 445, 448 model assessment, 201 model selection, 201 module, 42 multicollinearity, 108, 266 multinomial logistic regression, 145, 63 multiple testing, 557-583 multi-task learning, 403 multivariate 150\\n\\nGaussian,\\n\\nmultivariate normal, 150\\n\\nnaive Bayes, 135, 158-161, 164— 167 namespace, 116 natural spline, 297, 298, 301, 317 NCI60 data set, 4, 5, 12, 546, 548— 550 negative binomial, 173 negative predictive value, 155, 156 neural network, 5, 399 node internal, 333 urity, 337-339 erminal, 333 noise, 21, 252 non-linear, 2, 11, 289-329 decision boundary, 377-382 kernel, 377-382 non-parametric, 20, 22-23, 111— 115, 193 normal (Gaussian) distribution, 146, 147, 150, 172, 476, 561 notebook, 40 null, 152 distribution, 561, 578 hypothesis, 76, 559 model, 87, 231, 245 rate, 186 NYSE data set, 12, 422-424, 466, 467 Occam’s razor, 426 odds, 140, 145, 195 OJ data set, 12, 365, 398 one-hot encoding, 92, 126, 403', 0.027272727272727275), ('bottom-up, 525 hierarchical, 521, 525-535 k-means, 11, 521-524 Cochran—Mantel—Haenszel test, 475 coefficient, 71 College data set, 12, 65, 286, 328 collinearity, 106-110 concatenation, 41 conditional probability, 35 confidence interval, 75-76, 90, 110, 292 confounding, 144 confusion matrix, 153, 176 continuous, 2 contour, 246 contour plot, 50 contrast, 94 convenience function, 53 convolution filter, 407 convolution layer, 407 convolutional neural network, 406— 413 correlation, 79, 82-83, 530 count data, 167, 170 Cox’s proportional hazards model, 480, 483-486 Cp, 87, 231, 232, 236-238 Credit data set, 12, 91, 92, 94, 97, 98, 106-109 cross-entropy, 405 cross-validation, 11, 31, 34, 201— 211, 231, 252, 270 k-fold, 206-209 leave-one-out, 204-206 curse of dimensionality, 115, 193, 266\\n\\ndata augmentation, 411 data frame, 55 Data sets Advertising, 15, 16, 19, 69, 71-73, 77, 78, 80, 82, 83, 85, 87-90, 95, 96, 109- 111 Auto, 12, 66, 98-101, 129, 197, 202-207, 327, 398 Bikeshare, 12, 167-172\\n\\nBoston, 12, 67, 117, 122, 133, 199, 227, 287, 327, 364, 556\\n\\nBrainCancer, 12, 472-474, 476, 482\\n\\nCaravan, 12, 184, 366 Carseats, 12, 126, 130, 364 CIFAR100, 406, 409-411, 448, 449', 0.02684684684684685), ('College, 12, 65, 286, 328 Credit, 12, 91, 92, 94, 97, 98, 106-109 Default, 12, 136-139, 141- 144, 152-156, 160, 161, 225, 226, 466 Fund, 12, 567-570, 572, 575, 576, 585, 588, 589 Heart, 339, 340, 344-347, 352, 353, 382, 383 Hitters, 12, 332, 333, 336, 338, 339, 366, 425, 426, 437, 446 IMDb, 413, 415, 416, 418, 420, 437, 458, 467 Income, 16-18, 21-23 Khan, 12, 579-581, 583, 590, 593 MNIST, 402-404, 406, 430, 431, 441, 444, 445, 448 NCI60, 4, 5, 12, 546, 548-550 NYSE, 12, 422-424, 466, 467 OJ, 12, 365, 398 Portfolio, 12 Publication, 12, 482-487 Smarket, 2, 3, 12, 173, 184, 196 USArrests, 12, 507, 508, 510, 512, 513, 515, 516, 518, 519 Wage, 1, 2, 8, 9, 12, 290, 291, 293, 295, 297-300, 302— 306, 309, 315, 327 Weekly, 12, 196, 226 type, 42 decision function, 387 decision tree, 11, 331-342\\n\\ndata', 0.015384615384615385), ('472—\\n\\nknot, 290, 294, 296-299 é, norm, 244 fy norm, 242 ag, 422 Laplace distribution, 251 asso, 11, 24, 244-251, 265-266, 336, 385, 484 eaf, 333, 526 earning rate, 429 east squares, 5, 21, 71-72, 140, 141, 229 line, 73 weighted, 103 evel, 92 everage, 104-106 ikelihood function, 141 inear, 2, 69-115 inear combination, 128, 230, 253, 505 inear discriminant analysis, 5, 11, 135, 138, 147-155, 164— 167, 377, 382 inear kernel, 380 inear model, 20, 69-115 inear regression, 5, 11, 69-115, 172-173 multiple, 80-90 simple, 70-80 ink function, 172, 173\\n\\ninkage, 529-530, 548 average, 529-530 centroid, 529-530 complete, 526, 529-530 single, 529-530 list, 41 list comprehension, 123 local minimum, 427\\n\\nlocal regression, 290 log odds, 145 log-rank test, 474-476, 483 logistic function, 139 logistic regression, 5, 11, 25, 135, 138-144, 164-167, 172— 173, 308-309, 377, 384— 385 multinomial, 145, 163 multiple, 142-144 logit, 140 loss function, 300, 385 low-dimensional, 262 LSTM RNN, 420', 0.015151515151515152), ('Occam’s razor, 426 odds, 140, 145, 195 OJ data set, 12, 365, 398 one-hot encoding, 92, 126, 403 one-standard-error rule, 240 one-versus-all, 384 one-versus-one, 384 one-versus-rest, 384 optimal separating hyperplane, 370 optimism of training error, 30 ordered categorical variable, 315 orthogonal, 257, 506 basis, 125 out-of-bag, 345 outlier, 103-104 output variable, 15 over-parametrized, 465\\n\\noverdispersion, 172\\n\\noverfitting, 21, 23, 25, 30-31, 88, 152, 233, 371\\n\\np-value, 77, 82, 560-562, 578-579 adjusted, 586 package, 42 parameter, 71 parametric, 20-22, 111-115 partial least squares, 254, 260-262, 282 partial likelihood, 480 path algorithm, 249 permutation, 578 permutation approach, 577-582 perpendicular, 257 Poisson distribution, 169, 172 Poisson regression, 135, 167-173 polynomial kernel, 380, 382 regression, 98-99, 289-292, 294— 295 pooling, 410 population regression line, 73 Portfolio data set, 12 positive predictive value, 155, 156 posterior distribution, 251 mode, 251 probability, 147 power, 108, 155, 563 precision, 155 prediction, 17 interval, 90, 110 predictor, 15 principal components, 505 analysis, 11, 254-260, 504-515 loading vector, 505, 506 missing values, 515-520 proportion of variance explained, 510-515, 547 regression, 11, 254-260, 280— 282, 504, 515', 0.014925373134328358), ('deep learning, 399\\n\\nDefault data set, 12, 136-139, 141— 144, 152-156, 160, 161, 225, 226, 466 degrees of freedom, 30, 266, 295, 296, 301 lendrogram, 521, 525-530 ensity function, 146 lependent variable, 15 erivative, 296, 300 tector layer, 410 leviance, 232 ictionary, 66 imension reduction, 230, 253-262 iscriminant function, 149 iscriminant method, 146-161 issimilarity, 530-532 istance correlation-based, 530-532, 554 Euclidean, 509, 522, 523, 529- 532 ouble descent, 431-435 ouble-exponential distribution, 251 ropout, 406, 431 ummy variable, 91-94, 138, 142, 292 aaanaagaaaAgaaaa aaqaaea\\n\\nearly stopping, 430 effective degrees of freedom, 301 eigen decomposition, 506, 516 elbow, 548 embedding, 418 embedding layer, 419 ensemble, 343-354 entropy, 337-339, 363 epochs, 430 error irreducible, 17, 30 rate, 34 reducible, 17 term, 16 Euclidean distance, 509, 522, 523, 529-532, 554 event time, 470 exception, 45 expected value, 18 exploratory data analysis, 504 exponential, 173\\n\\nexponential family, 173\\n\\nF-statistic, 84', 0.014705882352941176), (\"In\\n\\n[27]: hit_trainer = Trainer(deterministic=True, max_epochs=50, log_every_n_steps=5, logger=hit_logger, callbacks=[ErrorTracker ()]) hit_trainer.fit(hit_module, datamodule=hit_dm)\\n\\nAt each step of SGD, the algorithm randomly selects 32 training observa- tions for the computation of the gradient. Recall from Section 10.7 that an epoch amounts to the number of SGD steps required to process n observa- tions. Since the training set has n = 175, and we specified a batch_size of 32 in the construction of hit_dm, an epoch is 175/32 = 5.5 SGD steps.\\n\\nAfter having fit the model, we can evaluate performance on our test data using the test() method of our trainer.\\n\\nIn [28]: hit_trainer.test(hit_module, datamodule=hit_dm)\\n\\nOut [28] : [{'test_loss': 104098.5469, 'test_mae': 229.5012}]\\n\\nThe results of the fit have been logged into a CSV file. We can find the results specific to this run in the experiment .metrics_file_path attribute of our logger. Note that each time the model is fit, the logger will output results into a new subdirectory of our directory logs/hitters.\\n\\nWe now create a plot of the MAE (mean absolute error) as a function of the number of epochs. First we retrieve the logged summaries.\", 0.014285714285714285), ('t\\n\\ndistribution\\n\\nSCENARIO 6 SCENARIO 4 040 035 0.30 018 0.20 022 024 026 028 0,90 0.32 SCENARIO 5 i 025 030 035 «040 045 020 os 6 6 KNN-Cv LDA Logistic NBayes KNN-1 KNN-1 KNN-cv Loa Logistic 0A KNN-1 KNN-cv Loa NBayes\\n\\nFIGURE 4.12. scenarios described in the main text. Bozxplots of the test error rates for each of the non-linear\\n\\nother approaches. The naive Bayes is violated, so naive Bayes performs poorly. sumption of independent predictors\\n\\nScenario 5: The data were generated from a normal distribution with un- correlated predictors. Then the responses were sampled from the logistic function applied to a complicated non-linear function of the predictors. The center panel of Figure 4.12 shows that both QDA and naive Bayes gave slightly better results than the linear methods, while the much more flexi- ble KNN-CV method gave the best results. But KNN with K = 1 gave the worst results out of all methods. This highlights the fact that even when the data exhibits a complex non-linear relationship, a non-parametric method such as KNN can still give poor results if chosen correctly. he level of smoothness is not', 0.013888888888888888), ('In [26]:| fig, ax = subplots(figsize=(8,8)) roc_curve(best_svm, X_train, y_train, name=\\'Training\\', color=\\'r\\', ax=ax);\\n\\nIn this example, the SVM appears to provide accurate predictions. By increasing 7 we can produce a more flexible fit and generate further im- provements in accuracy.\\n\\nIn [27]:| svm_flex = SVC(kernel=\"rbf\", gamma=50,\\n\\n.function_ decision()\\n\\nroc_curve()\\n\\nC=1) svm_flex.fit(X_train, y_train) fig, ax = subplots (figsize=(8,8)) roc_curve(svm_flex, X_train, y_train, name=\\'Training $\\\\gamma=50$\\', color=\\'r\\', ax=ax);\\n\\nHowever, these ROC curves are all on the training data. We are really more interested in the level of prediction accuracy on the test data. When we compute the ROC curves on the test data, the model with y = 0.5 appears to provide the most accurate results.\\n\\nIn [28]: roc_curve(svm_flex, X_test, y_test, name=\\'Test $\\\\gamma=50$\\', color=\\'b\\', ax=ax) fig;\\n\\nLet’s look at our tuned SVM.\\n\\n[29]: fig, ax = subplots (figsize=(8,8)) for (X_, y_, c, name) in zip( (X_train, X_test), (y_train, y_test), Cr\\', \\'b\\'), (\\'CV tuned on training\\', ‘CV tuned on test\\')): roc_curve (best_svm, xX, y-> name=name, ax=ax, color=c)', 0.013888888888888888), ('StandardScaler (), 185, 438, 537, 555 statsmodels, 116, 173 std(), 186 Stepwise(), 269 str.contains(), 59 subplots(), 48 sum(), 43, 268 summarize(), 118, 129, 223 226 summary (), 119, 322, 587 super (), 440 SupportVectorClassifier(), 387, 389-391, 393 SupportVectorRegression(), 394 SVC(), see SupportVector- Classifier () svd(), 539 SVR(), see SupportVector- Regression() TensorDataset(), 441 to_numpy(), 437 torch, 435 torchinfo, 436 torchmetrics, 436 torchvision, 436 ToTensor(), 444 train_test_split(), 186, 216 transform(), 118, 119 ttest_1samp(), 584 ttest_ind(), 590 ttest_rel(), 587 tuple, 43 uniform(), 555 value_counts(), 66 var (), 536 variance_inflation_factor(), 116, 124\\n\\nVIF (), see variance_inflation- _factor() where(), 355 zip, 60, 312\\n\\nq-values, 589 quadratic, 98 quadratic discriminant analysis, 4, 135, 156-157, 164-167', 0.0136986301369863), ('9.6.8 ROC Curves\\n\\nSVMs and support vector classifiers output class labels for each observation. However, it is also possible to obtain fitted values for each observation, which are the numerical scores used to obtain the class labels. For instance, in the case of a support vector classifier, the fitted value for an observation X = (X1,X2,..., Xp)? takes the form Bo + 61X1 + BoX2+...+6)Xp. For an SVM with a non-linear kernel, the equation that yields the fitted value is given in (9.23). The sign of the fitted value determines on which side of the decision boundary the observation lies. Therefore, the relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero then the observation is assigned to one class, and if it is less than zero then it is assigned to the other. By changing this threshold from zero to some positive value, we skew the classifications in favor of one class versus the other. By considering a range of these thresholds, positive and negative, we produce the ingredients for a ROC plot. We can access these values by calling the decision_function() method of a fitted SVM estimator.\\n\\nThe function ROCCurveDisplay.from_estimator() (which we have abbre- viated to roc_curve()) will produce a plot of a ROC curve. It takes a fitted estimator as its first argument, followed by a model matrix X and labels y. The argument name is used in the legend, while color is used for the color of the line. Results are plotted on our axis object ax.', 0.013513513513513514), ('11\">8.3.5 Bayesian Additive Regression Trees. ........ 362</td></tr><tr><td>4 Exercises... 0</td><td></td><td></td><td>ee</td><td></td><td></td><td></td><td></td><td></td><td></td><td>363</td></tr><tr><td>upport Vector Machines</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>367</td></tr><tr><td>1 Maximal Margin Classifier</td><td></td><td></td><td></td><td>...............0.0.</td><td></td><td></td><td></td><td></td><td></td><td>367</td></tr><tr><td>9.1.1 What Isa Hyperplane?............000..</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>368</td></tr><tr><td>9.1.2 Classification</td><td></td><td>Using</td><td>a</td><td>Separating</td><td></td><td></td><td>Hyperplane</td><td>...</td><td></td><td>368</td></tr><tr><td>9.1.3 The Maximal</td><td></td><td></td><td>Margin</td><td>Classifier...</td><td></td><td></td><td></td><td>2.0.0...</td><td></td><td>370</td></tr></tbody></table>', 0.013513513513513514), ('Recall that in order to create the left-hand panel of Figure 5.2, we ran- omly divided the data set into two parts, a training set and a validation set. If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the test MSE. As an lustration, the right-hand panel of Figure 5.2 displays ten different vali- ation set MSE curves from the Auto data set, produced using ten different random splits of the observations into training and validation sets. All ten curves indicate that the model with a quadratic term has a dramatically smaller validation set MSE than the model with only a linear term. Fur- thermore, all ten curves indicate that there is not much benefit in including cubic or higher-order polynomial terms in the model. But it is worth noting that each of the ten curves results in a different test MSE estimate for each of the ten regression models considered. And there is no consensus among the curves as to which model results in the smallest validation set MSE. Based on the variability among these curves, all that we can conclude with any confidence is that the linear fit is not adequate for this data.\\n\\nThe validation set approach is conceptually simple and is easy to imple- ment. But it has two potential drawbacks:', 0.013333333333333334), ('8This is related to the important concept of multiple testing, which is the focus of Chapter 13.\\n\\nhigh- dimensional\\n\\nvariable selection\\n\\nX» only, and (4) a model containing both X; and X2. We can then se- lect the best model out of all of the models that we have considered. How do we determine which model is best? Various statistics can be used to judge he quality of a model. These include Mallow’s C,, Akaike informa- tion criterion (AIC), Bayesian information criterion (BIC), and adjusted R?. These are discussed in more detail in Chapter 6. We can also deter- mine residuals, in order to search for patterns. which model is best by plotting various model outputs, such as the\\n\\nUnfortunately, there are a total of 2? models that contain subsets of p variables. This means that even for moderate p, trying out every possible subset of the predictors is infeasible. For instance, we saw that if p = 2, then there 930 — small and e are 2? = 4 models to consider. But if p = 30, then we must consider 073,741,824 models! This is not practical. Therefore, unless p is very ; we cannot consider all 2? models, and instead we need an automated fficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:', 0.013157894736842105), ('The right-hand panel of Figure 5.8 displays the same three curves us- ing the KNN approach for classification, as a function of the value of Kk (which in this context indicates the number of neighbors used in the KNN classifier, rather than the number of CV folds used). Again the training error rate declines as the method becomes more flexible, and so we see that the training error rate cannot be used to select the optimal value for K. Though the cross-validation error curve slightly underestimates the test error rate, it takes on a minimum very close to the best value for K.', 0.013157894736842105), ('Scenario 6: The observations were generated from a normal distribution with a different diagonal covariance matrix for each class. However, the sample size was very small: just n = 6 in each class. Naive Bayes performed very well, because its assumptions are met. LDA and logistic regression performed poorly because the true decision boundary is non-linear, due to the unequal covariance matrices. QDA performed a bit worse than naive Bayes, because given the very small sample size, the former incurred too much variance in estimating he correlation between the predictors within each class. KNN’s performance also suffered due to the very small sample size.\\n\\nThese six examples illustrate that no one method will dominate the oth- ers in every situation. When the true decision boundaries are the LDA and logistic regression approaches will inear, then end to perform well. When the boundaries are moderately non-linear, QDA or naive Bayes may give better results. Finally, for much more complicated decision boundaries, non-parametric approach such as KNN can be superior. But smoothness for a non-parametric approach mus he level of be chosen carefully. In the next chapter we examine a number of approaches for choosing the correct level of smoothness and, in general, for selecting the best overall method.', 0.012987012987012988), ('Years < 4.5 t RBI 460.5 Hits <|117.5 Putouts < 82 Years|< 3.5 Years|< 3.5 5.487 5.394 6.189 4.622 6.183 Walks |< 43.5 Walks c 2.5 Runs 5 47.5 | RBI 4 80.5 eis ser1 8407 odes Years[=65 | : 7.289\\n\\n6.459\\n\\n7.007\\n\\nFIGURE 8.4. Regression tree analysis for the Hitters data. The unpruned tree that results from top-down greedy splitting on the training data is shown.\\n\\nbinary splitting to grow a classification tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits. A natural alternative to RSS is the classification error rate. Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is simply the fraction of the training observations in that region that do not belong to the most common class:\\n\\nHere fmx represents the proportion of training observations in the mth region that are from the kth class. However, it turns out that classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable.\\n\\nThe Gini index is defined by\\n\\nK G =O Pk = Pm), (8.6) k=1\\n\\na measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the pyn,’s are close to zero or one. For this reason the Gini index is referred to as a measure of', 0.01282051282051282), ('There are many possible classification techniques, or classifiers, that one might use to predict a qualitative response. We touched on some of these in Sections 2.1.5 and 2.2.3. In this chapter we discuss some widely-used classifiers: logistic regression, linear discriminant analysis, quadratic dis- criminant analysis, naive Bayes, and K-nearest neighbors. The discussion of logistic regression is used as a jumping-off point for a discussion of gen- eralized linear models, and in particular, Poisson regression. We discuss more computer-intensive classification methods in later chapters: these in- clude generalized additive models (Chapter 7); trees, random forests, and boosting (Chapter 8); and support vector machines (Chapter 9).\\n\\n4.1 An Overview of Classification\\n\\nClassification problems occur often, perhaps even more so than regression problems. Some examples include:\\n\\n© Springer Nature Switzerland AG 2023\\n\\n135\\n\\nG. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics, https://doi.org/10.1007/978-3-03 1-38747-0_4\\n\\nqualitative\\n\\nclassification\\n\\nclassifier', 0.01282051282051282), ('Ss\\n\\nSs\\n\\nregression, 386 survival analysis, 469-502 curve, 472, 483 ‘unction, 472 ime, 470 synergy, 70, 89, 95-98, 110-111 systematic, 16 t-distribution, 77, 165 t-statistic, 76 t-test one-sample, 583, 584, 588 aired, 587 wo-sample, 559, 570, 571, 577— 581, 584, 590 test error, 35, 37, 176 MSE, 28-32 observations, 28 set, 30 statistic, 559 theoretical distribution, 577 time series, 101 total sum of squares, 79 tracking, 102 train, 21 training data, 20 error, 35, 37, 176 MSE, 28-31 transformer, 311 tree, 331-342 tree-based method, 331 true negative, 155 true positive, 155 true positive rate, 155, 156, 382 truncated power basis, 296 Tukey’s method, 571, 585, 587 tuning parameter, 187, 240, 484\\n\\ntwo-sample t-test, 474 Type Terror, 155, 562-565 Type I error rate, 563 Type IJ error, 155, 563, 568, 584\\n\\nunsupervised learning, 25-27, 255,\\n\\n260, 503-552 USArrests data set, 12, 507, 508, 510, 512, 513, 515, 516, 518, 519 validation set, 202 approach, 202-204 variable, 15 dependent, 15 dummy, 91-94, 97-98 importance, 346, 360 independent, 15 indicator, 35 input, 15 output, 15 qualitative, 91-94, 97-98 selection, 86, 230, 244 variance, 18, 31-34, 159 inflation factor, 108-110, 123 varying coefficient model, 305 Wage data set, 1, 2, 8, 9, 12, 290, 291, 293, 295, 297-300, 302-306, 309, 315, 327 weak learner, 343 weakest link pruning, 336 Weekly data set, 12, 196, 226 weight freezing, 412, 419 weight sharing, 418 weighted least squares, 103, 304 weights, 404 with replacement, 214 within class covariance, 150 wrapper, 217', 0.01282051282051282), ('2. We can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in Chapter 5.\\n\\nWe consider both of these approaches below.\\n\\nC,, AIC, BIC, and Adjusted R?\\n\\nWe show in Chapter 2 that the training set MSE is generally an under- estimate of the test MSE. (Recall that MSE = RSS/n.) This is because when we fit a model to the training data using least squares, we specifi- cally estimate the regression coefficients such that the training RSS (but not the test RSS) is as small as possible. In particular, the training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set R? cannot be used to select from among a set of models with different numbers of variables.\\n\\nHowever, a number of techniques for adjusting the training error for the model size are available. These approaches can be used to select among a set of models with different numbers of variables. We now consider four such approaches: C,, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted R?. Figure 6.2 displays C,, BIC, and adjusted R? for the best model of each size produced by best subset selection on the Credit data set.', 0.012658227848101266), ('Therefore, to assess the model fit, we must take a different approach, which involves stratifying the observations using the coefficient estimates. In particular, for each test observation, we compute the “risk” score\\n\\nbudget; * Pouaget + impact; - Bimpact;\\n\\nwhere Bouaget and Bimpact are the coefficient estimates for these two features from the training set. We then use these risk scores to categorize the obser- vations based on their “risk”. For instance, the high risk group consists of the observations for which budget; - Bouaget + impact; ‘Bimpact i is largest; by\\n\\n15Cyoss-validation for the Cox model is more involved than for linear or logistic re- gression, because the objective function is not a sum over the observations.', 0.012658227848101266), ('TABLE 4.6. Possible results when applying a classifier or diagnostic test to a population.', 0.012658227848101266), ('n=\"11\">4.4 4.5</td><td>.3.2</td><td>Estimating the Regression</td><td></td><td></td><td></td><td></td><td>Coefficients</td><td></td><td></td><td></td><td></td><td>......</td><td></td><td>.</td><td>40</td></tr><tr><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>.3.3</td><td>Making Predictions... ..........-..4.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>41</td></tr><tr><td>.3.4 4</td><td>Multiple Logistic Regression...</td><td></td><td></td><td></td><td></td><td>.....-.....</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>42</td></tr><tr><td>3.5 4</td><td>Multinomial Logistic</td><td></td><td>Regression.</td><td></td><td></td><td>..........</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>44</td></tr><tr><td>Generative</td><td>Models for Classification...</td><td></td><td></td><td></td><td></td><td>.</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>46</td></tr><tr><td>A, 4</td><td>Linear Discriminant Analysis</td><td></td><td></td><td></td><td></td><td>forp=1.......</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>AT</td></tr><tr><td>4.2 4</td><td>Linear Discriminant Analysis</td><td></td><td></td><td></td><td></td><td>forp&gt;1</td><td></td><td></td><td></td><td></td><td>.......</td><td></td><td></td><td>50</td></tr><tr><td>4.3 4</td><td>Quadratic Discriminant</td><td></td><td></td><td>Analysis</td><td></td><td></td>', 0.012658227848101266), ('12.1 The Challenge of Unsupervised Learning\\n\\nSupervised learning is a well-understood area. In fact, if you have read the preceding chapters in this book, then you should by now have a good grasp of supervised learning. For instance, if you are asked to predict a binary outcome from a data set, you have a very well developed set of tools at your disposal (such as logistic regression, linear discriminant analysis, classification trees, support vector machines, and more) as well as a clear\\n\\n© Springer Nature Switzerland AG 2023\\n\\nG. James et al., An Introduction to Statistical Learning, Springer Texts in Statistics, https://doi.org/10.1007/978-3-03 1-38747-0_12\\n\\nCheck for updates\\n\\n503\\n\\nunderstanding of how to assess the quality of the results obtained (using cross-validation, validation on an independent test set, and so forth).', 0.012658227848101266)], 'question': 'Tell me all classification metrics.'}\n","U funkciju za ocenjivanje dokumenata uslo je:46\n","Broj dokumenata koji pripadaju datom kontekstu:7\n","[('TABLE 4.7. Important measures for classification and diagnostic testing, derived from quantities in Table 4.6.\\n\\nalternative (non-null) hypothesis. In the context of the Default data, “+” indicates an individual who defaults, and “—” indicates one who does not.\\n\\nTable 4.7 lists many of the popular performance measures that are used in this context. The denominators for the false positive and true positive rates are the actual population counts in each class. In contrast, the denominators for the positive predictive value and the negative predictive value are the total predicted counts for each class.', 0.08075130698081517), ('2.2.3 The Classification Setting\\n\\nThus far, our discussion of model accuracy has been focused on the regres- sion setting. But many of the concepts that we have encountered, such as the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that y; is no longer quan- titative. Suppose that we seek to estimate f on the basis of training obser- vations {(71,y1),.. 1>Yn)}, Where now y1,.--,Yn are qualitative. The most common approach for quantifying the accuracy of our estimate f is the training error rate, the proportion of mistakes that are made if we apply\\n\\nbias-variance trade-off\\n\\nerror rate\\n\\nour estimate f to the training observations:\\n\\n1 = lui 4 Gi). (2.8) i=1\\n\\nHere %; is the predicted class label for the ith observation using f. And I(y; # GH) is an indicator variable that equals 1 if y; A Gj and zero if y; = G. If I(y; A 9) = 0 then the ith observation was classified correctly by our classification method; otherwise it was misclassified. Hence Equation 2.8 computes the fraction of incorrect classifications.\\n\\nEquation 2.8 is referred to as the training error rate because it is com- puted based on the data that was used to train our classifier. As in the regression setting, we are most interested in the error rates that result from applying our classifier to test observations that were not used in training. The test error rate associated with a set of test observations of the form (xo, yo) is given by', 0.062399843888819126), ('classification error rate\\n\\nGini index\\n\\n= — Training — Cross-Validation — Test 08 i 0.2 0.0 L Tree Size\\n\\nMean Squared Error\\n\\nFIGURE 8.5. Regression tree analysis for the Hitters data. The training, cross-validation, and test MSE are shown as a function of the number of terminal nodes in the pruned tree. Standard error bands are displayed. The minimum cross-validation error occurs at a tree size of three.\\n\\nnode purity—a small value indicates that a node contains predominantly observations from a single class.\\n\\nAn alternative to the Gini index is entropy, given by\\n\\nK D=- Ss Pmk log Pmk- (8.7) k=l\\n\\nSince 0 < Pmxz < 1, it follows that 0 < —pms log Pm. One can show that the entropy will take on a value near zero if the pm,’s are all near zero or near one. Therefore, like the Gini index, the entropy will take on a small value if the mth node is pure. In fact, it turns out that the Gini index and the entropy are quite similar numerically.\\n\\nWhen building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.', 0.04387881031716648), ('<table><tbody><tr><td>Predicted</td><td>— or Null</td><td>True Neg. (TN)</td><td>False Neg. (FN)</td><td>N*</td></tr><tr><td>class</td><td>+ or Non-null</td><td>False Pos. (FP)</td><td>True Pos. (TP)</td><td>p*</td></tr></tbody></table>', 0.043269344498852697), ('marized over all possible thresholds, is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. For this data the AUC is 0.95, which is close to the maximum of 1.0, so would be considered very good. We expect a classifier that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training). ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds. It turns out that the ROC curve for the logistic regression model of Section 4.3.4 fit to these data is virtually indistinguishable from this one for the LDA model, so we do not display it here.\\n\\nAs we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the sensitivity and one minus the specificity of our classifier. Since there is an almost bewildering array of terms used in this context, we now give a summary. Table 4.6 shows the possible results when applying a classifier (or diagnostic test) to a population. To make the connection with the epidemiology literature, we think of “+” as the “disease” that we are trying to detect, and “—” as the “non-disease” state. To make the connection to the classical hypothesis testing literature, we think of “—” as the hypothesis and “+” as the', 0.029709507042253523), ('bottom-up, 525 hierarchical, 521, 525-535 k-means, 11, 521-524 Cochran—Mantel—Haenszel test, 475 coefficient, 71 College data set, 12, 65, 286, 328 collinearity, 106-110 concatenation, 41 conditional probability, 35 confidence interval, 75-76, 90, 110, 292 confounding, 144 confusion matrix, 153, 176 continuous, 2 contour, 246 contour plot, 50 contrast, 94 convenience function, 53 convolution filter, 407 convolution layer, 407 convolutional neural network, 406— 413 correlation, 79, 82-83, 530 count data, 167, 170 Cox’s proportional hazards model, 480, 483-486 Cp, 87, 231, 232, 236-238 Credit data set, 12, 91, 92, 94, 97, 98, 106-109 cross-entropy, 405 cross-validation, 11, 31, 34, 201— 211, 231, 252, 270 k-fold, 206-209 leave-one-out, 204-206 curse of dimensionality, 115, 193, 266\\n\\ndata augmentation, 411 data frame, 55 Data sets Advertising, 15, 16, 19, 69, 71-73, 77, 78, 80, 82, 83, 85, 87-90, 95, 96, 109- 111 Auto, 12, 66, 98-101, 129, 197, 202-207, 327, 398 Bikeshare, 12, 167-172\\n\\nBoston, 12, 67, 117, 122, 133, 199, 227, 287, 327, 364, 556\\n\\nBrainCancer, 12, 472-474, 476, 482\\n\\nCaravan, 12, 184, 366 Carseats, 12, 126, 130, 364 CIFAR100, 406, 409-411, 448, 449', 0.02684684684684685), ('Years < 4.5 t RBI 460.5 Hits <|117.5 Putouts < 82 Years|< 3.5 Years|< 3.5 5.487 5.394 6.189 4.622 6.183 Walks |< 43.5 Walks c 2.5 Runs 5 47.5 | RBI 4 80.5 eis ser1 8407 odes Years[=65 | : 7.289\\n\\n6.459\\n\\n7.007\\n\\nFIGURE 8.4. Regression tree analysis for the Hitters data. The unpruned tree that results from top-down greedy splitting on the training data is shown.\\n\\nbinary splitting to grow a classification tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits. A natural alternative to RSS is the classification error rate. Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is simply the fraction of the training observations in that region that do not belong to the most common class:\\n\\nHere fmx represents the proportion of training observations in the mth region that are from the kth class. However, it turns out that classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable.\\n\\nThe Gini index is defined by\\n\\nK G =O Pk = Pm), (8.6) k=1\\n\\na measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the pyn,’s are close to zero or one. For this reason the Gini index is referred to as a measure of', 0.01282051282051282)]\n"]}],"source":["from query_translation import load_generalization_query_chain\n","result = final_rag_chain.invoke({\"question\":\"Tell me all classification metrics.\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYjo4PNcdENy","outputId":"3b46c8e7-a148-437a-8042-bf4fc12ea886"},"outputs":[{"name":"stdout","output_type":"stream","text":["The classification metrics mentioned in the provided context include:\n","\n","1. **True Positive (TP)** - The count of correctly predicted positive instances.\n","2. **False Positive (FP)** - The count of incorrectly predicted positive instances.\n","3. **True Negative (TN)** - The count of correctly predicted negative instances.\n","4. **False Negative (FN)** - The count of incorrectly predicted negative instances.\n","5. **Training Error Rate** - The proportion of mistakes made when applying the classifier to the training observations, calculated using the formula:\n","   \\[\n","   \\text{Training Error Rate} = \\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)\n","   \\]\n","   where \\( I(y_i \\neq \\hat{y}_i) \\) is an indicator variable that equals 1 if the prediction is incorrect and 0 if it is correct.\n","6. **Positive Predictive Value (PPV)** - The proportion of true positive predictions among all positive predictions.\n","7. **Negative Predictive Value (NPV)** - The proportion of true negative predictions among all negative predictions.\n","8. **Sensitivity (True Positive Rate)** - The proportion of actual positives that are correctly identified.\n","9. **Specificity (True Negative Rate)** - The proportion of actual negatives that are correctly identified.\n","10. **Gini Index** - A measure of total variance across classes, defined as:\n","    \\[\n","    G = 1 - \\sum_{k=1}^{K} P_k^2\n","    \\]\n","11. **Entropy** - Another measure of impurity, defined as:\n","    \\[\n","    D = -\\sum_{k=1}^{K} P_{mk} \\log P_{mk}\n","    \\]\n","\n","These metrics are used to evaluate the performance of classification models. The information is derived from the context provided, specifically from the sections discussing classification metrics and error rates.\n"]}],"source":["print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-LWqhjGdENy","outputId":"1f795ace-b0e1-4e67-eaa6-46c539559647"},"outputs":[{"data":{"text/plain":["['8.6.2 Simple Linear Regression\\n\\nIn this section we will construct model matrices (also called design matri- ces) using the ModelSpec() transform from ISLP.models.\\n\\nWe will use the Boston housing data set, which is contained in the ISLP package. The Boston dataset records medv (median house value) for 506 neighborhoods around Boston. We will build a regression model to pre- dict medv using 13 predictors such as rmvar (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940), and lstat (percent of households with low socioeconomic status). We will use statsmodels for this task, a Python package that implements several com- monly used regression methods.\\n\\nWe have included a simple loading function load_data() in the ISLP pack- age: load_data()\\n\\nIn [8]:| Boston = load_data(\"Boston\") Boston.columns\\n\\nOut[8]: Index([\\'crim\\', \\'zn\\', \\'indus\\', \\'chas\\', \\'nox\\', \\'rm\\', \\'age\\', \\'dis\\', ‘\\'rad\\', \\'tax\\', \\'ptratio\\', \\'black\\', \\'lstat\\', \\'medv\\'], dtype=\\'object\\')\\n\\nType Boston? to find out more about these data.\\n\\nWe start by using the sm.0LS() function to fit a simple linear regression model. Our response will be medv and 1stat will be the single predictor. For this model, we can create the model matrix by hand. sm.OLS()',\n"," \"In [9]: Out [9] : X = pd.DataFrame({'intercept': np.ones(Boston.shape[0]), 'lstat': Boston['lstat']}) X[:4] intercept Ilstat (0) 1.0 4.98 1 1.0 9.14 2 1.0 4.03 3 1.0 2.94\\n\\nWe extract the response, and fit the model.\\n\\nIn [10]: y = Boston['medv'] model = sm.OLS(y, X) results = model.fit()\\n\\nNote that sm.0LS() does not fit the model; it specifies the model, and then model.fit() does the actual fitting.\\n\\nOur ISLP function summarize() produces a simple table of the parame- ter estimates, their standard errors, t-statistics and p-values. The function takes a single argument, such as the object results returned here by the fit method, and returns such a summary.\\n\\n[11]: [11]: summarize (results) coef std err t Plt intercept 34.5538 0.563 61.415 0.0 lstat -0.9500 0.039 -24.528 0.0\\n\\nIn\\n\\nOut\\n\\nBefore we describe other methods for working with fitted models, we outline a more useful and general framework for constructing a model ma- trix X.\",\n"," 'return\\n\\n.set_xscale()\\n\\n.set_yscale()\\n\\n(f) Create a function, PlotPower(), that allows you to create a plot of x against x**a for a fixed a and a sequence of values of x. For instance, if you call\\n\\nPlotPower(np.arange(1, 11), 3)\\n\\nthen a plot should be created with an x-axis taking on values 1,2,...,10, and a y-axis taking on values 13, 2°,..., 10°.\\n\\n16. Using the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, naive Bayes, and KNN models using various subsets of the predictors. Describe your findings.\\n\\nHint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set.\\n\\n5)\\n\\nResampling Methods\\n\\nResampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain addi the fitted model. For example, in order to estimate ional information about the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the rest obtain information that once using the original ulting fits differ. Such an approach may allow us to would not be available from fitting the model only training sample.',\n"," '15. This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\\n\\n(a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.\\n\\n(b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the hypothesis Ho : 6; = 0?\\n\\nHow do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the a-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regres- sion model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.',\n"," \"In\\n\\n[22]:| ax = Boston.plot.scatter('lstat', 'medv') abline(ax, results.params [0], results.params[1], Igest linewidth=3)\\n\\nThus, the final call to ax.plot() isax.plot(xlim, ylim, 'r--', linewidth=3). We have used the argument 'r--' to produce a red dashed line, and added an argument to make it of width 3. There is some evidence for non-linearity in the relationship between 1stat and medv. We will explore this issue later in this lab.\\n\\nAs mentioned above, there is an existing function to add a line to a plot — ax.axline() — but knowing how to write such functions empowers us to create more expressive displays.\\n\\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. We can find the fitted values and residuals of the fit as attributes of the results object. Various influence measures describing the regression model are computed with the get_influence() method. As we will not use the fig component returned as the first value from subplots(), we simply capture the second returned value in ax below.\\n\\nIn [23]:| ax = subplots (figsize=(8,8)) [1]\",\n"," \"to_numpy ()\\n\\n[12]: (X_train, X_test, Y_train, Y_test) = train_test_split(X, Y,\\n\\nIn\\n\\ntest_size=1/3, random_state=1)\\n\\nLinear Models\\n\\nWe fit the linear model and evaluate the test error directly.\\n\\n[13]: hit_lm = LinearRegression().fit(X_train, Y_train) Yhat_test = hit_lm.predict (X_test) np.abs(Yhat_test - Y_test).mean()\\n\\nIn\\n\\nOut [13] : 259.7153\\n\\nNext we fit the lasso using sklearn. We are using mean absolute error to select and evaluate a model, rather than mean squared error. The spe- cialized solver we used in Section 6.5.2 uses only mean squared error. So here, with a bit more work, we create a cross-validation grid and perform the cross-validation directly.\\n\\nWe encode a pipeline with two steps: we first normalize the features using a StandardScaler() transform, and then fit the lasso without further normalization.\\n\\n[14]: scaler = StandardScaler(with_mean=True, with_std=True) lasso = Lasso(warm_start=True, max_iter=30000) standard_lasso = Pipeline(steps=[('scaler', scaler), ('lasso', lasso)])\",\n"," \"In\\n\\n[105] : for idx, (x, y) in enumerate (nyse_dm.train_dataloader()): out = nyse_model (x) print(y.size(), out.size()) if idx >= 2: break\\n\\ntorch.Size([64]) torch.Size([64]) torch.Size([64]) torch.Size([64]) torch.Size([64]) torch.Size([64])\\n\\nWe follow our previous example for setting up a trainer for a regression problem, requesting the R? metric to be be computed at each epoch.\\n\\nIn\\n\\n[106] : nyse_optimizer = RMSprop(nyse_model.parameters(), 1r=0.001) nyse_module = SimpleModule.regression(nyse_model , optimizer=nyse_optimizer, metrics={'r2':R2Score()})\\n\\nFitting the model should by now be familiar. The results on the test data are very similar to the linear AR model.\",\n"," \"mp-argmax()\\n\\n.columns.\\n\\ndrop()\\n\\nOut [26] : Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', ‘age', ‘dis', 'rad', 'tax', 'ptratio', 'lstat'], dtype='object')\\n\\nWe can now fit the model with all the variables in terms using the same model matrix builder.\\n\\nIn [27]: X = MS(terms).fit_transform(Boston) model = sm.OLS(y, X) results = model.fit() summarize (results)\",\n"," \"3.6.3 Multiple Linear Regression\\n\\nIn order to fit a multiple linear regression model using least squares, we again use the ModelSpec() transform to construct the required model matrix and response. The arguments to ModelSpec() can be quite general, but in this case a list of column names suffice. We consider a fit here with the two variables 1stat and age.\\n\\nIn [25]:|X = MS(['1lstat', 'age']).fit_transform(Boston) modeli = sm.OLS(y, X) resultsi = modeli.fit() summarize(results1)\\n\\nOut [25]\\n\\n: coef std err t Polit intercept 33.2228 0.731 45.458 0.000 lstat -1.0321 0.048 -21.416 0.000 age 0.0345 0.012 2.826 0.005\\n\\nNotice how we have compacted the first line into a succinct expression describing the construction of xX.\\n\\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\\n\\nIn [26]:|\\n\\nterms\\n\\n=\\n\\nBoston.columns.drop('medv')\\n\\nterms\\n\\nsanding\",\n"," 'Even when the dimension is small, we might prefer linear regression to KNN from an interpretability standpoint. If the test MSE of KNN is only slightly lower than that of linear regression, we might be willing to forego a little bit of prediction accuracy for the sake of a simple model that can be described in terms of just a few coefficients, and for which p-values are available.\\n\\ncurse of di- mensionality\\n\\n3.6 Lab: Linear Regression\\n\\n3.6.1 Importing packages\\n\\nWe import our standard libraries at this top level.\\n\\n[1]: import numpy as np import pandas as pd from matplotlib.pyplot import subplots\\n\\nIn\\n\\nNew imports\\n\\nThroughout this lab we will introduce new functions and libraries. However, we will import them here to emphasize these are the new code objects in this lab. Keeping imports near the top of a notebook makes the code more readable, since scanning the first few lines tells us what libraries are used.\\n\\nIn [2]: import statsmodels.api as sm\\n\\nWe will provide relevant details about the functions below as they are needed.\\n\\nBesides importing whole modules, it is also possible to import only a few items from a given module. This will help keep the namespace clean. We will use a few specific objects from the statsmodels package which we import here.',\n"," \"4.7.7 Linear and Poisson Regression on the Bikeshare Data\\n\\nHere we fit linear and Poisson regression models to the Bikeshare data, as described in Section 4.6. The response bikers measures the number of bike rentals per hour in Washington, DC in the period 2010-2012.\\n\\nIn [64]: Bike = load_data('Bikeshare')\\n\\nLet’s have a peek at the dimensions and names of the variables in this dataframe.\\n\\nIn [65]: Bike.shape, Bike.columns\\n\\nOut [65]\\n\\n: ((8645, 15), Index(['season', 'mnth', 'day', 'hr', 'holiday', 'weekday', '‘workingday', 'weathersit', 'temp', ‘atemp', 'hum', 'windspeed', 'casual', 'registered', 'bikers'], dtype='object'))\\n\\nLinear Regression\\n\\nWe begin by fitting a linear regression model to the data.\\n\\nIn [66]: X = MS(['mnth', ‘hr', ‘workingday', 'temp', '‘weathersit']).fit_transform(Bike) Y = Bike['bikers'] M_lm = sm.OLS(Y, X).fitQ summarize (M_1m)\\n\\nOut [66] :\",\n"," \"ModelSpec()\\n\\nWe first describe this process for our simple regression model using a single predictor 1stat in the Boston data frame, but will use it repeatedly in more complex tasks in this and other labs in this book. In our case the transform is created by the expression design = MS(['lstat']).\\n\\nThe fit() method takes the original array and may do some initial com- putations on it, as specified in the transform object. For example, it may compute means and standard deviations for centering and scaling. The transform() method applies the fitted transformation to the array of data, and produces the model matrix.\\n\\nIn [12]:| design = MS(['lstat']) design = design. fit (Boston) X = design.transform(Boston) X(:4]\\n\\n[12]: intercept Ilstat le) 1.0 4.98 4 1.0 9.14 2 1.0 4.03 3 1.0 2.94\\n\\nOut\\n\\nIn this simple case, the fit () method does very little; it simply checks that the variable '1stat' specified in design exists in Boston. Then transform() constructs the model matrix with two columns: an intercept and the vari- able lstat.\\n\\nThese two operations can be combined with the fit_transform() method.\\n\\nIn [13]:| design = MS(['lstat']) X = design. fit_transform(Boston) X(:4]\\n\\nOut [13] : le) 4\",\n"," \"Out\\n\\n[95] : Index(['DJ_return_1', 'log_volume_1', 'log_volatility_1', 'DJ_return_2', 'log_volume_2', 'log_volatility_2', 'DJ_return_3', 'log_volume_3', 'log_volatility_3', 'DJ_return_4', 'log_volume_4', 'log_volatility_4', 'DJ_return_5', '‘log_volume_5', 'log_volatility_5'], dtype='object')\\n\\nWe first fit a simple linear model and compute the R? on the test data using the score() method.\\n\\nIn [96]: M = LinearRegression() M.fit(X[train], Y[train]) M.score(X[~train], Y[.train])\\n\\nOut [96] : 0.4129\\n\\nWe refit this model, including the factor variable day_of_week. For a cate- gorical series in pandas, we can form the indicators using the get _dummies() method.\",\n"," 'The image displays the words \"Linear Regression\" written in a serif font with a blue color. The text appears to be clean and prominently centered in the image, suggesting a focus on the topic of linear regression, which is a fundamental concept in statistics and machine learning involving the modeling of relationships between variables. The simplicity of the design directs attention solely to the words \"Linear Regression.\"',\n"," \"intercept 1.0\\n\\n1.0\\n\\nIlstat\\n\\n4.98\\n\\n9.14\\n\\n2\\n\\n1.0\\n\\n4.03\\n\\n3\\n\\n1.0\\n\\n2.94\\n\\nNote that, as in the previous code chunk when the two steps were done separately, the design object is changed as a result of the fit () operation. The power of this pipeline will become clearer when we fit more complex models that involve interactions and transformations.\\n\\nLet’s return to our fitted regression model. The object results has several methods that can be used for inference. We already presented a function summarize() for showing the essentials of the fit. For a full and somewhat exhaustive summary of the fit, we can use the summary() method (output not shown).\\n\\nIn [14]: | results. summary ()\\n\\nThe fitted coefficients can also be retrieved as the params attribute of results.\\n\\nIn [15]: | results.params\\n\\n.fit_ transform()\\n\\n120 3. Linear Regression\\n\\nOut [15]: intercept 34.553841 lstat -0.950049 dtype: float64\\n\\nThe get_prediction() method can be used to obtain predictions, and . was . wae get, produce confidence intervals and prediction intervals for the prediction of preg iction() medv for given values of 1stat.\\n\\nWe first create a new data frame, in this case containing only the vari- able 1stat, with the values for this variable at which we wish to make predictions. We then use the transform() method of design to create the corresponding model matrix.\\n\\nIn [16]:|new_df = pd.DataFrame({'lstat':[5, 10, 15]}) newX = design.transform(new_df) newX\\n\\nOut[16]:\\n\\n0\",\n"," 'in -min\\n\\n(@d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\\n\\n(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\\n\\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\\n\\n10. This exercise involves the Boston housing data set.\\n\\nTo begin, load in the Boston data set, which is part of the ISLP library.\\n\\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\\n\\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\\n\\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\\n\\nDo any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\\n\\nHow many of the suburbs in this data set bound the Charles river?',\n"," 'It turns out that linear regression can be used to answer each of these questions. We will first discuss all of these questions in a general context, and then return to them in this specific context in Section 3.4.\\n\\n3.1 Simple Linear Regression\\n\\nSimple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a sin- gle predictor variable X. It assumes that there is approximately a linear relationship between X and Y. Mathematically, we can write this linear relationship as\\n\\nY = Bo + BX. (3.1)\\n\\nYou might read “=” as “is approximately modeled as”. We will sometimes describe (3.1) by saying that we are regressing Y on X (or Y onto X).\\n\\nsynergy\\n\\ninteraction\\n\\nsimple linear regression\\n\\nFor example, X may represent TV advertising and Y may represent sales. Then we can regress sales onto TV by fitting the model\\n\\nsales © 69 + 81 x TV.\\n\\nIn Equation 3.1, 89 and 8; are two unknown constants that represent the intercept and slope terms in the linear model. Together, 39 and 3, are known as the model coefficients or parameters. Once we have used our training data to produce estimates Bo and By for the model coefficients, we can predict future sales on the basis of a particular value of TV advertising by computing\\n\\n9 = Bo + fie, (3.2)\\n\\nwhere ¥ indicates a prediction of Y on the basis of X = x. Here we use a hat symbol, * , to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response.',\n"," \"In\\n\\n[3]:| Auto = load_data('Auto') Auto_train, Auto_valid = train_test_split (Auto, test_size=196, random_state=0)\\n\\nNow we can fit a linear regression using only the observations corre- sponding to the training set Auto_train.\\n\\nIn [4]:| hp_mm = MS(['horsepower']) X_train = hp_mm.fit_transform(Auto_train) y_train = Auto_train['mpg'] model = sm.OLS(y_train, X_train) results = model.fit()\\n\\nWe now use the predict () method of results evaluated on the model ma- trix for this model created using the validation data set. We also calculate the validation MSE of our model.\\n\\nIn (5]:| X_valid = hp_mm.transform(Auto_valid) y_valid = Auto_valid['mpg'] valid_pred = results.predict(X_valid) np.mean((y_valid - valid_pred) **2)\\n\\nOut [5]: 23.6166\\n\\nHence our estimate for the validation MSE of the linear regression fit is 23.62.\\n\\nWe can also estimate the validation error for higher-degree polynomial regressions. We first provide a function evalMSE() that takes a model string as well as a training and test set and returns the MSE on the test set.\",\n"," '5. We have seen that we can fit an SVM with a non-linear kernel in order o perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.\\n\\n(a) Generate a data set with n = 500 and p = 2, such that the obser- vations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows:\\n\\nrng = np.random.default_rng(5) xt rng.uniform(size=500) - 0.5 x2 rng.uniform(size=500) - 0.5 y = x1**2 - x2**2 > 0\\n\\n(b) Plot the observations, colored according to their class labels. Your plot should display X, on the x-axis, and X2 on the y- axis.\\n\\n(c) Fit a logistic regression model to the data, using X; and X» as predictors.\\n\\n(d) Apply this model to the training data in order to obtain a pre- dicted class label for each training observation. Plot the ob- servations, colored according to the predicted class labels. The decision boundary should be linear.\\n\\n(e) Now fit a logistic regression model to the data using non-linear functions of X; and X2 as predictors (e.g. X?7, X1 x Xo, log(X2), and so forth).\\n\\n(f) Apply this model to the training data in order to obtain a pre- dicted class label for each training observation. Plot the ob- servations, colored according to the predicted class labels. The decision boundary should be obviously non-linear. If it is not, then repeat (a)—(e) until you come up with an example in which the predicted class labels are obviously non-linear.',\n"," '1The assumption of linearity is often a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true relationship is linear.\\n\\npopulation\\n\\n)oovoccion line\\n\\nleast squares line\\n\\nFIGURE 3.3. A simulated data set. Left: The red line represents the true relationship, f(X) = 2+ 3X, which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate for f(X) based on the observed data, shown in black. Right: The population regression line is again shown in red, and the least squares line in dark blue. In light blue, ten least squares lines are shown, each computed on the basis of a separate random set of observations. Each least squares line is different, but on average, the least squares lines are quite close to the population regression line.\\n\\nthe population mean js of some random variable Y. Unfortunately, 1 is unknown, but we do have access to n observations from Y, yi,..-,Yn; which we can use to estimate jz. A reasonable estimate is fi = y, where y= i ean y; is the sample mean. The sample mean and the population mean are different, but in general the sample mean will provide a good estimate of the population mean. In the same way, the unknown coefficients 39 and in linear regression define the population regression line. We seek to estimate these unknown coefficients using 3p and (; given in (3.4). These coefficient estimates define the least squares line.']"]},"execution_count":320,"metadata":{},"output_type":"execute_result"}],"source":["retriever.invoke(\"Show me from scratch how to perform linear regression in Python on Boston dataset.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDRyskTRdENy","outputId":"e1d28b81-917f-4a92-ea5a-3ec22ff45efe"},"outputs":[{"name":"stdout","output_type":"stream","text":["INPUT {'context_documents': [('An Application to the Credit Data\\n\\nIn Figure 6.4, the ridge regression coefficient estimates for the Credit data set are displayed. In the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\\\. For example, the black solid line represents the ridge regression estimate for the income coefficient, as \\\\ is varied. At the extreme left-hand side of the plot, \\\\ is essentially zero, and so the corresponding ridge coefficient estimates are the same as the usual least squares esti- mates. But as A increases, the ridge coefficient estimates shrink towards zero. When X is extremely large, then all of the ridge coefficient estimates are basically zero; this corresponds to the model that contains no pre- dictors. In this plot, the income, limit, rating, and student variables are displayed in distinct colors, since these variables tend to have by far the largest coefficient estimates. While the ridge coefficient estimates tend to decrease in aggregate as \\\\ increases, individual coefficients, such as rating and income, may occasionally increase as X increases.', 0.08333333333333333), ('parameter\\n\\nshrinkage penalty\\n\\no 5 Ks}\\n\\n3 8\\n\\n$ 2\\n\\no\\n\\n84 a — Income $4 , ~S --- Limit o / 37 Ts, Rating 5 34 ant 2 5 ~~ Student BS wn i Oo 3s4 ee OF oF 8 | B Of $e 84 2 84 si 6 1 o 8 8 8 T T T 8 T T T T T 16-02 16+00 1e+02 te+04 0.0 02 04 06 08 1.0 aR * x 8x\"Tl2/IIlle\\n\\nFIGURE 6.4. The standardized ridge regression coefficients are displayed for the Credit data set, as a function of X and ||B%*||2/||A\\\\l2.\\n\\nthe relative impact of these two terms on the regression coefficient, esti- mates. When A = 0, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as \\\\ — 00, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Unlike least squares, which generates only one set of co- efficient estimates, ridge regression will produce a different set of coefficient estimates, BR, for each value of A. Selecting a good value for 4 is critical; we defer this discussion to Section 6.2.3, where we use cross-validation.\\n\\nNote that in (6.5), the shrinkage penalty is applied to 61,...,8,, but not to the intercept 89. We want to shrink the estimated association of each variable with the response; however, we do not want to shrink the intercept, which is simply a measure of the mean value of the response when 21 = 42 =... = Lip = 0. If we assume that the variables—that is, the columns of the data matrix X—have been centered to have mean zero before ridge regression is performed, then the estimated intercept will take the form By = = SL yi/n-', 0.0819672131147541), ('lasso\\n\\nsparse\\n\\nStandardized Coefficients\\n\\n100 200 300 400 100 200 300 400 0 L | — Income --- Limit Rating --- Student T 1 1 T T T T T T T 20 50 100 200 500 2000 5000 0.0 02 04 06 08 1.0 d WE lh ! + : Standardized Coefficients -100 0 200 L L -300\\n\\nFIGURE 6.6. The standardized lasso coefficients on the Credit data set are shown as a function of and ||B*||1/||8\\\\l1-\\n\\nAs an example, consider the coefficient plots in Figure 6.6, which are gen- erated from applying the lasso to the Credit data set. When A = 0, then the lasso simply gives the least squares fit, and when A becomes sufficiently large, the lasso gives the model in which all coefficient estimates equal zero. However, in between these two extremes, the ridge regression and lasso models are quite different from each other. Moving from left to right in the right-hand panel of Figure 6.6, we observe that at first the lasso re- sults in a model that contains only the rating predictor. Then student and limit enter the model almost simultaneously, shortly followed by income. Eventually, the remaining variables enter the model. Hence, depending on the value of A, the lasso can produce a model involving any number of vari- ables. In contrast, ridge regression will always include all of the variables in the model, although the magnitude of the coefficient estimates will depend on A.', 0.08038914490527393), ('so that they are all on the same scale. In (6.6), the denominator is the estimated standard deviation of the jth predictor. Consequently, all of the standardized predictors will have a standard deviation of one. As a re- sult the final fit will not depend on the scale on which the predictors are measured. In Figure 6.4, the y-axis displays the standardized ridge regres- sion coefficient estimates—that is, the coefficient estimates that result from performing ridge regression using standardized predictors.\\n\\nWhy Does Ridge Regression Improve Over Least Squares?\\n\\nRidge regression’s advantage over least squares is rooted in the bias-variance trade-off. As increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. This is illustrated in the left-hand panel of Figure 6.5, using a simulated data set containing p = 45 predictors and n = 50 observations. The green curve in the left-hand panel\\n\\né2\\n\\nnorm\\n\\nscale equivariant\\n\\n34 24 8 - 8 84 J 5 84 — E ——————— ws | woe FT Go FT 2 i 84 § s+ as o &4 c 84 ict 2 ef = e24 o4 o4 T T T T T T T T T te-01 te+01 te+03 0.0 02 O4 06 08 1.0 aR * r Wx ll2/MBlle', 0.031746031746031744), ('6.2.1 Ridge Regression\\n\\nRecall from Chapter 3 that the least squares fitting procedure estimates Bo, 61,..., 8p using the values that minimize\\n\\n2 n Pp RSS = Ss yi — Bo - Ss Bj xij i=1 j=l\\n\\nRidge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates 6” are the values that minimize\\n\\nn 2 P Pp P Se | vi — Bo — S2 Bjariy | +A BF =RSS+AS7 BF, (6.5) j=l i=l j=l j=l\\n\\nwhere \\\\ > 0 is a tuning parameter, to be determined separately. Equa- tion 6.5 trades off two different criteria. As with least squares, ridge regres- sion seeks coefficient estimates that fit the data well, by making the RSS small. However, the second term, Ay 63, called a shrinkage penalty, is small when (,..., 8p are close to zero, and so it has the effect of shrinking the estimates of 3; towards zero. The tuning parameter \\\\ serves to control\\n\\none-\\n\\nstandard-\\n\\nerror\\n\\nrule\\n\\nridge regression\\n\\ntuning', 0.03149801587301587), ('FIGURE 3.15. Contour plots for the RSS values as a function of the parameters 6 for various regressions involving the Credit data set. In each plot, the black dots represent the coefficient values corresponding to the minimum RSS. Left: A contour plot of RSS for the regression of balance onto age and limit. The minimum value is well defined. Right: A contour plot of RSS for the regression of balance onto rating and limit. Because of the collinearity, there are many pairs (Biimit, GRating) with a similar value for RSS.\\n\\nthat correspond to the same RSS, with ellipses nearest to the center tak- ing on the lowest values of RSS. The black dots and associated dashed lines represent the coefficient estimates that result in the smallest possible RSS—in other words, these are the least squares estimates. The axes for limit and age have been scaled so that the plot includes possible coeffi- cient estimates that are up to four standard errors on either side of the least squares estimates. Thus the plot includes all plausible values for the coefficients. For example, we see that the true limit coefficient is almost certainly somewhere between 0.15 and 0.20.', 0.03149801587301587), ('statistic\\n\\n40 50 60 70 80 9 30 Rating 600 800 400 200 2000 4000 6000 8000 Limit 12000 T T T T T T 2000 4000 6000 8000 12000 Limit\\n\\nAge\\n\\nFIGURE 3.14. Scatterplots of the observations from the Credit data set. Left: A plot of age versus limit. These two variables are not collinear. Right: A plot of rating versus limit. There is high collinearity.\\n\\nIt is clear There isa we do no between 1 rom this equation that h; increases with the distance of x; from Z. simple extension of h; to the case of multiple predictors, though provide the formula here. The leverage statistic h; is always /n and 1, and the average leverage for all the observations is always equal to (p+ 1)/n. So if a given observation has a leverage statistic that great point has high leverage. ly exceeds (p+1)/n, then we may suspect that the corresponding\\n\\nThe right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus h; for the data in the left-hand panel of Figure 3.13. Ob- servation 4 high s leverage o plot a on the 1 stands out as having a very high leverage statistic as well as a udentized residual. In other words, it is an outlier as well as a high bservation. This is a particularly dangerous combination! This so reveals the reason that observation 20 had relatively little e least squares fit in Figure 3.12: it has low leverage. ect,', 0.016129032258064516), ('FIGURE 6.1. For each possible model containing a subset of the ten predictors in the Credit data set, the RSS and R? are displayed. The red frontier tracks the best model for a given number of predictors, according to RSS and R?. Though the data set contains only ten predictors, the x-axis ranges from 1 to 1 , since one of the variables is categorical and takes on three values, leading to the creation of two dummy variables.\\n\\nThen the model My, fit on the full training set is delivered for k. These approaches are discussed in Section 6.1.3. he chosen\\n\\nAn application of best subset selection is shown in Figure plotted point corresponds to a least squares regression model 6.1. Each fit using a different subset of the 10 predictors in the Credit data set, discussed in Chapter 3. Here the variable region is a three-level qualitative variable, and so is represented by two dummy variables, which are selected sepa- rately in this case. Hence, there are a total of 11 possible varia les which can be included in the model. We have plotted the RSS and R? statistics for each model, as a function of the number of variables. The red curves connect the best models for each model size, according to RSS or R?. The figure shows that, as expected, these quantities improve as the number of variables increases; however, from the three-variable model on, there is little improvement in RSS and R? as a result of including additional predictors.', 0.015625), ('6.2.3 Selecting the Tuning Parameter\\n\\nJust as the subset selection approaches considered in Section 6.1 require a method to determine which of the models under consideration is best, implementing ridge regression and the lasso requires a method for selecting a value for the tuning parameter A in (6.5) and (6.7), or equivalently, the value of the constraint s in (6.9) and (6.8). Cross-validation provides a sim- ple way to tackle this problem. We choose a grid of \\\\ values, and compute the cross-validation error for each value of \\\\, as described in Chapter 5. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.\\n\\nFigure 6.12 displays the choice of \\\\ that results from performing leave- one-out cross-validation on the ridge regression fits from the Credit data set. The dashed vertical lines indicate the selected value of X. In this case he value is relatively small, indicating that the optimal fit only involves a mall amount of shrinkage relative to the least squares solution. In addition, hat would give a very similar error. In a case like this we might simply use he least squares solution. t. s the dip is not very pronounced, so there is rather a wide range of values t. t.', 0.015625), (\"ridgeCVv)])\\n\\npipeCV.fit(X,\\n\\nY)\\n\\nLet’s produce a plot again of the cross-validation error to see that it is similar to using skm.GridSearchCV.\\n\\nIn\\n\\n[38] : tuned_ridge = pipeCV.named_steps['ridge'] ridgeCV_fig, ax = subplots(figsize=(8,8)) ax.errorbar (-np.log(lambdas) , tuned_ridge.mse_path_.mean(1), yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K)) ax.axvline(-np.log(tuned_ridge.alpha_), c='k', 1ls='--') ax.set_ylim([50000,250000]) ax.set_xlabel('$-\\\\log(\\\\lambda)$', fontsize=20) ax.set_ylabel('Cross-validated MSE', fontsize=20) ;\\n\\nWe see that the value of \\\\ that results in the smallest cross-validation error is 1.19e-02, available as the value tuned_ridge.alpha_. What is the test MSE associated with this value of \\\\?\\n\\nIn [39]: np.min(tuned_ridge.mse_path_.mean(1))\\n\\nOut [39] : 115526.71\\n\\nThis represents a further improvement over the test MSE that we got using = 4. Finally, tuned_ridge.coef_ has the coefficients fit on the entire data set at this value of A.\\n\\nIn [40]: tuned_ridge.coef_\\n\\nOut [40] :\\n\\narray ([-222.80877051, 238.77246614, 3.21103754, -2.93050845, 3.64888723, 108.90953869, -50.81896152, -105.15731984, 122.00714801, 57.1859509 , 210.35170348, 118.05683748, -150.21959435, 30.36634231, -61.62459095, 77.73832472, 40.07350744, -25.02151514, -13.68429544])\\n\\nAs expected, none of the coefficients are zero—ridge regression does not perform variable selection!\", 0.015625)], 'question': 'explanation to me ridge regression on the Credit dataset and show me formula for it.'}\n","U funkciju za ocenjivanje dokumenata uslo je:10\n","Broj dokumenata koji pripadaju datom kontekstu:7\n","[('An Application to the Credit Data\\n\\nIn Figure 6.4, the ridge regression coefficient estimates for the Credit data set are displayed. In the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\\\. For example, the black solid line represents the ridge regression estimate for the income coefficient, as \\\\ is varied. At the extreme left-hand side of the plot, \\\\ is essentially zero, and so the corresponding ridge coefficient estimates are the same as the usual least squares esti- mates. But as A increases, the ridge coefficient estimates shrink towards zero. When X is extremely large, then all of the ridge coefficient estimates are basically zero; this corresponds to the model that contains no pre- dictors. In this plot, the income, limit, rating, and student variables are displayed in distinct colors, since these variables tend to have by far the largest coefficient estimates. While the ridge coefficient estimates tend to decrease in aggregate as \\\\ increases, individual coefficients, such as rating and income, may occasionally increase as X increases.', 0.08333333333333333), ('parameter\\n\\nshrinkage penalty\\n\\no 5 Ks}\\n\\n3 8\\n\\n$ 2\\n\\no\\n\\n84 a — Income $4 , ~S --- Limit o / 37 Ts, Rating 5 34 ant 2 5 ~~ Student BS wn i Oo 3s4 ee OF oF 8 | B Of $e 84 2 84 si 6 1 o 8 8 8 T T T 8 T T T T T 16-02 16+00 1e+02 te+04 0.0 02 04 06 08 1.0 aR * x 8x\"Tl2/IIlle\\n\\nFIGURE 6.4. The standardized ridge regression coefficients are displayed for the Credit data set, as a function of X and ||B%*||2/||A\\\\l2.\\n\\nthe relative impact of these two terms on the regression coefficient, esti- mates. When A = 0, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as \\\\ — 00, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Unlike least squares, which generates only one set of co- efficient estimates, ridge regression will produce a different set of coefficient estimates, BR, for each value of A. Selecting a good value for 4 is critical; we defer this discussion to Section 6.2.3, where we use cross-validation.\\n\\nNote that in (6.5), the shrinkage penalty is applied to 61,...,8,, but not to the intercept 89. We want to shrink the estimated association of each variable with the response; however, we do not want to shrink the intercept, which is simply a measure of the mean value of the response when 21 = 42 =... = Lip = 0. If we assume that the variables—that is, the columns of the data matrix X—have been centered to have mean zero before ridge regression is performed, then the estimated intercept will take the form By = = SL yi/n-', 0.0819672131147541), ('lasso\\n\\nsparse\\n\\nStandardized Coefficients\\n\\n100 200 300 400 100 200 300 400 0 L | — Income --- Limit Rating --- Student T 1 1 T T T T T T T 20 50 100 200 500 2000 5000 0.0 02 04 06 08 1.0 d WE lh ! + : Standardized Coefficients -100 0 200 L L -300\\n\\nFIGURE 6.6. The standardized lasso coefficients on the Credit data set are shown as a function of and ||B*||1/||8\\\\l1-\\n\\nAs an example, consider the coefficient plots in Figure 6.6, which are gen- erated from applying the lasso to the Credit data set. When A = 0, then the lasso simply gives the least squares fit, and when A becomes sufficiently large, the lasso gives the model in which all coefficient estimates equal zero. However, in between these two extremes, the ridge regression and lasso models are quite different from each other. Moving from left to right in the right-hand panel of Figure 6.6, we observe that at first the lasso re- sults in a model that contains only the rating predictor. Then student and limit enter the model almost simultaneously, shortly followed by income. Eventually, the remaining variables enter the model. Hence, depending on the value of A, the lasso can produce a model involving any number of vari- ables. In contrast, ridge regression will always include all of the variables in the model, although the magnitude of the coefficient estimates will depend on A.', 0.08038914490527393), ('so that they are all on the same scale. In (6.6), the denominator is the estimated standard deviation of the jth predictor. Consequently, all of the standardized predictors will have a standard deviation of one. As a re- sult the final fit will not depend on the scale on which the predictors are measured. In Figure 6.4, the y-axis displays the standardized ridge regres- sion coefficient estimates—that is, the coefficient estimates that result from performing ridge regression using standardized predictors.\\n\\nWhy Does Ridge Regression Improve Over Least Squares?\\n\\nRidge regression’s advantage over least squares is rooted in the bias-variance trade-off. As increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. This is illustrated in the left-hand panel of Figure 6.5, using a simulated data set containing p = 45 predictors and n = 50 observations. The green curve in the left-hand panel\\n\\né2\\n\\nnorm\\n\\nscale equivariant\\n\\n34 24 8 - 8 84 J 5 84 — E ——————— ws | woe FT Go FT 2 i 84 § s+ as o &4 c 84 ict 2 ef = e24 o4 o4 T T T T T T T T T te-01 te+01 te+03 0.0 02 O4 06 08 1.0 aR * r Wx ll2/MBlle', 0.031746031746031744), ('6.2.1 Ridge Regression\\n\\nRecall from Chapter 3 that the least squares fitting procedure estimates Bo, 61,..., 8p using the values that minimize\\n\\n2 n Pp RSS = Ss yi — Bo - Ss Bj xij i=1 j=l\\n\\nRidge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates 6” are the values that minimize\\n\\nn 2 P Pp P Se | vi — Bo — S2 Bjariy | +A BF =RSS+AS7 BF, (6.5) j=l i=l j=l j=l\\n\\nwhere \\\\ > 0 is a tuning parameter, to be determined separately. Equa- tion 6.5 trades off two different criteria. As with least squares, ridge regres- sion seeks coefficient estimates that fit the data well, by making the RSS small. However, the second term, Ay 63, called a shrinkage penalty, is small when (,..., 8p are close to zero, and so it has the effect of shrinking the estimates of 3; towards zero. The tuning parameter \\\\ serves to control\\n\\none-\\n\\nstandard-\\n\\nerror\\n\\nrule\\n\\nridge regression\\n\\ntuning', 0.03149801587301587), ('6.2.3 Selecting the Tuning Parameter\\n\\nJust as the subset selection approaches considered in Section 6.1 require a method to determine which of the models under consideration is best, implementing ridge regression and the lasso requires a method for selecting a value for the tuning parameter A in (6.5) and (6.7), or equivalently, the value of the constraint s in (6.9) and (6.8). Cross-validation provides a sim- ple way to tackle this problem. We choose a grid of \\\\ values, and compute the cross-validation error for each value of \\\\, as described in Chapter 5. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.\\n\\nFigure 6.12 displays the choice of \\\\ that results from performing leave- one-out cross-validation on the ridge regression fits from the Credit data set. The dashed vertical lines indicate the selected value of X. In this case he value is relatively small, indicating that the optimal fit only involves a mall amount of shrinkage relative to the least squares solution. In addition, hat would give a very similar error. In a case like this we might simply use he least squares solution. t. s the dip is not very pronounced, so there is rather a wide range of values t. t.', 0.015625), (\"ridgeCVv)])\\n\\npipeCV.fit(X,\\n\\nY)\\n\\nLet’s produce a plot again of the cross-validation error to see that it is similar to using skm.GridSearchCV.\\n\\nIn\\n\\n[38] : tuned_ridge = pipeCV.named_steps['ridge'] ridgeCV_fig, ax = subplots(figsize=(8,8)) ax.errorbar (-np.log(lambdas) , tuned_ridge.mse_path_.mean(1), yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K)) ax.axvline(-np.log(tuned_ridge.alpha_), c='k', 1ls='--') ax.set_ylim([50000,250000]) ax.set_xlabel('$-\\\\log(\\\\lambda)$', fontsize=20) ax.set_ylabel('Cross-validated MSE', fontsize=20) ;\\n\\nWe see that the value of \\\\ that results in the smallest cross-validation error is 1.19e-02, available as the value tuned_ridge.alpha_. What is the test MSE associated with this value of \\\\?\\n\\nIn [39]: np.min(tuned_ridge.mse_path_.mean(1))\\n\\nOut [39] : 115526.71\\n\\nThis represents a further improvement over the test MSE that we got using = 4. Finally, tuned_ridge.coef_ has the coefficients fit on the entire data set at this value of A.\\n\\nIn [40]: tuned_ridge.coef_\\n\\nOut [40] :\\n\\narray ([-222.80877051, 238.77246614, 3.21103754, -2.93050845, 3.64888723, 108.90953869, -50.81896152, -105.15731984, 122.00714801, 57.1859509 , 210.35170348, 118.05683748, -150.21959435, 30.36634231, -61.62459095, 77.73832472, 40.07350744, -25.02151514, -13.68429544])\\n\\nAs expected, none of the coefficients are zero—ridge regression does not perform variable selection!\", 0.015625)]\n"]}],"source":["result2 = final_rag_chain.invoke({\"question\":\"explanation to me ridge regression on the Credit dataset and show me formula for it.\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78KSCQ16dENy","outputId":"89cf339f-6575-48b4-ceec-0e009a092a02"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross-validation error is a method used to estimate the test error rate by holding out a subset of the training observations from the fitting process and then applying the statistical learning method to those held-out observations. \n","\n","In the context of classification problems, the cross-validation error can be quantified using the number of misclassified observations. The formula for the leave-one-out cross-validation (LOOCV) error rate is given as:\n","\n","\\[\n","\\text{Err}_{CV(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{Err}_i\n","\\]\n","\n","where \\(\\text{Err}_i\\) is the error for the \\(i\\)-th observation, defined as \\(I(y_i \\neq \\hat{y}_i)\\), where \\(I\\) is an indicator function that equals 1 if the predicted value \\(\\hat{y}_i\\) does not match the actual value \\(y_i\\).\n","\n","This information is derived from the context provided, specifically from the section discussing cross-validation in classification problems and the formula for LOOCV error rate (source: \"5.1.5 Cross-Validation on Classification Problems\").\n"]}],"source":["print(result2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6AHqlgFdENz","outputId":"0e72c044-5d2c-4d31-fabc-29ecff9e5f6c"},"outputs":[{"data":{"text/plain":["['An Application to the Credit Data\\n\\nIn Figure 6.4, the ridge regression coefficient estimates for the Credit data set are displayed. In the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\\\. For example, the black solid line represents the ridge regression estimate for the income coefficient, as \\\\ is varied. At the extreme left-hand side of the plot, \\\\ is essentially zero, and so the corresponding ridge coefficient estimates are the same as the usual least squares esti- mates. But as A increases, the ridge coefficient estimates shrink towards zero. When X is extremely large, then all of the ridge coefficient estimates are basically zero; this corresponds to the model that contains no pre- dictors. In this plot, the income, limit, rating, and student variables are displayed in distinct colors, since these variables tend to have by far the largest coefficient estimates. While the ridge coefficient estimates tend to decrease in aggregate as \\\\ increases, individual coefficients, such as rating and income, may occasionally increase as X increases.',\n"," 'parameter\\n\\nshrinkage penalty\\n\\no 5 Ks}\\n\\n3 8\\n\\n$ 2\\n\\no\\n\\n84 a — Income $4 , ~S --- Limit o / 37 Ts, Rating 5 34 ant 2 5 ~~ Student BS wn i Oo 3s4 ee OF oF 8 | B Of $e 84 2 84 si 6 1 o 8 8 8 T T T 8 T T T T T 16-02 16+00 1e+02 te+04 0.0 02 04 06 08 1.0 aR * x 8x\"Tl2/IIlle\\n\\nFIGURE 6.4. The standardized ridge regression coefficients are displayed for the Credit data set, as a function of X and ||B%*||2/||A\\\\l2.\\n\\nthe relative impact of these two terms on the regression coefficient, esti- mates. When A = 0, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as \\\\ — 00, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Unlike least squares, which generates only one set of co- efficient estimates, ridge regression will produce a different set of coefficient estimates, BR, for each value of A. Selecting a good value for 4 is critical; we defer this discussion to Section 6.2.3, where we use cross-validation.\\n\\nNote that in (6.5), the shrinkage penalty is applied to 61,...,8,, but not to the intercept 89. We want to shrink the estimated association of each variable with the response; however, we do not want to shrink the intercept, which is simply a measure of the mean value of the response when 21 = 42 =... = Lip = 0. If we assume that the variables—that is, the columns of the data matrix X—have been centered to have mean zero before ridge regression is performed, then the estimated intercept will take the form By = = SL yi/n-',\n"," 'lasso\\n\\nsparse\\n\\nStandardized Coefficients\\n\\n100 200 300 400 100 200 300 400 0 L | — Income --- Limit Rating --- Student T 1 1 T T T T T T T 20 50 100 200 500 2000 5000 0.0 02 04 06 08 1.0 d WE lh ! + : Standardized Coefficients -100 0 200 L L -300\\n\\nFIGURE 6.6. The standardized lasso coefficients on the Credit data set are shown as a function of and ||B*||1/||8\\\\l1-\\n\\nAs an example, consider the coefficient plots in Figure 6.6, which are gen- erated from applying the lasso to the Credit data set. When A = 0, then the lasso simply gives the least squares fit, and when A becomes sufficiently large, the lasso gives the model in which all coefficient estimates equal zero. However, in between these two extremes, the ridge regression and lasso models are quite different from each other. Moving from left to right in the right-hand panel of Figure 6.6, we observe that at first the lasso re- sults in a model that contains only the rating predictor. Then student and limit enter the model almost simultaneously, shortly followed by income. Eventually, the remaining variables enter the model. Hence, depending on the value of A, the lasso can produce a model involving any number of vari- ables. In contrast, ridge regression will always include all of the variables in the model, although the magnitude of the coefficient estimates will depend on A.',\n"," '6.2.1 Ridge Regression\\n\\nRecall from Chapter 3 that the least squares fitting procedure estimates Bo, 61,..., 8p using the values that minimize\\n\\n2 n Pp RSS = Ss yi — Bo - Ss Bj xij i=1 j=l\\n\\nRidge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates 6” are the values that minimize\\n\\nn 2 P Pp P Se | vi — Bo — S2 Bjariy | +A BF =RSS+AS7 BF, (6.5) j=l i=l j=l j=l\\n\\nwhere \\\\ > 0 is a tuning parameter, to be determined separately. Equa- tion 6.5 trades off two different criteria. As with least squares, ridge regres- sion seeks coefficient estimates that fit the data well, by making the RSS small. However, the second term, Ay 63, called a shrinkage penalty, is small when (,..., 8p are close to zero, and so it has the effect of shrinking the estimates of 3; towards zero. The tuning parameter \\\\ serves to control\\n\\none-\\n\\nstandard-\\n\\nerror\\n\\nrule\\n\\nridge regression\\n\\ntuning',\n"," 'FIGURE 3.15. Contour plots for the RSS values as a function of the parameters 6 for various regressions involving the Credit data set. In each plot, the black dots represent the coefficient values corresponding to the minimum RSS. Left: A contour plot of RSS for the regression of balance onto age and limit. The minimum value is well defined. Right: A contour plot of RSS for the regression of balance onto rating and limit. Because of the collinearity, there are many pairs (Biimit, GRating) with a similar value for RSS.\\n\\nthat correspond to the same RSS, with ellipses nearest to the center tak- ing on the lowest values of RSS. The black dots and associated dashed lines represent the coefficient estimates that result in the smallest possible RSS—in other words, these are the least squares estimates. The axes for limit and age have been scaled so that the plot includes possible coeffi- cient estimates that are up to four standard errors on either side of the least squares estimates. Thus the plot includes all plausible values for the coefficients. For example, we see that the true limit coefficient is almost certainly somewhere between 0.15 and 0.20.']"]},"execution_count":210,"metadata":{},"output_type":"execute_result"}],"source":["retriever.invoke(\"explanation to me ridge regression on the Credit dataset and show me formula for it\") # 18. dokument je pravi!!!!! NAPISAO SAM FORMULA I ONDA SE POJAVIO DRUGI\n","# Tell me formula for quadratic logistic regression model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UcrHboTdENz"},"outputs":[],"source":["retriever.search_kwargs = {\"k\":20}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72NfgFLpdEN2","outputId":"44871b20-1806-4f1c-c4d0-ab8ce4605646"},"outputs":[{"data":{"text/plain":["'The provided context does not contain any information about Bayes classifiers or K-Nearest Neighbors (KNN) decision boundaries. It primarily discusses the bootstrap method, its application in estimating standard errors, and some examples related to linear regression and statistical learning methods.\\n\\nTo compare the decision boundaries of Bayes classifiers and KNN, we can summarize the general differences:\\n\\n1. **Bayes Classifier**:\\n   - The Bayes classifier uses the concept of probability and assumes a probabilistic model of the data. It calculates the posterior probabilities of each class given the input features and assigns the class with the highest probability.\\n   - The decision boundary is typically smooth and can be complex, depending on the underlying distribution of the data. It is influenced by the assumptions made about the distribution of the features (e.g., Gaussian distribution).\\n\\n2. **K-Nearest Neighbors (KNN)**:\\n   - KNN is a non-parametric method that classifies a data point based on the majority class among its K nearest neighbors in the feature space.\\n   - The decision boundary created by KNN is often more irregular and can adapt closely to the training data. It can create complex boundaries that follow the shape of the data distribution, especially with a small value of K.\\n\\nIn visual terms, if you were to plot the decision boundaries of both classifiers on a 2D feature space, the Bayes decision boundary would likely appear as a smooth curve, while the KNN decision boundary would appear jagged and could change significantly with different values of K.\\n\\nIf you need a specific visual comparison or further details, please provide additional context or data related to Bayes and KNN classifiers.'"]},"execution_count":275,"metadata":{},"output_type":"execute_result"}],"source":["from query_translation import load_hypodocument_chain\n","\n","hydo_question = load_hypodocument_chain()\n","retrieval_chain =  hydo_question | retriever\n","retireved_docs = retrieval_chain.invoke({\"question\":\"formula for standard error of bootstrap estimates.\"})\n","# RAG\n","template = \"\"\"Answer the following question based on this context:\n","\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","final_rag_chain = (\n","    prompt\n","    | chat\n","    | StrOutputParser()\n",")\n","\n","final_rag_chain.invoke({\"context\":retireved_docs,\"question\":\"Show me the comparison between Bayes and KNN decision boundary.\"})"]},{"cell_type":"markdown","metadata":{"id":"aToPr0pXdEN3"},"source":["**Mongo**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tJF-tWPdEN3"},"outputs":[],"source":["from mongo_db_connection import semantic_search\n","\n","query =  \"What is a studentized residual, and how is it used to identify outliers?\"\n","#RunnableLambda(semantic_search).map(query)\n","\n","mongo_db_retriever_chain = (query_generation_chain\n","                            |(lambda x: x.queries)\n","                            | RunnableLambda(semantic_search).map()\n","                            | reciprocal_rank_fusion\n","                            )\n","final_chain = (\n","    {\"context_documents\":mongo_db_retriever_chain,\n","     \"question\":itemgetter(\"question\")}\n","    | RunnableLambda(grade_retrieved_documents)\n","    | final_prompt\n","    | chat\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DugZMCwidEN3","outputId":"5d2bc459-f736-46c9-de23-61209f670b09"},"outputs":[{"name":"stdout","output_type":"stream","text":["INPUT {'context_documents': [('Residual plots can be used to identify outliers. In this example, the out- lier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a resid- ual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual e; by its estimated standard error. Observations whose studentized residuals are greater than 3 in abso- lute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier’s studentized residual exceeds 6, while all other observations have studentized residuals between —2 and 2.\\n\\nIf we believe that an outlier has occurred due tion or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. o an error in data collec-', 0.08333333333333333), ('Sometimes we have a good idea of the variance of each response. For example, the ith response could be an average of n; raw observations. If each of these raw observations is uncorrelated with variance o?, then their average has variance 0? = o7/nj. In this case a simple remedy is to fit our model by weighted least squares, with weights proportional to the inverse variances—i.e. w; = n; in this case. Most linear regression software allows for observation weights.\\n\\n4. Outliers\\n\\nAn outlier is a point for which y; is far from the value predicted by the\\n\\nhetero- scedasticity\\n\\nweighted least squares\\n\\nontlien *) er\\n\\n26 of 5 oa o 4 g 24 go é a ©0 go 277 oO ° =z oo o G0 Mo g 4 oO 3 Og.000°. ° 3 ass og] BOF GBH oe 8 5° 8 0° 74 0 80k ° o og ‘ lO o lO ° 20 2 4 6 20 2 4 6 xX Fitted Values Fitted Values\\n\\n.\\n\\nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between —3 and 3.\\n\\nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.', 0.0814383923849815), (\"Many methods have been developed to properly take account of corre- lations in the error terms in time series data. Correlation among the error terms can also occur outside of time series data. For instance, consider a study in which individuals’ heights are predicted from their weights. The assumption of uncorrelated errors could be violated if some of the indi- viduals in the study are members of the same family, eat the same diet, or have been exposed to the same environmental factors. In general, the assumption of uncorrelated errors is extremely important for linear regres- sion as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.\\n\\ntracking\\n\\n0 3B 3 ir\\n\\nResponse Y Response log(Y) 15 998, x J 4 a | ek es ae 10 0. 2 $4 Sree ee ee 3s ° 3 g°4 334 eT ye 4 34 ' 605 o © 24 e4 er 437 cy T T T T T PT T T T T T 10 15 20 25 30 24 26 28 3.0 3.2 3.4 Fitted values Fitted values\\n\\nFIGURE 3.11. Residual plots. In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals, and emphasize patterns. Left: The funnel shape indicates heteroscedasticity. Right: The response has been log transformed, and there is now no evidence of heteroscedasticity.\", 0.07886904761904762), ('The red point (observation 20) in the left-hand panel of Figure 3.12 illustrates a typical outlier. The red solid line is the least squares regression fit, while the blue dashed line is the least squares fit after removal of the outlier. In this case, removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a miniscule reduction in the intercept. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed. Since the RSE is used to compute all confidence intervals and p-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Similarly, inclusion of the outlier causes the R? to decline from 0.892 to 0.805.', 0.06452452301209573), ('residual plot\\n\\nYi — Yi, versus the predictor x;. In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values §;. Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\\n\\nThe left panel of Figure 3.9 displays a residual plot from the linear re- gression of mpg ont: Figure 3.8. The re © horsepower on the Auto data set that was illustrated in ine is a smooth fit to the residuals, which is displayed in order to make it easier to identify any trends. The residuals exhibit a clear U-shape, which provides a strong indication of non-linearity in the data. In contrast, the ri that results from t appears to be littl term improves the ght-hand panel of Figure 3.9 displays the residual plot he model (3.36), which contains a quadratic term. There e pattern in the residuals, suggesting that the quadratic fit to the data.\\n\\nIf the residual p data, then a simp predictors, ot indicates that there are non-linear associations in the le approach is to use non-linear transformations of the such as log X, VX, and X?, in the regression model. In the later chapters of this book, we will discuss other more advanced non-linear approaches for ad ressing this issue.', 0.06377247488101534), (\"In\\n\\n[34]:| ax = subplots (figsize=(8,8)) [1] ax.scatter(results3.fittedvalues, results3.resid) ax.set_xlabel('Fitted value') ax.set_ylabel('Residual') ax.axhline(0, c='k', 1ls='--')\\n\\nWe see that when the quadratic term is included in the model, there is little discernible pattern in the residuals. In order to create a cubic or higher-degree polynomial fit, we can simply change the degree argument to poly(.\\n\\n3.6.7 Qualitative Predictors\\n\\nHere we use the Carseats data, which is included in the ISLP package. We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.\\n\\nIn [35]:| Carseats = load_data('Carseats') Carseats.columns\\n\\n[35]: Index(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price', 'ShelveLoc', 'Age', 'Education', 'Urban', 'US'], dtype='object')\", 0.015873015873015872), ('statistic\\n\\n40 50 60 70 80 9 30 Rating 600 800 400 200 2000 4000 6000 8000 Limit 12000 T T T T T T 2000 4000 6000 8000 12000 Limit\\n\\nAge\\n\\nFIGURE 3.14. Scatterplots of the observations from the Credit data set. Left: A plot of age versus limit. These two variables are not collinear. Right: A plot of rating versus limit. There is high collinearity.\\n\\nIt is clear There isa we do no between 1 rom this equation that h; increases with the distance of x; from Z. simple extension of h; to the case of multiple predictors, though provide the formula here. The leverage statistic h; is always /n and 1, and the average leverage for all the observations is always equal to (p+ 1)/n. So if a given observation has a leverage statistic that great point has high leverage. ly exceeds (p+1)/n, then we may suspect that the corresponding\\n\\nThe right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus h; for the data in the left-hand panel of Figure 3.13. Ob- servation 4 high s leverage o plot a on the 1 stands out as having a very high leverage statistic as well as a udentized residual. In other words, it is an outlier as well as a high bservation. This is a particularly dangerous combination! This so reveals the reason that observation 20 had relatively little e least squares fit in Figure 3.12: it has low leverage. ect,', 0.015625)], 'question': 'What is a studentized residual, and how is it used to identify outliers?'}\n","U funkciju za ocenjivanje dokumenata uslo je:7\n","Broj dokumenata koji pripadaju datom kontekstu:3\n","[('Residual plots can be used to identify outliers. In this example, the out- lier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a resid- ual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual e; by its estimated standard error. Observations whose studentized residuals are greater than 3 in abso- lute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier’s studentized residual exceeds 6, while all other observations have studentized residuals between —2 and 2.\\n\\nIf we believe that an outlier has occurred due tion or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. o an error in data collec-', 0.08333333333333333), ('Sometimes we have a good idea of the variance of each response. For example, the ith response could be an average of n; raw observations. If each of these raw observations is uncorrelated with variance o?, then their average has variance 0? = o7/nj. In this case a simple remedy is to fit our model by weighted least squares, with weights proportional to the inverse variances—i.e. w; = n; in this case. Most linear regression software allows for observation weights.\\n\\n4. Outliers\\n\\nAn outlier is a point for which y; is far from the value predicted by the\\n\\nhetero- scedasticity\\n\\nweighted least squares\\n\\nontlien *) er\\n\\n26 of 5 oa o 4 g 24 go é a ©0 go 277 oO ° =z oo o G0 Mo g 4 oO 3 Og.000°. ° 3 ass og] BOF GBH oe 8 5° 8 0° 74 0 80k ° o og ‘ lO o lO ° 20 2 4 6 20 2 4 6 xX Fitted Values Fitted Values\\n\\n.\\n\\nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between —3 and 3.\\n\\nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.', 0.0814383923849815), ('statistic\\n\\n40 50 60 70 80 9 30 Rating 600 800 400 200 2000 4000 6000 8000 Limit 12000 T T T T T T 2000 4000 6000 8000 12000 Limit\\n\\nAge\\n\\nFIGURE 3.14. Scatterplots of the observations from the Credit data set. Left: A plot of age versus limit. These two variables are not collinear. Right: A plot of rating versus limit. There is high collinearity.\\n\\nIt is clear There isa we do no between 1 rom this equation that h; increases with the distance of x; from Z. simple extension of h; to the case of multiple predictors, though provide the formula here. The leverage statistic h; is always /n and 1, and the average leverage for all the observations is always equal to (p+ 1)/n. So if a given observation has a leverage statistic that great point has high leverage. ly exceeds (p+1)/n, then we may suspect that the corresponding\\n\\nThe right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus h; for the data in the left-hand panel of Figure 3.13. Ob- servation 4 high s leverage o plot a on the 1 stands out as having a very high leverage statistic as well as a udentized residual. In other words, it is an outlier as well as a high bservation. This is a particularly dangerous combination! This so reveals the reason that observation 20 had relatively little e least squares fit in Figure 3.12: it has low leverage. ect,', 0.015625)]\n","A studentized residual is computed by dividing each residual by its estimated standard error. It is used to identify outliers by examining the absolute value of the studentized residuals; observations whose studentized residuals are greater than 3 in absolute value are considered possible outliers. For example, in the context provided, one outlier had a studentized residual that exceeded 6, while all other observations had studentized residuals between -2 and 2 (as mentioned in the first part of the context).\n"]}],"source":["result = final_chain.invoke({\"question\":query}) # mongo db chain\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vK10Znn9dEN3","outputId":"fecb0bcd-5ea0-4fb9-d0ee-6acfa5494e25"},"outputs":[{"data":{"text/plain":["['Residual plots can be used to identify outliers. In this example, the out- lier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a resid- ual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual e; by its estimated standard error. Observations whose studentized residuals are greater than 3 in abso- lute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier’s studentized residual exceeds 6, while all other observations have studentized residuals between —2 and 2.\\n\\nIf we believe that an outlier has occurred due tion or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. o an error in data collec-',\n"," 'Sometimes we have a good idea of the variance of each response. For example, the ith response could be an average of n; raw observations. If each of these raw observations is uncorrelated with variance o?, then their average has variance 0? = o7/nj. In this case a simple remedy is to fit our model by weighted least squares, with weights proportional to the inverse variances—i.e. w; = n; in this case. Most linear regression software allows for observation weights.\\n\\n4. Outliers\\n\\nAn outlier is a point for which y; is far from the value predicted by the\\n\\nhetero- scedasticity\\n\\nweighted least squares\\n\\nontlien *) er\\n\\n26 of 5 oa o 4 g 24 go é a ©0 go 277 oO ° =z oo o G0 Mo g 4 oO 3 Og.000°. ° 3 ass og] BOF GBH oe 8 5° 8 0° 74 0 80k ° o og ‘ lO o lO ° 20 2 4 6 20 2 4 6 xX Fitted Values Fitted Values\\n\\n.\\n\\nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between —3 and 3.\\n\\nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.',\n"," 'The red point (observation 20) in the left-hand panel of Figure 3.12 illustrates a typical outlier. The red solid line is the least squares regression fit, while the blue dashed line is the least squares fit after removal of the outlier. In this case, removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a miniscule reduction in the intercept. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed. Since the RSE is used to compute all confidence intervals and p-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Similarly, inclusion of the outlier causes the R? to decline from 0.892 to 0.805.',\n"," 'residual plot\\n\\nYi — Yi, versus the predictor x;. In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values §;. Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\\n\\nThe left panel of Figure 3.9 displays a residual plot from the linear re- gression of mpg ont: Figure 3.8. The re © horsepower on the Auto data set that was illustrated in ine is a smooth fit to the residuals, which is displayed in order to make it easier to identify any trends. The residuals exhibit a clear U-shape, which provides a strong indication of non-linearity in the data. In contrast, the ri that results from t appears to be littl term improves the ght-hand panel of Figure 3.9 displays the residual plot he model (3.36), which contains a quadratic term. There e pattern in the residuals, suggesting that the quadratic fit to the data.\\n\\nIf the residual p data, then a simp predictors, ot indicates that there are non-linear associations in the le approach is to use non-linear transformations of the such as log X, VX, and X?, in the regression model. In the later chapters of this book, we will discuss other more advanced non-linear approaches for ad ressing this issue.',\n"," \"In\\n\\n[34]:| ax = subplots (figsize=(8,8)) [1] ax.scatter(results3.fittedvalues, results3.resid) ax.set_xlabel('Fitted value') ax.set_ylabel('Residual') ax.axhline(0, c='k', 1ls='--')\\n\\nWe see that when the quadratic term is included in the model, there is little discernible pattern in the residuals. In order to create a cubic or higher-degree polynomial fit, we can simply change the degree argument to poly(.\\n\\n3.6.7 Qualitative Predictors\\n\\nHere we use the Carseats data, which is included in the ISLP package. We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.\\n\\nIn [35]:| Carseats = load_data('Carseats') Carseats.columns\\n\\n[35]: Index(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price', 'ShelveLoc', 'Age', 'Education', 'Urban', 'US'], dtype='object')\"]"]},"execution_count":239,"metadata":{},"output_type":"execute_result"}],"source":["semantic_search(query)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAnTiQigdEN4","outputId":"eb7c8cfe-ab1e-45fc-b0f4-4ab3fa67e3d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["1310 loaded from results/chunked_elements_1.pkl.\n","104 loaded from results/1_table_summaries.pkl.\n","1206 loaded from results/1_txt_summaries.pkl.\n","221 loaded from results/1_image_summaries.pkl.\n","268 loaded from results/formula_descriptions.pkl.\n","1799 documents stored.\n"]}],"source":["chunks_path = \"results/chunked_elements_1.pkl\" # chunk_by_title strategy, combine_text_under_n_chars=1000,max_characters=1600, multipage_sections=True, new_after_n_chars=1200, overlap=True,\n","txt_summaries_path = \"results/1_txt_summaries.pkl\"\n","tbl_summaries_path = \"results/1_table_summaries.pkl\"\n","retriever = create_retriever(chunks_path,txt_summaries_path,tbl_summaries_path,\"similarity\",5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eO-ExC5MdEN4"},"outputs":[],"source":["#delete_collection(retriever)\n","retriever.invoke(query)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtqYwuY-dEN4"},"outputs":[],"source":["from faiss_db_connection import create_retriever\n","faiss_retriever = create_retriever()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJh31R1AdEN4","outputId":"cf20cecd-23ea-45cc-cbe8-189ab100fa9b"},"outputs":[{"data":{"text/plain":["[Element(type='text', text='Residual plots can be used to identify outliers. In this example, the out- lier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a resid- ual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual e; by its estimated standard error. Observations whose studentized residuals are greater than 3 in abso- lute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier’s studentized residual exceeds 6, while all other observations have studentized residuals between —2 and 2.\\n\\nIf we believe that an outlier has occurred due tion or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. o an error in data collec-', page_no=113),\n"," Element(type='text', text='Sometimes we have a good idea of the variance of each response. For example, the ith response could be an average of n; raw observations. If each of these raw observations is uncorrelated with variance o?, then their average has variance 0? = o7/nj. In this case a simple remedy is to fit our model by weighted least squares, with weights proportional to the inverse variances—i.e. w; = n; in this case. Most linear regression software allows for observation weights.\\n\\n4. Outliers\\n\\nAn outlier is a point for which y; is far from the value predicted by the\\n\\nhetero- scedasticity\\n\\nweighted least squares\\n\\nontlien *) er\\n\\n26 of 5 oa o 4 g 24 go é a ©0 go 277 oO ° =z oo o G0 Mo g 4 oO 3 Og.000°. ° 3 ass og] BOF GBH oe 8 5° 8 0° 74 0 80k ° o og ‘ lO o lO ° 20 2 4 6 20 2 4 6 xX Fitted Values Fitted Values\\n\\n.\\n\\nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between —3 and 3.\\n\\nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.', page_no=112),\n"," Element(type='text', text='The red point (observation 20) in the left-hand panel of Figure 3.12 illustrates a typical outlier. The red solid line is the least squares regression fit, while the blue dashed line is the least squares fit after removal of the outlier. In this case, removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a miniscule reduction in the intercept. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed. Since the RSE is used to compute all confidence intervals and p-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Similarly, inclusion of the outlier causes the R? to decline from 0.892 to 0.805.', page_no=113),\n"," Element(type='text', text='residual plot\\n\\nYi — Yi, versus the predictor x;. In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values §;. Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\\n\\nThe left panel of Figure 3.9 displays a residual plot from the linear re- gression of mpg ont: Figure 3.8. The re © horsepower on the Auto data set that was illustrated in ine is a smooth fit to the residuals, which is displayed in order to make it easier to identify any trends. The residuals exhibit a clear U-shape, which provides a strong indication of non-linearity in the data. In contrast, the ri that results from t appears to be littl term improves the ght-hand panel of Figure 3.9 displays the residual plot he model (3.36), which contains a quadratic term. There e pattern in the residuals, suggesting that the quadratic fit to the data.\\n\\nIf the residual p data, then a simp predictors, ot indicates that there are non-linear associations in the le approach is to use non-linear transformations of the such as log X, VX, and X?, in the regression model. In the later chapters of this book, we will discuss other more advanced non-linear approaches for ad ressing this issue.', page_no=109),\n"," Element(type='text', text=\"In\\n\\n[34]:| ax = subplots (figsize=(8,8)) [1] ax.scatter(results3.fittedvalues, results3.resid) ax.set_xlabel('Fitted value') ax.set_ylabel('Residual') ax.axhline(0, c='k', 1ls='--')\\n\\nWe see that when the quadratic term is included in the model, there is little discernible pattern in the residuals. In order to create a cubic or higher-degree polynomial fit, we can simply change the degree argument to poly(.\\n\\n3.6.7 Qualitative Predictors\\n\\nHere we use the Carseats data, which is included in the ISLP package. We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.\\n\\nIn [35]:| Carseats = load_data('Carseats') Carseats.columns\\n\\n[35]: Index(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price', 'ShelveLoc', 'Age', 'Education', 'Urban', 'US'], dtype='object')\", page_no=135),\n"," Element(type='text', text=',, , Regression()\\n\\n6.6 Exercises\\n\\nConceptual\\n\\n1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2,.. .,p predictors. Explain your answers:\\n\\n(a) Which o: training RSS? the three models with k predictors has the smallest\\n\\n(b) Which o: test RSS? the three models with k predictors has the smallest\\n\\n(c) True or False:\\n\\ni. The predictors in t stepwise are a subset o! model identified he k-variable mode the predictors by forward stepwise identified by forward in the (k+1)-variable selection.\\n\\nii. T ward varia he predictors in ble model id stepwise are a sul lentified by he k-variable mo: bset o. the predictors backward stepwise selection. ified by back- in the (k + 1)- el ident\\n\\niii. T ward varia he predictors in ble model id stepwise are a sul lentified by he k-variable mo: bset o. el identified by back- the predictors in the (k + 1)- orward stepwise selection.\\n\\niv. T he predictors in t stepwise are a subset o! model identified he k-variable mode the predictors identified by forward in the (k+1)-variable by backward stepwise selection.\\n\\nT ne predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.', page_no=290)]"]},"execution_count":246,"metadata":{},"output_type":"execute_result"}],"source":["faiss_retriever.invoke(query)"]},{"cell_type":"markdown","metadata":{"id":"U-bEmUzwdEN4"},"source":["**Chroma**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEa8qIcWdEN4","outputId":"9c3ff542-7c93-44c5-d916-322cebeef0b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["INPUT {'context_documents': [('Residual plots can be used to identify outliers. In this example, the out- lier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a resid- ual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual e; by its estimated standard error. Observations whose studentized residuals are greater than 3 in abso- lute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier’s studentized residual exceeds 6, while all other observations have studentized residuals between —2 and 2.\\n\\nIf we believe that an outlier has occurred due tion or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. o an error in data collec-', 0.08333333333333333), ('ontlien *) er\\n\\n26 of 5 oa o 4 g 24 go é a ©0 go 277 oO ° =z oo o G0 Mo g 4 oO 3 Og.000°. ° 3 ass og] BOF GBH oe 8 5° 8 0° 74 0 80k ° o og ‘ lO o lO ° 20 2 4 6 20 2 4 6 xX Fitted Values Fitted Values\\n\\n.\\n\\nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between —3 and 3.\\n\\nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.', 0.0819672131147541), ('The red point (observation 20) in the left-hand panel of Figure 3.12 illustrates a typical outlier. The red solid line is the least squares regression fit, while the blue dashed line is the least squares fit after removal of the outlier. In this case, removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a miniscule reduction in the intercept. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed. Since the RSE is used to compute all confidence intervals and p-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Similarly, inclusion of the outlier causes the R? to decline from 0.892 to 0.805.', 0.06451612903225806), (\"0 3B 3 ir\\n\\nResponse Y Response log(Y) 15 998, x J 4 a | ek es ae 10 0. 2 $4 Sree ee ee 3s ° 3 g°4 334 eT ye 4 34 ' 605 o © 24 e4 er 437 cy T T T T T PT T T T T T 10 15 20 25 30 24 26 28 3.0 3.2 3.4 Fitted values Fitted values\\n\\nFIGURE 3.11. Residual plots. In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals, and emphasize patterns. Left: The funnel shape indicates heteroscedasticity. Right: The response has been log transformed, and there is now no evidence of heteroscedasticity.\", 0.06274801587301587), ('Age\\n\\nFIGURE 3.14. Scatterplots of the observations from the Credit data set. Left: A plot of age versus limit. These two variables are not collinear. Right: A plot of rating versus limit. There is high collinearity.\\n\\nIt is clear There isa we do no between 1 rom this equation that h; increases with the distance of x; from Z. simple extension of h; to the case of multiple predictors, though provide the formula here. The leverage statistic h; is always /n and 1, and the average leverage for all the observations is always equal to (p+ 1)/n. So if a given observation has a leverage statistic that great point has high leverage. ly exceeds (p+1)/n, then we may suspect that the corresponding\\n\\nThe right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus h; for the data in the left-hand panel of Figure 3.13. Ob- servation 4 high s leverage o plot a on the 1 stands out as having a very high leverage statistic as well as a udentized residual. In other words, it is an outlier as well as a high bservation. This is a particularly dangerous combination! This so reveals the reason that observation 20 had relatively little e least squares fit in Figure 3.12: it has low leverage. ect,', 0.04762704813108039), ('Let 9 = Bo + Bix; be the prediction for Y based on the ith value of X. Then e; = y; — 9; represents the ith residual—this is the difference between the ith observed response value and the ith response value that is predicted by our linear model. We define the residual sum of squares (RSS) as\\n\\nRSS = ef +e3 +++: +2,\\n\\nor equivalently as\\n\\nRSS = (1 — Bo — B11)? + (2 — Bo — Bia)? +--+ (Yn — Bo - rtm)”. (3.3)\\n\\nThe least squares approach chooses Bo and Bi to minimize the RSS. Using some calculus, one can show that the minimizers are\\n\\n(3.4)\\n\\nintercept slope coefficient parameter\\n\\nleast squares', 0.031746031746031744), (\"In\\n\\n[34]:| ax = subplots (figsize=(8,8)) [1] ax.scatter(results3.fittedvalues, results3.resid) ax.set_xlabel('Fitted value') ax.set_ylabel('Residual') ax.axhline(0, c='k', 1ls='--')\\n\\nWe see that when the quadratic term is included in the model, there is little discernible pattern in the residuals. In order to create a cubic or higher-degree polynomial fit, we can simply change the degree argument to poly(.\\n\\n3.6.7 Qualitative Predictors\\n\\nHere we use the Carseats data, which is included in the ISLP package. We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.\\n\\nIn [35]:| Carseats = load_data('Carseats') Carseats.columns\\n\\n[35]: Index(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price', 'ShelveLoc', 'Age', 'Education', 'Urban', 'US'], dtype='object')\", 0.015873015873015872), ('In a simple linear regression, high leverage observations are fairly easy to identify, since we can simply look for observations for which the predictor value is outside of the normal range of the observations. But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors. An example is shown in the center panel of Figure 3.13, for a data set with two predictors, X; and X2. Most of the observations’ predictor values fall within the blue dashed ellipse, but the red observation is well outside of this range. But neither its value for X; nor its value for X2 is unusual. So if we examine just X1 or just X, we will fail to notice this high leverage point. This problem is more pronounced in multiple regression settings with more than two predictors, because then there is no simple way to plot all dimensions of the data simultaneously.', 0.015625)], 'question': 'What is a studentized residual, and how is it used to identify outliers?'}\n","U funkciju za ocenjivanje dokumenata uslo je:8\n","Broj dokumenata koji pripadaju datom kontekstu:3\n","[('Residual plots can be used to identify outliers. In this example, the out- lier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a resid- ual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual e; by its estimated standard error. Observations whose studentized residuals are greater than 3 in abso- lute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier’s studentized residual exceeds 6, while all other observations have studentized residuals between —2 and 2.\\n\\nIf we believe that an outlier has occurred due tion or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. o an error in data collec-', 0.08333333333333333), ('ontlien *) er\\n\\n26 of 5 oa o 4 g 24 go é a ©0 go 277 oO ° =z oo o G0 Mo g 4 oO 3 Og.000°. ° 3 ass og] BOF GBH oe 8 5° 8 0° 74 0 80k ° o og ‘ lO o lO ° 20 2 4 6 20 2 4 6 xX Fitted Values Fitted Values\\n\\n.\\n\\nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between —3 and 3.\\n\\nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.', 0.0819672131147541), ('Age\\n\\nFIGURE 3.14. Scatterplots of the observations from the Credit data set. Left: A plot of age versus limit. These two variables are not collinear. Right: A plot of rating versus limit. There is high collinearity.\\n\\nIt is clear There isa we do no between 1 rom this equation that h; increases with the distance of x; from Z. simple extension of h; to the case of multiple predictors, though provide the formula here. The leverage statistic h; is always /n and 1, and the average leverage for all the observations is always equal to (p+ 1)/n. So if a given observation has a leverage statistic that great point has high leverage. ly exceeds (p+1)/n, then we may suspect that the corresponding\\n\\nThe right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus h; for the data in the left-hand panel of Figure 3.13. Ob- servation 4 high s leverage o plot a on the 1 stands out as having a very high leverage statistic as well as a udentized residual. In other words, it is an outlier as well as a high bservation. This is a particularly dangerous combination! This so reveals the reason that observation 20 had relatively little e least squares fit in Figure 3.12: it has low leverage. ect,', 0.04762704813108039)]\n","A studentized residual is a type of residual that has been standardized by dividing each residual by its estimated standard error. This transformation allows for a more consistent comparison of residuals across different observations. \n","\n","In the context of identifying outliers, studentized residuals are particularly useful because they provide a way to assess how far an observation's residual is from what would be expected under the model. Observations with studentized residuals greater than 3 in absolute value are considered possible outliers. For example, in the provided context, an outlier was identified with a studentized residual exceeding 6, while other observations had studentized residuals ranging between -2 and 2. This indicates that the outlier is significantly different from the other observations, suggesting it may warrant further investigation or potential removal from the dataset.\n"]}],"source":["result2 = final_rag_chain.invoke({\"question\":query}) # Chroma\n","print(result2)"]},{"cell_type":"markdown","metadata":{"id":"lkbpCBtYdEN4"},"source":["**FAISS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SE_EOs08dEN5"},"outputs":[],"source":["from faiss_db_connection import create_retriever\n","\n","faiss_retriever = create_retriever()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JkX-b3W6dEN5"},"outputs":[],"source":["def retrieve_lists_of_text(data: list[list[Element]])->list[list[str]]:\n","    cleared_lists = []\n","    for elements_list in data:\n","        cleared_lists.append([elem.text for elem in elements_list])\n","    return cleared_lists\n","\n","\n","faiss_db_retriever_chain = (query_generation_chain\n","                            |(lambda x: x.queries)\n","                            | faiss_retriever.map()\n","                            | RunnableLambda(retrieve_lists_of_text)\n","                            | reciprocal_rank_fusion\n","                            )\n","final_faiss_chain = (\n","    {\"context_documents\":faiss_db_retriever_chain,\n","     \"question\":itemgetter(\"question\")}\n","    | RunnableLambda(grade_retrieved_documents) # retrieval check\n","    | final_prompt\n","    | chat\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LA_5s4JdEN5","outputId":"06340e61-b313-4491-e315-7b218d044d08"},"outputs":[{"name":"stdout","output_type":"stream","text":["INPUT {'context_documents': [('Residual plots can be used to identify outliers. In this example, the out- lier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a resid- ual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual e; by its estimated standard error. Observations whose studentized residuals are greater than 3 in abso- lute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier’s studentized residual exceeds 6, while all other observations have studentized residuals between —2 and 2.\\n\\nIf we believe that an outlier has occurred due tion or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. o an error in data collec-', 0.08333333333333333), ('Sometimes we have a good idea of the variance of each response. For example, the ith response could be an average of n; raw observations. If each of these raw observations is uncorrelated with variance o?, then their average has variance 0? = o7/nj. In this case a simple remedy is to fit our model by weighted least squares, with weights proportional to the inverse variances—i.e. w; = n; in this case. Most linear regression software allows for observation weights.\\n\\n4. Outliers\\n\\nAn outlier is a point for which y; is far from the value predicted by the\\n\\nhetero- scedasticity\\n\\nweighted least squares\\n\\nontlien *) er\\n\\n26 of 5 oa o 4 g 24 go é a ©0 go 277 oO ° =z oo o G0 Mo g 4 oO 3 Og.000°. ° 3 ass og] BOF GBH oe 8 5° 8 0° 74 0 80k ° o og ‘ lO o lO ° 20 2 4 6 20 2 4 6 xX Fitted Values Fitted Values\\n\\n.\\n\\nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between —3 and 3.\\n\\nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.', 0.0814383923849815), (\"Many methods have been developed to properly take account of corre- lations in the error terms in time series data. Correlation among the error terms can also occur outside of time series data. For instance, consider a study in which individuals’ heights are predicted from their weights. The assumption of uncorrelated errors could be violated if some of the indi- viduals in the study are members of the same family, eat the same diet, or have been exposed to the same environmental factors. In general, the assumption of uncorrelated errors is extremely important for linear regres- sion as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.\\n\\ntracking\\n\\n0 3B 3 ir\\n\\nResponse Y Response log(Y) 15 998, x J 4 a | ek es ae 10 0. 2 $4 Sree ee ee 3s ° 3 g°4 334 eT ye 4 34 ' 605 o © 24 e4 er 437 cy T T T T T PT T T T T T 10 15 20 25 30 24 26 28 3.0 3.2 3.4 Fitted values Fitted values\\n\\nFIGURE 3.11. Residual plots. In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals, and emphasize patterns. Left: The funnel shape indicates heteroscedasticity. Right: The response has been log transformed, and there is now no evidence of heteroscedasticity.\", 0.07886904761904762), ('The red point (observation 20) in the left-hand panel of Figure 3.12 illustrates a typical outlier. The red solid line is the least squares regression fit, while the blue dashed line is the least squares fit after removal of the outlier. In this case, removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a miniscule reduction in the intercept. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance, in this example, the RSE is 1.09 when the outlier is included in the regression, but it is only 0.77 when the outlier is removed. Since the RSE is used to compute all confidence intervals and p-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Similarly, inclusion of the outlier causes the R? to decline from 0.892 to 0.805.', 0.06452452301209573), ('residual plot\\n\\nYi — Yi, versus the predictor x;. In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted) values §;. Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\\n\\nThe left panel of Figure 3.9 displays a residual plot from the linear re- gression of mpg ont: Figure 3.8. The re © horsepower on the Auto data set that was illustrated in ine is a smooth fit to the residuals, which is displayed in order to make it easier to identify any trends. The residuals exhibit a clear U-shape, which provides a strong indication of non-linearity in the data. In contrast, the ri that results from t appears to be littl term improves the ght-hand panel of Figure 3.9 displays the residual plot he model (3.36), which contains a quadratic term. There e pattern in the residuals, suggesting that the quadratic fit to the data.\\n\\nIf the residual p data, then a simp predictors, ot indicates that there are non-linear associations in the le approach is to use non-linear transformations of the such as log X, VX, and X?, in the regression model. In the later chapters of this book, we will discuss other more advanced non-linear approaches for ad ressing this issue.', 0.06377247488101534), ('statistic\\n\\n40 50 60 70 80 9 30 Rating 600 800 400 200 2000 4000 6000 8000 Limit 12000 T T T T T T 2000 4000 6000 8000 12000 Limit\\n\\nAge\\n\\nFIGURE 3.14. Scatterplots of the observations from the Credit data set. Left: A plot of age versus limit. These two variables are not collinear. Right: A plot of rating versus limit. There is high collinearity.\\n\\nIt is clear There isa we do no between 1 rom this equation that h; increases with the distance of x; from Z. simple extension of h; to the case of multiple predictors, though provide the formula here. The leverage statistic h; is always /n and 1, and the average leverage for all the observations is always equal to (p+ 1)/n. So if a given observation has a leverage statistic that great point has high leverage. ly exceeds (p+1)/n, then we may suspect that the corresponding\\n\\nThe right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus h; for the data in the left-hand panel of Figure 3.13. Ob- servation 4 high s leverage o plot a on the 1 stands out as having a very high leverage statistic as well as a udentized residual. In other words, it is an outlier as well as a high bservation. This is a particularly dangerous combination! This so reveals the reason that observation 20 had relatively little e least squares fit in Figure 3.12: it has low leverage. ect,', 0.04639423076923077), (\"In\\n\\n[34]:| ax = subplots (figsize=(8,8)) [1] ax.scatter(results3.fittedvalues, results3.resid) ax.set_xlabel('Fitted value') ax.set_ylabel('Residual') ax.axhline(0, c='k', 1ls='--')\\n\\nWe see that when the quadratic term is included in the model, there is little discernible pattern in the residuals. In order to create a cubic or higher-degree polynomial fit, we can simply change the degree argument to poly(.\\n\\n3.6.7 Qualitative Predictors\\n\\nHere we use the Carseats data, which is included in the ISLP package. We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.\\n\\nIn [35]:| Carseats = load_data('Carseats') Carseats.columns\\n\\n[35]: Index(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price', 'ShelveLoc', 'Age', 'Education', 'Urban', 'US'], dtype='object')\", 0.03125763125763126), (\"Observat ition\\n\\nFIGURE 3.10. Plots of residuals from simulated time series data sets generated with differing levels of correlation p between error terms for adjacent time points.\\n\\ntern. On the other hand, if the error terms are positively correlated, then we may see tracking in the residuals— similar values. Figure 3.10 provides an the residuals from a linear regression lated errors. There is no evidence of a hat is, adjacent residuals may have illustration. In the top panel, we see fit to data generated with uncorre- time-related trend in the residuals. In contrast, the residuals in the bottom panel are from a data set in which adjacent errors had a correlation of 0.9. residuals—adjacent residuals tend to center panel illustrates a more modera' Now there is a clear pattern in the ake on similar values. Finally, the e case in which the residuals had a correlation of 0.5. There is still evidence of tracking, but the pattern is less clear.\", 0.03076923076923077)], 'question': 'What is a studentized residual, and how is it used to identify outliers?'}\n","U funkciju za ocenjivanje dokumenata uslo je:8\n","Broj dokumenata koji pripadaju datom kontekstu:3\n","[('Residual plots can be used to identify outliers. In this example, the out- lier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a resid- ual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual e; by its estimated standard error. Observations whose studentized residuals are greater than 3 in abso- lute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier’s studentized residual exceeds 6, while all other observations have studentized residuals between —2 and 2.\\n\\nIf we believe that an outlier has occurred due tion or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. o an error in data collec-', 0.08333333333333333), ('Sometimes we have a good idea of the variance of each response. For example, the ith response could be an average of n; raw observations. If each of these raw observations is uncorrelated with variance o?, then their average has variance 0? = o7/nj. In this case a simple remedy is to fit our model by weighted least squares, with weights proportional to the inverse variances—i.e. w; = n; in this case. Most linear regression software allows for observation weights.\\n\\n4. Outliers\\n\\nAn outlier is a point for which y; is far from the value predicted by the\\n\\nhetero- scedasticity\\n\\nweighted least squares\\n\\nontlien *) er\\n\\n26 of 5 oa o 4 g 24 go é a ©0 go 277 oO ° =z oo o G0 Mo g 4 oO 3 Og.000°. ° 3 ass og] BOF GBH oe 8 5° 8 0° 74 0 80k ° o og ‘ lO o lO ° 20 2 4 6 20 2 4 6 xX Fitted Values Fitted Values\\n\\n.\\n\\nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6; typically we expect values between —3 and 3.\\n\\nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.', 0.0814383923849815), ('statistic\\n\\n40 50 60 70 80 9 30 Rating 600 800 400 200 2000 4000 6000 8000 Limit 12000 T T T T T T 2000 4000 6000 8000 12000 Limit\\n\\nAge\\n\\nFIGURE 3.14. Scatterplots of the observations from the Credit data set. Left: A plot of age versus limit. These two variables are not collinear. Right: A plot of rating versus limit. There is high collinearity.\\n\\nIt is clear There isa we do no between 1 rom this equation that h; increases with the distance of x; from Z. simple extension of h; to the case of multiple predictors, though provide the formula here. The leverage statistic h; is always /n and 1, and the average leverage for all the observations is always equal to (p+ 1)/n. So if a given observation has a leverage statistic that great point has high leverage. ly exceeds (p+1)/n, then we may suspect that the corresponding\\n\\nThe right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus h; for the data in the left-hand panel of Figure 3.13. Ob- servation 4 high s leverage o plot a on the 1 stands out as having a very high leverage statistic as well as a udentized residual. In other words, it is an outlier as well as a high bservation. This is a particularly dangerous combination! This so reveals the reason that observation 20 had relatively little e least squares fit in Figure 3.12: it has low leverage. ect,', 0.04639423076923077)]\n","A studentized residual is a type of residual that has been standardized by dividing each residual by its estimated standard error. This transformation allows for a more consistent comparison of residuals across different observations, as it accounts for the variability in the data.\n","\n","In the context of identifying outliers, studentized residuals are particularly useful because they provide a standardized measure of how far an observation's actual value is from the value predicted by the model, relative to the expected variability of that observation. Observations with studentized residuals greater than 3 in absolute value are considered possible outliers. For example, if an observation has a studentized residual exceeding 6, it is indicative of a significant deviation from the model's predictions, suggesting that it may be an outlier. This method helps to identify points that may warrant further investigation, either due to potential errors in data collection or because they may indicate deficiencies in the model itself.\n"]}],"source":["result_faiss = final_faiss_chain.invoke({\"question\":query})\n","print(result_faiss)"]},{"cell_type":"markdown","metadata":{"id":"9bKi3nV7dEN5"},"source":["**RAPTOR** - Recursive Abstractive Processing for Tree-Organized Retrieval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fo_Xn1U3dEN5"},"outputs":[],"source":["# Raptop - koristi se za kreiranje indeksa i retrieval dokumenata.\n","# 1. leafs = set startnih dokumenata (tabele, text i image summary-ji)\n","# 2. Leafs dobijaju svoje vektorske reprezentacije i bivaju klasterovani\n","# 3. Klasteri se sumarizuju u apstraktnije klastere sličnih dokumenata\n","# 4. ponavljanjem procesa, dobija se tree struktura čiji su listovi leafs, a koren root summary\n","import matplotlib.pyplot as plt\n","import tiktoken\n","\n","def num_tokens_from_string(string: str, encoding_name: str) -> int:\n","    \"\"\"Returns the number of tokens in a text string.\"\"\"\n","    encoding = tiktoken.get_encoding(encoding_name)\n","    num_tokens = len(encoding.encode(string)) # broj tokena enkodiranog stringa\n","    return num_tokens\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGcMPLWjdEN5"},"outputs":[],"source":["all_documents = [] # ovo su svi dokumenti\n","docs_texts = []\n","for k,v in zip(list(retriever.docstore.yield_keys()),retriever.docstore.mget(list(retriever.docstore.yield_keys()))):\n","    all_documents.append(Document(page_content=v,metadata={'id':k}))\n","    docs_texts.append(v)\n","token_counts = [num_tokens_from_string(d,\"cl100k_base\") for d in docs_texts]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oD-PEje3dEN5","outputId":"df01ad38-5ed0-45e5-ffcc-d3f610692954"},"outputs":[{"data":{"text/plain":["<function matplotlib.pyplot.show(close=None, block=None)>"]},"execution_count":45,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 1000x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.figure(figsize=(10, 6))\n","plt.hist(token_counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n","plt.title(\"Histogram of Token Counts\")\n","plt.xlabel(\"Token Count\")\n","plt.ylabel(\"Frequency\")\n","plt.grid(axis=\"y\", alpha=0.75)\n","plt.show"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LfDFkKNdEN5"},"outputs":[],"source":["from raptor import recursive_embed_cluster_summarize\n","leaf_texts = docs_texts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-eCJ0SPdEN6","outputId":"491948f9-d0a6-4d52-f180-554862925fbd"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Korisnik\\virtual_environment\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n","Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n","the same time. Both libraries are known to be incompatible and this\n","can cause random crashes or deadlocks on Linux when loaded in the\n","same Python program.\n","Using threadpoolctl may cause crashes or deadlocks. For more\n","information and possible workarounds, please see\n","    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n","\n","  warnings.warn(msg, RuntimeWarning)\n"]},{"name":"stdout","output_type":"stream","text":["--Generated 246 clusters--\n"]},{"name":"stderr","output_type":"stream","text":["Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n"]},{"name":"stdout","output_type":"stream","text":["--Generated 54 clusters--\n"]},{"name":"stderr","output_type":"stream","text":["Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n"]},{"name":"stdout","output_type":"stream","text":["--Generated 10 clusters--\n"]},{"name":"stderr","output_type":"stream","text":["Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n","Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n"]}],"source":["results_raptor = recursive_embed_cluster_summarize(leaf_texts,level=1,n_levels=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjyKiiY2dEN6","outputId":"df9eb378-cf08-436a-981b-5ce49d225e52"},"outputs":[{"data":{"text/plain":["{1: (                                                   text  \\\n","  0     The image is a matrix of scatter plots showing...   \n","  1     The image consists of two line graphs, each di...   \n","  2     The image is a scatter plot that represents th...   \n","  3     The image consists of two residual plots side ...   \n","  4     The image contains three separate line charts,...   \n","  ...                                                 ...   \n","  1526  174, 181 random(), 555 RandomForestRegressor()...   \n","  1527  StandardScaler (), 185, 438, 537, 555 statsmod...   \n","  1528  qualitative, 2, 27, 91, 135, 167, 202 variable...   \n","  1529  elbow, 514 semi-supervised learning, 27 sensit...   \n","  1530  Ss\\n\\nSs\\n\\nregression, 386 survival analysis,...   \n","  \n","                                                     embd  cluster  \n","  0     [-0.015677038580179214, -0.003662582952529192,...  [159.0]  \n","  1     [-0.002979329554364085, -0.011187168769538403,...  [159.0]  \n","  2     [-0.009872213937342167, 0.0017569734482094646,...  [158.0]  \n","  3     [1.2744384548568632e-05, 0.011577052064239979,...  [125.0]  \n","  4     [-0.012854015454649925, -0.005465165711939335,...  [157.0]  \n","  ...                                                 ...      ...  \n","  1526  [-0.020383048802614212, -0.028288688510656357,...  [140.0]  \n","  1527  [-0.025181828066706657, 0.0021019831765443087,...  [140.0]  \n","  1528  [-0.029653435572981834, -0.0015261805383488536...  [143.0]  \n","  1529  [-0.01720026321709156, 0.010386365465819836, 0...  [143.0]  \n","  1530  [-0.007193999830633402, 0.008238938637077808, ...  [143.0]  \n","  \n","  [1531 rows x 3 columns],\n","                                               summaries  level  cluster\n","  0    The provided documentation outlines various vi...      1    159.0\n","  1    The provided documentation describes a series ...      1    158.0\n","  2    The provided documentation describes a series ...      1    125.0\n","  3    The provided documentation describes a series ...      1    157.0\n","  4    The provided documentation describes an image ...      1     55.0\n","  ..                                                 ...    ...      ...\n","  241  The provided documentation discusses the conce...      1     34.0\n","  242  The provided documentation appears to be a com...      1    139.0\n","  243  The provided documentation appears to be an ex...      1    141.0\n","  244  The provided documentation appears to be a ref...      1    143.0\n","  245  The provided documentation appears to be a ref...      1    140.0\n","  \n","  [246 rows x 3 columns]),\n"," 2: (                                                  text  \\\n","  0    The provided documentation outlines various vi...   \n","  1    The provided documentation describes a series ...   \n","  2    The provided documentation describes a series ...   \n","  3    The provided documentation describes a series ...   \n","  4    The provided documentation describes an image ...   \n","  ..                                                 ...   \n","  241  The provided documentation discusses the conce...   \n","  242  The provided documentation appears to be a com...   \n","  243  The provided documentation appears to be an ex...   \n","  244  The provided documentation appears to be a ref...   \n","  245  The provided documentation appears to be a ref...   \n","  \n","                                                    embd cluster  \n","  0    [-0.01967238076031208, 0.007165884133428335, 0...  [23.0]  \n","  1    [-0.0161142498254776, 0.001830568304285407, 0....  [23.0]  \n","  2    [-0.014638488180935383, 0.014370137825608253, ...  [23.0]  \n","  3    [-0.014265493489801884, 0.0009027489577420056,...  [23.0]  \n","  4    [-0.0022828879300504923, 0.02129572257399559, ...  [21.0]  \n","  ..                                                 ...     ...  \n","  241  [-0.01516241766512394, 0.01594032719731331, 0....  [49.0]  \n","  242  [-0.013050644658505917, 0.023228377103805542, ...  [17.0]  \n","  243  [-0.026880621910095215, 0.01632426492869854, 0...   [8.0]  \n","  244  [-0.012599239125847816, 0.017360499128699303, ...  [17.0]  \n","  245  [-0.01534389890730381, 0.020614812150597572, 0...  [17.0]  \n","  \n","  [246 rows x 3 columns],\n","                                              summaries  level  cluster\n","  0   The provided documentation offers a detailed o...      2     23.0\n","  1   The provided documentation outlines various us...      2     21.0\n","  2   The provided documentation offers a detailed e...      2     40.0\n","  3   The provided documentation offers a detailed e...      2     20.0\n","  4   The provided documentation offers a detailed e...      2     22.0\n","  5   The provided documentation offers a detailed e...      2     49.0\n","  6   The provided documentation is a subset of the ...      2     18.0\n","  7   The provided documentation offers a detailed e...      2     39.0\n","  8   The provided documentation offers a comprehens...      2     38.0\n","  9   The provided documentation offers a comprehens...      2      7.0\n","  10  The provided documentation offers a thorough e...      2     50.0\n","  11  The provided documentation offers a comprehens...      2     36.0\n","  12  The provided documentation appears to be a col...      2     19.0\n","  13  The provided documentation offers a comprehens...      2      4.0\n","  14  The provided documentation offers an extensive...      2     31.0\n","  15  The provided documentation offers a comprehens...      2     29.0\n","  16  The provided documentation offers a detailed e...      2     25.0\n","  17  The provided documentation offers a comprehens...      2     34.0\n","  18  The provided documentation offers an in-depth ...      2     53.0\n","  19  The provided documentation offers a detailed o...      2      5.0\n","  20  The provided documentation appears to be a com...      2     17.0\n","  21  The provided documentation appears to be a com...      2      8.0\n","  22  The provided documentation offers a comprehens...      2     37.0\n","  23  The provided documentation appears to be a col...      2     30.0\n","  24  The provided documentation appears to be a det...      2     27.0\n","  25  The provided documentation offers a comprehens...      2     51.0\n","  26  The provided documentation offers a detailed o...      2     16.0\n","  27  The provided documentation offers an in-depth ...      2     43.0\n","  28  The provided documentation offers a comprehens...      2     10.0\n","  29  The provided documentation excerpt from the La...      2     15.0\n","  30  The provided documentation offers a comprehens...      2     12.0\n","  31  The provided documentation offers a comprehens...      2     46.0\n","  32  The provided documentation excerpt offers a th...      2     47.0\n","  33  The provided documentation excerpts from the L...      2      9.0\n","  34  The provided documentation offers a comprehens...      2     41.0\n","  35  The provided documentation excerpt offers a co...      2      2.0\n","  36  The provided documentation offers an in-depth ...      2      1.0\n","  37  The provided documentation offers a thorough i...      2     14.0\n","  38  The provided documentation outlines a structur...      2      3.0\n","  39  The provided documentation offers a comprehens...      2     48.0\n","  40  The provided documentation offers an in-depth ...      2     52.0\n","  41  The provided documentation offers a comprehens...      2     11.0\n","  42  The provided documentation focuses on polynomi...      2     45.0\n","  43  The provided documentation offers a comprehens...      2      0.0\n","  44  The provided documentation excerpt offers a th...      2     13.0\n","  45  The provided documentation offers a detailed e...      2     42.0\n","  46  The provided documentation offers a detailed e...      2     33.0\n","  47  The provided documentation offers an in-depth ...      2     35.0\n","  48  The provided documentation offers a comprehens...      2     44.0\n","  49  The provided documentation offers a comprehens...      2      6.0\n","  50  The provided documentation excerpts from the L...      2     28.0\n","  51  The provided documentation offers a comprehens...      2     32.0\n","  52  The provided documentation offers a detailed e...      2     26.0\n","  53  The provided documentation offers a detailed e...      2     24.0),\n"," 3: (                                                 text  \\\n","  0   The provided documentation offers a detailed o...   \n","  1   The provided documentation outlines various us...   \n","  2   The provided documentation offers a detailed e...   \n","  3   The provided documentation offers a detailed e...   \n","  4   The provided documentation offers a detailed e...   \n","  5   The provided documentation offers a detailed e...   \n","  6   The provided documentation is a subset of the ...   \n","  7   The provided documentation offers a detailed e...   \n","  8   The provided documentation offers a comprehens...   \n","  9   The provided documentation offers a comprehens...   \n","  10  The provided documentation offers a thorough e...   \n","  11  The provided documentation offers a comprehens...   \n","  12  The provided documentation appears to be a col...   \n","  13  The provided documentation offers a comprehens...   \n","  14  The provided documentation offers an extensive...   \n","  15  The provided documentation offers a comprehens...   \n","  16  The provided documentation offers a detailed e...   \n","  17  The provided documentation offers a comprehens...   \n","  18  The provided documentation offers an in-depth ...   \n","  19  The provided documentation offers a detailed o...   \n","  20  The provided documentation appears to be a com...   \n","  21  The provided documentation appears to be a com...   \n","  22  The provided documentation offers a comprehens...   \n","  23  The provided documentation appears to be a col...   \n","  24  The provided documentation appears to be a det...   \n","  25  The provided documentation offers a comprehens...   \n","  26  The provided documentation offers a detailed o...   \n","  27  The provided documentation offers an in-depth ...   \n","  28  The provided documentation offers a comprehens...   \n","  29  The provided documentation excerpt from the La...   \n","  30  The provided documentation offers a comprehens...   \n","  31  The provided documentation offers a comprehens...   \n","  32  The provided documentation excerpt offers a th...   \n","  33  The provided documentation excerpts from the L...   \n","  34  The provided documentation offers a comprehens...   \n","  35  The provided documentation excerpt offers a co...   \n","  36  The provided documentation offers an in-depth ...   \n","  37  The provided documentation offers a thorough i...   \n","  38  The provided documentation outlines a structur...   \n","  39  The provided documentation offers a comprehens...   \n","  40  The provided documentation offers an in-depth ...   \n","  41  The provided documentation offers a comprehens...   \n","  42  The provided documentation focuses on polynomi...   \n","  43  The provided documentation offers a comprehens...   \n","  44  The provided documentation excerpt offers a th...   \n","  45  The provided documentation offers a detailed e...   \n","  46  The provided documentation offers a detailed e...   \n","  47  The provided documentation offers an in-depth ...   \n","  48  The provided documentation offers a comprehens...   \n","  49  The provided documentation offers a comprehens...   \n","  50  The provided documentation excerpts from the L...   \n","  51  The provided documentation offers a comprehens...   \n","  52  The provided documentation offers a detailed e...   \n","  53  The provided documentation offers a detailed e...   \n","  \n","                                                   embd cluster  \n","  0   [-0.013447742909193039, 0.014409736730158329, ...   [6.0]  \n","  1   [-0.010730503126978874, 0.021625353023409843, ...   [7.0]  \n","  2   [-0.015756240114569664, 0.02306387573480606, 0...   [1.0]  \n","  3   [-0.0016251634806394577, -0.007020027376711368...   [6.0]  \n","  4   [-0.008395404554903507, 0.015777571126818657, ...   [7.0]  \n","  5   [-0.00349379051476717, 0.03006736747920513, -0...   [2.0]  \n","  6   [-0.01500693615525961, 0.03188629820942879, 0....   [0.0]  \n","  7   [-0.012858813628554344, 0.020920351147651672, ...   [1.0]  \n","  8   [-0.021953308954834938, 0.017565321177244186, ...   [1.0]  \n","  9   [-0.02319224365055561, 0.0024377445224672556, ...   [7.0]  \n","  10  [-0.010415700264275074, 0.008233362808823586, ...   [6.0]  \n","  11  [0.007177100051194429, 0.017854228615760803, 0...   [3.0]  \n","  12  [-0.0019014512654393911, 0.027220776304602623,...   [0.0]  \n","  13  [-0.01287197694182396, 0.007247467990964651, 0...   [7.0]  \n","  14  [-0.015282507054507732, 0.027116721495985985, ...   [5.0]  \n","  15  [-0.007157947868108749, 0.030437877401709557, ...   [5.0]  \n","  16  [-0.008101217448711395, 0.011866183951497078, ...   [4.0]  \n","  17  [0.005547303706407547, 0.028938939794898033, 0...   [3.0]  \n","  18  [-0.010795055888593197, 0.03368426486849785, 0...   [3.0]  \n","  19  [-0.013692738488316536, 0.021691204980015755, ...   [7.0]  \n","  20  [-0.007412642240524292, 0.030508002266287804, ...   [0.0]  \n","  21  [-0.011902745813131332, 0.023517023772001266, ...   [7.0]  \n","  22  [-0.015468873083591461, 0.018412822857499123, ...   [7.0]  \n","  23  [-0.017931081354618073, 0.0218854658305645, 0....   [5.0]  \n","  24  [-0.0067911227233707905, 0.02544126845896244, ...   [4.0]  \n","  25  [-0.012421349994838238, 0.024516893550753593, ...   [2.0]  \n","  26  [-0.018863923847675323, 0.018192708492279053, ...   [0.0]  \n","  27  [0.0064683957025408745, 0.030584150925278664, ...   [8.0]  \n","  28  [-0.004010277334600687, 0.027750441804528236, ...   [8.0]  \n","  29  [0.00039221084443852305, 0.028462190181016922,...   [0.0]  \n","  30  [-0.011443152092397213, 0.030613450333476067, ...   [2.0]  \n","  31  [-0.005689265672117472, 0.038725484162569046, ...   [8.0]  \n","  32  [-0.002112405374646187, 0.014737868681550026, ...   [2.0]  \n","  33  [-0.009300551377236843, 0.02415989711880684, -...   [0.0]  \n","  34  [-0.013840800151228905, 0.032412342727184296, ...   [1.0]  \n","  35  [-0.0012828034814447165, 0.026256171986460686,...   [9.0]  \n","  36  [0.0020877427887171507, 0.024312954396009445, ...   [9.0]  \n","  37  [0.012256684713065624, 0.018711550161242485, 0...   [0.0]  \n","  38  [-0.009373517706990242, 0.024591859430074692, ...   [1.0]  \n","  39  [-0.001628600643016398, 0.030231568962335587, ...   [2.0]  \n","  40  [-0.007787994109094143, 0.005383961368352175, ...   [2.0]  \n","  41  [-0.01526623498648405, 0.024146944284439087, 0...   [8.0]  \n","  42  [-0.016433527693152428, 0.02579573728144169, 0...   [8.0]  \n","  43  [-0.0014836733462288976, 0.04637840390205383, ...   [9.0]  \n","  44  [0.0060841734521090984, 0.02411867119371891, 0...   [8.0]  \n","  45  [-0.0049690427258610725, 0.019071467220783234,...   [2.0]  \n","  46  [0.004066512454301119, 0.023273374885320663, 0...   [3.0]  \n","  47  [0.0039001021068543196, 0.021128837019205093, ...   [3.0]  \n","  48  [-0.01455230824649334, 0.012900498695671558, -...   [8.0]  \n","  49  [-0.03748791292309761, -0.008847254328429699, ...   [7.0]  \n","  50  [-0.016504259780049324, 0.013151618652045727, ...   [0.0]  \n","  51  [-0.008859441615641117, 0.017882568761706352, ...   [5.0]  \n","  52  [-0.014006695710122585, 0.01598234847187996, 0...   [4.0]  \n","  53  [-0.005476148333400488, 0.017171373590826988, ...   [4.0]  ,\n","                                             summaries  level  cluster\n","  0  The provided documentation offers a comprehens...      3      6.0\n","  1  The provided documentation offers a comprehens...      3      7.0\n","  2  The provided documentation offers a comprehens...      3      1.0\n","  3  The provided documentation offers a comprehens...      3      2.0\n","  4  The provided documentation offers a comprehens...      3      0.0\n","  5  The provided documentation offers a thorough e...      3      3.0\n","  6  The provided documentation offers a comprehens...      3      5.0\n","  7  The provided documentation offers an in-depth ...      3      4.0\n","  8  The provided documentation offers a comprehens...      3      8.0\n","  9  The provided documentation offers a detailed e...      3      9.0)}"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["results_raptor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3nWN3aidEN6"},"outputs":[],"source":["with open('results/raptor.pkl', 'wb') as fileraptor:\n","    pickle.dump(results_raptor,fileraptor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ZvdBE7WdEN6"},"outputs":[],"source":["with open('results/raptor.pkl', 'rb') as fileraptor:\n","    raptor_data = pickle.load(fileraptor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2dfsQzldEN6"},"outputs":[],"source":["all_texts = leaf_texts.copy()\n","for lvl in sorted(raptor_data.keys()):\n","    summaries_raptor = raptor_data[lvl][1][\"summaries\"].tolist()\n","    all_texts.extend(summaries_raptor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUKrP8smdEN6"},"outputs":[],"source":["vectorstore_raptor = Chroma.from_texts(texts=all_texts,embedding=OpenAIEmbeddings(openai_api_key=OPEN_API_KEY))\n","retriever_raptor = vectorstore_raptor.as_retriever() # pisalo je samo vectorstore"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2GvoykdRdEN6"},"outputs":[],"source":["raptor_retriever_chain = (query_generation_chain\n","                            |(lambda x: x.queries)\n","                            | retriever_raptor.map()\n","                            #| RunnableLambda(retrieve_lists_of_text)\n","                            | reciprocal_rank_fusion # ovo radi nad svim dokumentima fuziju\n","                            )\n","final_raptor_chain = (\n","    {\"context_documents\":raptor_retriever_chain,\n","     \"question\":itemgetter(\"question\")}\n","    | RunnableLambda(grade_retrieved_documents) # retrieval check\n","    | final_prompt\n","    | chat\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWxjBqZkdEN6","outputId":"0beb5681-8b2d-490a-b7b1-1bfa72760934"},"outputs":[{"name":"stdout","output_type":"stream","text":["INPUT {'context_documents': [(Document(metadata={'doc_id': 'df0a7b11-527f-4624-aa16-d0fd86e4cda8'}, page_content='Residual plots help identify outliers in data analysis. In the example provided, an outlier is visible in the residual plot. However, determining the threshold for what constitutes an outlier can be challenging. To improve this, studentized residuals are used, calculated by dividing each residual by its estimated standard error. Observations with studentized residuals greater than 3 in absolute value are considered potential outliers. In the example, one outlier has a studentized residual exceeding 6, while others range between -2 and 2. If an outlier is suspected due to data collection errors, it may be removed, but caution is advised as it could also indicate model deficiencies, such as missing predictors.'), 0.08333333333333333), (Document(metadata={'doc_id': 'bfddef66-f737-47cd-9728-13d78c51f448'}, page_content='The text discusses the concept of variance in responses and the use of weighted least squares for linear regression when dealing with uncorrelated raw observations. It explains that the average of these observations has a reduced variance, and suggests using weights proportional to the inverse variances for model fitting. Additionally, it defines outliers as points that deviate significantly from predicted values, which can occur due to errors in data collection. A figure illustrates the impact of outliers on regression lines and residual plots, highlighting a specific outlier with a studentized residual of 6, which is outside the typical range of -3 to 3.'), 0.0814383923849815), (Document(metadata={'doc_id': '16a3f17c-a954-4420-bbc8-74d62c1c176a'}, page_content=\"The text discusses the impact of an outlier (observation 20) on a least squares regression analysis. The red point represents the outlier, while the red solid line shows the regression fit with the outlier included, and the blue dashed line shows the fit after the outlier's removal. Removing the outlier has minimal effect on the slope and a slight reduction in the intercept. However, it significantly affects the residual standard error (RSE), which increases from 0.77 to 1.09 when the outlier is included, impacting the computation of confidence intervals and p-values. Additionally, the inclusion of the outlier reduces the coefficient of determination (R²) from 0.892 to 0.805, indicating a decline in the model's explanatory power.\"), 0.06452452301209573), (Document(metadata={'doc_id': 'f0d4052e-86e7-46ec-bbd6-2d7e50ab9a2c'}, page_content=\"The text discusses the importance of accounting for correlations in error terms in statistical analyses, particularly in linear regression. It highlights that such correlations can arise not only in time series data but also in other contexts, such as predicting individuals' heights from their weights, where family ties or shared environments may introduce correlated errors. The text emphasizes the necessity of good experimental design to minimize these correlations.\\n\\nThe accompanying figure (Figure 3.11) presents residual plots. The left plot shows a funnel shape, indicating heteroscedasticity, while the right plot, which features a log transformation of the response variable, demonstrates that heteroscedasticity is no longer present. The red line in each plot represents a smooth fit to the residuals, and the blue lines indicate the outer quantiles, helping to identify patterns in the residuals.\"), 0.047619047619047616), (Document(metadata={'doc_id': '473a1666-99bc-4b21-a49d-1c0906c0e0b0'}, page_content='The text discusses the concept of residual plots in the context of linear regression models. It explains that residuals, which are the differences between observed and predicted values, should ideally show no discernible pattern when plotted against predictors. A clear pattern, such as a U-shape observed in the residual plot from a linear regression of mpg on horsepower, indicates non-linearity in the data. In contrast, a residual plot from a model including a quadratic term shows no such pattern, suggesting a better fit. The text also mentions that if residual plots indicate non-linear associations, one approach is to apply non-linear transformations to the predictors, with further advanced methods discussed in later chapters.'), 0.03252247488101534), (Document(metadata={'doc_id': '7684e661-3b4e-491a-8c12-bdd6f26f7974'}, page_content='The provided content discusses the analysis of residuals from a regression model that includes a quadratic term, indicating that there is no significant pattern in the residuals. It also mentions the potential for creating cubic or higher-degree polynomial fits by adjusting the degree argument. Additionally, it introduces the Carseats dataset from the ISLP package, which contains various predictors for predicting child car seat sales across 400 locations. The dataset includes columns such as Sales, CompPrice, Income, Advertising, Population, Price, ShelveLoc, Age, Education, Urban, and US.'), 0.015873015873015872)], 'question': 'What is a studentized residual, and how is it used to identify outliers?'}\n","U funkciju za ocenjivanje dokumenata uslo je:6\n","Broj dokumenata koji pripadaju datom kontekstu:2\n","[(Document(metadata={'doc_id': 'df0a7b11-527f-4624-aa16-d0fd86e4cda8'}, page_content='Residual plots help identify outliers in data analysis. In the example provided, an outlier is visible in the residual plot. However, determining the threshold for what constitutes an outlier can be challenging. To improve this, studentized residuals are used, calculated by dividing each residual by its estimated standard error. Observations with studentized residuals greater than 3 in absolute value are considered potential outliers. In the example, one outlier has a studentized residual exceeding 6, while others range between -2 and 2. If an outlier is suspected due to data collection errors, it may be removed, but caution is advised as it could also indicate model deficiencies, such as missing predictors.'), 0.08333333333333333), (Document(metadata={'doc_id': 'bfddef66-f737-47cd-9728-13d78c51f448'}, page_content='The text discusses the concept of variance in responses and the use of weighted least squares for linear regression when dealing with uncorrelated raw observations. It explains that the average of these observations has a reduced variance, and suggests using weights proportional to the inverse variances for model fitting. Additionally, it defines outliers as points that deviate significantly from predicted values, which can occur due to errors in data collection. A figure illustrates the impact of outliers on regression lines and residual plots, highlighting a specific outlier with a studentized residual of 6, which is outside the typical range of -3 to 3.'), 0.0814383923849815)]\n"]},{"data":{"text/plain":["'A studentized residual is a type of residual that has been standardized by dividing it by its estimated standard error. This process allows for a more accurate assessment of how far an observation deviates from the predicted values in a regression model. \\n\\nIn the context of identifying outliers, studentized residuals are particularly useful because they provide a way to quantify the extremity of a residual relative to the variability of the data. Observations with studentized residuals greater than 3 in absolute value are typically considered potential outliers. For example, if a studentized residual exceeds 6, as noted in the provided context, it indicates a significant deviation from the expected value, marking it as a strong candidate for being an outlier. \\n\\nOverall, studentized residuals help analysts identify points that may be problematic due to data collection errors or model deficiencies, guiding decisions on whether to investigate or potentially remove these outliers from the analysis.'"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["final_raptor_chain.invoke({\"question\":query})"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python (virtual_environment)","language":"python","name":"virtual_environment"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
